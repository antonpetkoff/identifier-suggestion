{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "1. [ ] Define input and output\n",
    " - Input: sequence of code tokens, where token to predict is <UNK>\n",
    " - Output: a list of predicted tokens, sorted in descending order of probability\n",
    "1. [ ] Define evaluation metric\n",
    " - Should be rank-aware, e.g. MRR, MAP, NDCG.\n",
    " - The target token is split into subtokens and the overlap between the predicted token and the target token at subtoken level is evaluated. This can be done with an F1 score.\n",
    "    - For example, if `transformSearchResponse` is the target token, its subtokens are `transform`, `search` and `response`. If the predicted token is `modifySearchResponse`, then the overlap is 2 subtokens out of 3.\n",
    "    - [ ] Should we account for the order of the subtokens? Most probably, yes.\n",
    "    ```\n",
    "    Precision = TP / (TP + FP) = #overlapping-predicted / (#overlapping-predicted + #nonoverlapping-predicted)\n",
    "    Recall = TP / (TP + FN) = #overlapping-predicted / (#overlapping-predicted + #nonoverlapping-required)\n",
    "    F1 = 2 * P * R / (P + R)\n",
    "    ```\n",
    " - The F1 approach is inspired by SQuAD and \"Suggesting accurate method and class names\" by Allamanis et al.\n",
    "\n",
    "## Execution Tasks\n",
    "\n",
    "1. [ ] Gather Data\n",
    "1. [ ] Analyze Data\n",
    "1. [ ] Implement an algorithm\n",
    "1. [ ] Create an evaluation loop\n",
    "1. [ ] Expose parameters of the algorithm\n",
    "1. [ ] Make experiments\n",
    "1. [ ] Document the experiments - Hypothesis, Data, Setup, Evaluation, Algorithm, Experiments, Conclusion, Further Steps\n",
    "\n",
    "\n",
    "\n",
    "## Discussion\n",
    "it isn't as simple as, jsut average some word embeddings\n",
    "it is easy to average the embeddings if you need to predict a word\n",
    "but i have to predict a an unknown amount of subtokens\n",
    "\n",
    "well, why don't we try to predict subtoken at a time\n",
    "but what type should it be?\n",
    "well, we can split all tokens into subtokens and count the PoS occurrences\n",
    "so that we know a few patterns upfront\n",
    "and use these patterns to fill in the subtokens with averaged word embeddings\n",
    "similar to the context we have, but filtered according to the PoS tag\n",
    "this is an interesting idea, because we do suggestion at the subtoken level\n",
    "i like this idea and we can try it\n",
    "\n",
    "of course the other idea is to use source code embeddings directly\n",
    "but they have to be learned on our source code base\n",
    "and this is a separate problem\n",
    "which involves learning a model\n",
    "which is what we try to avoid with our simplistic baseline\n",
    "we are just exploring ideas\n",
    "a baseline without learning might be a failure\n",
    "but the ideas we try and pick up during the design and experimentation are what is most valuable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_camel_case(token):\n",
    "#     TODO: implement me\n",
    "    return token\n",
    "\n",
    "def get_subtokens(token):\n",
    "    return split_by_camel_case(token)\n",
    "\n",
    "def compute_f1(target_token, predicted_token):\n",
    "    target_subtokens = get_subtokens(target_token)\n",
    "    predicted_subtokens = get_subtokens(predicted_token)\n",
    "    overlapping = Counter(target_subtokens) & Counter(predicted_subtokens)\n",
    "    overlapping_count = sum(overlapping.values())\n",
    "    \n",
    "    precision = 1.0 * overlapping_count / len(predicted_subtokens)\n",
    "    recall = 1.0 * overlapping_count / len(target_subtokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(['transform', 'search', 'response'], ['modify', 'search', 'response', 'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/buildSrc/src/test/java/org/elasticsearch/grad...</td>\n",
       "      <td>testInvalidBlockQuote</td>\n",
       "      <td>void</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/buildSrc/src/test/java/org/elasticsearch/grad...</td>\n",
       "      <td>testSimpleBlockQuote</td>\n",
       "      <td>void</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/buildSrc/src/test/java/org/elasticsearch/grad...</td>\n",
       "      <td>testMultipleBlockQuotes</td>\n",
       "      <td>void</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/buildSrc/src/test/java/org/elasticsearch/grad...</td>\n",
       "      <td>testEscapingInBlockQuote</td>\n",
       "      <td>void</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/buildSrc/src/test/java/org/elasticsearch/grad...</td>\n",
       "      <td>testIsDocWriteRequest</td>\n",
       "      <td>void</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115630</th>\n",
       "      <td>/plugins/repository-s3/src/main/java/org/elast...</td>\n",
       "      <td>refine</td>\n",
       "      <td>S3ClientSettings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115631</th>\n",
       "      <td>/plugins/repository-s3/src/main/java/org/elast...</td>\n",
       "      <td>load</td>\n",
       "      <td>Map&lt;String, S3ClientSettings&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115632</th>\n",
       "      <td>/plugins/repository-s3/src/main/java/org/elast...</td>\n",
       "      <td>loadCredentials</td>\n",
       "      <td>S3BasicCredentials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115633</th>\n",
       "      <td>/plugins/repository-s3/src/main/java/org/elast...</td>\n",
       "      <td>getClientSettings</td>\n",
       "      <td>S3ClientSettings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115634</th>\n",
       "      <td>/plugins/repository-s3/src/main/java/org/elast...</td>\n",
       "      <td>equals</td>\n",
       "      <td>bo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115635 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     file  \\\n",
       "0       /buildSrc/src/test/java/org/elasticsearch/grad...   \n",
       "1       /buildSrc/src/test/java/org/elasticsearch/grad...   \n",
       "2       /buildSrc/src/test/java/org/elasticsearch/grad...   \n",
       "3       /buildSrc/src/test/java/org/elasticsearch/grad...   \n",
       "4       /buildSrc/src/test/java/org/elasticsearch/grad...   \n",
       "...                                                   ...   \n",
       "115630  /plugins/repository-s3/src/main/java/org/elast...   \n",
       "115631  /plugins/repository-s3/src/main/java/org/elast...   \n",
       "115632  /plugins/repository-s3/src/main/java/org/elast...   \n",
       "115633  /plugins/repository-s3/src/main/java/org/elast...   \n",
       "115634  /plugins/repository-s3/src/main/java/org/elast...   \n",
       "\n",
       "                              id                           type  \n",
       "0          testInvalidBlockQuote                           void  \n",
       "1           testSimpleBlockQuote                           void  \n",
       "2        testMultipleBlockQuotes                           void  \n",
       "3       testEscapingInBlockQuote                           void  \n",
       "4          testIsDocWriteRequest                           void  \n",
       "...                          ...                            ...  \n",
       "115630                    refine               S3ClientSettings  \n",
       "115631                      load  Map<String, S3ClientSettings>  \n",
       "115632           loadCredentials             S3BasicCredentials  \n",
       "115633         getClientSettings               S3ClientSettings  \n",
       "115634                    equals                             bo  \n",
       "\n",
       "[115635 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/method-names/elastic-search-clean.csv', delimiter=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(str):\n",
    "    words = [[str[0]]]\n",
    "  \n",
    "    for c in str[1:]: \n",
    "        if words[-1][-1].islower() and c.isupper(): \n",
    "            words.append(list(c)) \n",
    "        else: \n",
    "            words[-1].append(c) \n",
    "  \n",
    "    return [''.join(word) for word in words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdfsda'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asdfSDA'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [test, invalid, block, quote]\n",
       "1               [test, simple, block, quote]\n",
       "2            [test, multiple, block, quotes]\n",
       "3         [test, escaping, in, block, quote]\n",
       "4            [test, is, doc, write, request]\n",
       "                         ...                \n",
       "115630                              [refine]\n",
       "115631                                [load]\n",
       "115632                   [load, credentials]\n",
       "115633               [get, client, settings]\n",
       "115634                              [equals]\n",
       "Name: id, Length: 115635, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id = df['id'] \\\n",
    "    .apply(camel_case_split) \\\n",
    "    .apply(lambda identifier: [word.lower() for word in identifier]) \\\n",
    "\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (45.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/tony/source/programming-tools/.venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 122.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [(test, NNP), (invalid, JJ), (block, NN), (quo...\n",
       "1    [(test, NNP), (simple, JJ), (block, NN), (quot...\n",
       "2    [(test, NN), (multiple, JJ), (block, NN), (quo...\n",
       "3    [(test, NN), (escaping, VBG), (in, IN), (block...\n",
       "4    [(test, NN), (is, VBZ), (doc, VBN), (write, NN...\n",
       "5              [(test, NN), (match, NN), (source, NN)]\n",
       "6    [(test, NNP), (parse, NNP), (os, NNP), (releas...\n",
       "7    [(test, NNP), (remove, VB), (trailing, VBG), (...\n",
       "8          [(test, NN), (remove, VB), (comments, NNS)]\n",
       "9    [(test, NN), (derive, VB), (i, PRP), (d, MD), ...\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_id.head(10).progress_apply(lambda parts: list(map(lambda token: (token.text, token.tag_) ,nlp(' '.join(parts)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

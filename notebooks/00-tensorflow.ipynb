{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import re\n",
    "import javalang\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../data/interim/preprocessed/sequences.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8,   15,  486,   10,    7,    5,  137, 4043,    4,  140,    3,\n",
       "        1674,  259,   48,   25,    4,   58,    2,    5,  137, 4043,    4,\n",
       "         140,  132,    3,    7,   17,  486,    2,    5, 1144,  609, 2062,\n",
       "           3,  486,    2,    5,    9,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [   8,   13,   42,    3,    7,    6, 1144,  609, 2062,    3,    7,\n",
       "           2,    2,    5,    9,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(df['inputs'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiers = ['public', 'private', 'protected', 'static']\n",
    "\n",
    "RE_WORDS = re.compile(r'''\n",
    "    # Find words in a string. Order matters!\n",
    "    [A-Z]+(?=[A-Z][a-z]) |  # All upper case before a capitalized word\n",
    "    [A-Z]?[a-z]+ |  # Capitalized words / all lower case\n",
    "    [A-Z]+ |  # All upper case\n",
    "    \\d+ | # Numbers\n",
    "    .+\n",
    "''', re.VERBOSE)\n",
    "\n",
    "def split_subtokens(str):\n",
    "    return [subtok for subtok in RE_WORDS.findall(str) if not subtok == '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_method(method_body):\n",
    "    method_content = method_body\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(method_content))\n",
    "    except:\n",
    "        print('ERROR in tokenizing: ' + method_content)\n",
    "        #tokens = method_content.split(' ')\n",
    "    if len(tokens) > 0:\n",
    "        return ' '.join([' '.join(split_subtokens(i.value)) for i in tokens if not i.value in modifiers])\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'void ( String foo Bar ) { System . out . println ( \"hello world\" ) ; }'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_method('void (String fooBar){System.out.println(\"hello world\");}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=1000,\n",
    "    filters='',\n",
    "    lower=False,\n",
    "    oov_token='<OOV>',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    ['{', 'printf', '(', '\"', 'hello', 'world', '\"', ')', ';', '}'],\n",
    "    ['{', 'fprintf', '(', '\"', 'hello', 'dad', '\"', ')', ';', '}'],\n",
    "    ['{', 'vprintf', '(', '\"', 'hello', 'mom', '\"', ')', ';', '}'],\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    'void ( String foo Bar ) { System . out . println ( \" hello world \" ) ; }'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(chain.from_iterable(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.num_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " '\"': 2,\n",
       " '{': 3,\n",
       " '(': 4,\n",
       " 'hello': 5,\n",
       " ')': 6,\n",
       " ';': 7,\n",
       " '}': 8,\n",
       " 'printf': 9,\n",
       " 'world': 10,\n",
       " 'fprintf': 11,\n",
       " 'dad': 12,\n",
       " 'vprintf': 13,\n",
       " 'mom': 14}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'printf', '(', '\"', 'hello', 'world', '\"', ')', ';', '}']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda l: flatten(l), sequences))\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9, 4, 2, 5, 10, 2, 6, 7, 8],\n",
       " [3, 11, 4, 2, 5, 12, 2, 6, 7, 8],\n",
       " [3, 13, 4, 2, 5, 14, 2, 6, 7, 8]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(map(lambda l: ' '.join(l), sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../data/interim/preprocessed/sequences.h5', key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (200,), types: tf.int64>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.stack(df['inputs'].values))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([61674, 16275, 29012, 40249, 25223,  6673, 15050, 59831, 11591,\n",
       "       14256, 15050, 39792, 68638, 25223,  6673, 15050, 32537, 11591,\n",
       "       40249, 33282, 16275, 68638, 25223, 60298, 11591, 16275, 68638,\n",
       "       25223, 32001, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100, 53100,\n",
       "       53100, 53100])>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted = next(iter(dataset.map(lambda seq: seq[1:])))\n",
    "tf.concat(axis=0, values=[shifted, [53100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([512, 256])>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.concat(values=[tf.constant(np.array([512])), [256]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3, 4, 5][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'We pad the sequece in the post-order where maxlen is max_seq_length.',\n",
    "    'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.string>\n",
      "\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.string>\n",
      "\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "<- encoded_text after py_function: Tensor(\"encode:0\", dtype=int64)\n",
      "<MapDataset shapes: <unknown>, types: tf.int64>\n",
      "\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[ 8  6 15  2  7 15 19  1 14 17 12  4]\n",
      "---> b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "<--- [20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[20 11 10 12  9 21  4 16 22 15 19  5 13 18  3]\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[ 8  6 15  2  7 15 19  1 14 17 12  4]\n",
      "---> b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "<--- [20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[20 11 10 12  9 21  4 16 22 15 19  5 13 18  3]\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[ 8  6 15  2  7 15 19  1 14 17 12  4]\n",
      "<MapDataset shapes: <unknown>, types: tf.float32>\n",
      "\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "---> b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "<--- [20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "---> b'If any vector is larger than max_seq_length, we truncate the post-sequence to shorten it.'\n",
      "<--- [20, 11, 10, 12, 9, 21, 4, 16, 22, 15, 19, 5, 13, 18, 3]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "---> b'We pad the sequece in the post-order where maxlen is max_seq_length.'\n",
      "<--- [8, 6, 15, 2, 7, 15, 19, 1, 14, 17, 12, 4]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 1. create Dataset entity\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sentences * 10)\n",
    "\n",
    "print(dataset)\n",
    "print()\n",
    "for item in dataset:\n",
    "    print(item.numpy())\n",
    "\n",
    "# 2. Tokenize\n",
    "# 3. Build vocabulary\n",
    "\n",
    "# TODO: swap out the tokenizer with the javalang tokenizer\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor in dataset.take(2):\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size\n",
    "\n",
    "# 4. Encode the text/tokens into numbers\n",
    "\n",
    "# TODO: this is very bad design by TF, because it includes both tokenization AND encoding\n",
    "# we already did the tokenization in the last step ...\n",
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "\n",
    "example_text = next(iter(dataset)).numpy()\n",
    "print(example_text)\n",
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)\n",
    "\n",
    "for text_tensor in dataset.take(10):\n",
    "    print(encoder.encode(text_tensor.numpy()))\n",
    "\n",
    "def encode(text_tensor):\n",
    "    print('--->', text_tensor.numpy())\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    print('<---', encoded_text)\n",
    "    \n",
    "    # TODO: why the fuck do we need to return a fucking list/tuple here?\n",
    "    return [encoded_text]\n",
    "\n",
    "def encode_map_fn(text):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    # TODO: but why?!?\n",
    "    encoded_text = tf.py_function(encode, inp=[text], Tout=tf.int64, name='encode')\n",
    "    \n",
    "    print(f'<- encoded_text after py_function: {encoded_text}')\n",
    "    \n",
    "    # `tf.data.Datasets` work best if all components have a shape set\n",
    "    #  so set the shapes manually: \n",
    "    # TODO: but why?!?\n",
    "#     encoded_text.set_shape([None])\n",
    "\n",
    "    return encoded_text\n",
    "\n",
    "dataset = dataset.map(encode_map_fn)\n",
    "\n",
    "print(dataset)\n",
    "print()\n",
    "for item in dataset.take(5):\n",
    "    print(item.numpy())\n",
    "\n",
    "# # 5. pad sequences\n",
    "# def pad_seq(seq):\n",
    "#     \"\"\"\n",
    "#         We pad the sequece in the post-order where maxlen is max_seq_length.\n",
    "#         If any vector is larger than max_seq_length, we truncate the post-sequence\n",
    "#     \"\"\"\n",
    "#     return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         [seq], # TODO: [seq.numpy()]?\n",
    "#         maxlen=max_seq_length,\n",
    "#         truncating='post',\n",
    "#         padding='post',\n",
    "#         value='',\n",
    "# #         dtype=np.float\n",
    "#     ).squeeze() # TODO: why sequeeze?\n",
    "\n",
    "# # def pad_map_fn(seq):\n",
    "# #     return tf.py_function(pad_seq, inp=[seq], Tout=(tf.float32))\n",
    "# def pad_seq_map_fn(seq):\n",
    "#     seq.numpy()\n",
    "\n",
    "# dataset = dataset.map(pad_seq_map_fn)\n",
    "    \n",
    "# 6. one-hot encode\n",
    "dataset = dataset.map(lambda seq: tf.one_hot(seq, vocab_size))\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "print()\n",
    "for item in dataset.take(5):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = ['transform', 'Search', 'Response']\n",
    "label2 = ['get', 'Abstract', 'Factory', 'Creator', 'Service']\n",
    "labels = [label1, label2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq):\n",
    "    \"\"\"\n",
    "        We pad the sequece in the post-order where maxlen is max_seq_length.\n",
    "        If any vector is larger than max_seq_length, we truncate the post-sequence\n",
    "    \"\"\"\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [seq], # TODO: [seq.numpy()]?\n",
    "        maxlen=max_seq_length,\n",
    "        truncating='post',\n",
    "        padding='post',\n",
    "        value='',\n",
    "#         dtype=np.float\n",
    "    ).squeeze() # TODO: why sequeeze?\n",
    "\n",
    "# def pad_map_fn(seq):\n",
    "#     return tf.py_function(pad_seq, inp=[seq], Tout=(tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'transform', b'Search', b'Response'], [b'get', b'Abstract', b'Factory', b'Creator', b'Service']]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor = tf.ragged.constant(labels)\n",
    "labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Service': 0,\n",
       " 'get': 1,\n",
       " 'Response': 2,\n",
       " 'Creator': 3,\n",
       " 'transform': 4,\n",
       " 'Abstract': 5,\n",
       " 'Factory': 6,\n",
       " 'Search': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocabulary = {token for label in labels for token in label}\n",
    "label_vocabulary\n",
    "\n",
    "label_to_index = { index: token for token, index in enumerate(label_vocabulary) }\n",
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=string, numpy=\n",
       "array([[b'transform', b'Search', b'Response', b'', b''],\n",
       "       [b'get', b'Abstract', b'Factory', b'Creator', b'Service']],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tensor.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`dtype` int32 is not compatible with `value`'s type: <class 'str'>\nYou should set `dtype=object` for variable length strings.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b765c471e63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2. From RaggedTensors to normal Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-6643ddfbd70d>\u001b[0m in \u001b[0;36mpad_seq\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         dtype=np.float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     ).squeeze() # TODO: why sequeeze?\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     79\u001b[0m         raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n\u001b[1;32m     80\u001b[0m                          \u001b[0;34m\"You should set `dtype=object` for variable length strings.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                          .format(dtype, type(value)))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `dtype` int32 is not compatible with `value`'s type: <class 'str'>\nYou should set `dtype=object` for variable length strings."
     ]
    }
   ],
   "source": [
    "# 2. From RaggedTensors to normal Tensors\n",
    "pad_seq(labels_tensor.to_tensor().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\nTraceback (most recent call last):\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 234, in __call__\n    return func(device, token, args)\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 123, in __call__\n    ret = self._func(*args)\n\n  File \"<ipython-input-37-7853e3c76557>\", line 4, in <lambda>\n    tf.py_function(lambda token: label_to_index[token], [labels_tensor.to_tensor()], tf.int64)\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 705, in __hash__\n    raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\nTypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n\n [Op:EagerPyFunc]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7853e3c76557>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ValueError: TypeError: object of type 'RaggedTensor' has no len()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# tf.map_fn(lambda token: label_to_index[token], labels_tensor.to_tensor())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(func, inp, Tout, name)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \"\"\"\n\u001b[0;32m--> 428\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_internal_py_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\u001b[0m in \u001b[0;36m_internal_py_func\u001b[0;34m(func, inp, Tout, stateful, eager, is_grad_func, name)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mis_async\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    318\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(input, token, Tout, is_async, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: TypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\nTraceback (most recent call last):\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 234, in __call__\n    return func(device, token, args)\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 123, in __call__\n    ret = self._func(*args)\n\n  File \"<ipython-input-37-7853e3c76557>\", line 4, in <lambda>\n    tf.py_function(lambda token: label_to_index[token], [labels_tensor.to_tensor()], tf.int64)\n\n  File \"/home/tony/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 705, in __hash__\n    raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\nTypeError: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.\n\n [Op:EagerPyFunc]"
     ]
    }
   ],
   "source": [
    "\n",
    "# first you have to convert the RaggedTensor to a normal tensor\n",
    "# ValueError: TypeError: object of type 'RaggedTensor' has no len()\n",
    "# tf.map_fn(lambda token: label_to_index[token], labels_tensor.to_tensor())\n",
    "tf.py_function(lambda token: label_to_index[token], [labels_tensor.to_tensor()], tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find valid device for node.\nNode:{{node OneHot}}\nAll kernels registered for op OneHot :\n  device='XLA_CPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_CPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_GPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT32]; T in [DT_STRING]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='GPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='XLA_GPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n [Op:OneHot] name: RaggedOneHot/one_hot/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ec106b544667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(op, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m   \"\"\"\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdispatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDISPATCH_ATTR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_dispatch.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ragged_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_array_ops.py\u001b[0m in \u001b[0;36mragged_one_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m    539\u001b[0m     return indices.with_flat_values(\n\u001b[1;32m    540\u001b[0m         array_ops.one_hot(indices.flat_values, depth, on_value, off_value, axis,\n\u001b[0;32m--> 541\u001b[0;31m                           dtype, name))\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, dtype, name)\u001b[0m\n\u001b[1;32m   3643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3644\u001b[0m     return gen_array_ops.one_hot(indices, depth, on_value, off_value, axis,\n\u001b[0;32m-> 3645\u001b[0;31m                                  name)\n\u001b[0m\u001b[1;32m   3646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mone_hot\u001b[0;34m(indices, depth, on_value, off_value, axis, name)\u001b[0m\n\u001b[1;32m   5547\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5548\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5549\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5550\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5551\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/identifier-suggestion/.venv/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find valid device for node.\nNode:{{node OneHot}}\nAll kernels registered for op OneHot :\n  device='XLA_CPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_CPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='XLA_GPU_JIT'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_VARIANT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_RESOURCE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT32]; T in [DT_STRING]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_STRING]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX128]\n  device='CPU'; TI in [DT_INT64]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_COMPLEX64]\n  device='CPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='CPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='CPU'; TI in [DT_INT64]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_BFLOAT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT8]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT32]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_UINT16]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='CPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='CPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='CPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT64]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT64]\n  device='GPU'; TI in [DT_INT64]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT32]; T in [DT_INT32]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_INT32]\n  device='GPU'; TI in [DT_INT64]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT32]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_BOOL]\n  device='GPU'; TI in [DT_INT64]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT32]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_DOUBLE]\n  device='GPU'; TI in [DT_INT64]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT32]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_FLOAT]\n  device='GPU'; TI in [DT_INT64]; T in [DT_HALF]\n  device='GPU'; TI in [DT_INT32]; T in [DT_HALF]\n  device='GPU'; TI in [DT_UINT8]; T in [DT_HALF]\n  device='XLA_GPU'; TI in [DT_INT32, DT_UINT8, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\n [Op:OneHot] name: RaggedOneHot/one_hot/"
     ]
    }
   ],
   "source": [
    "tf.one_hot(labels_tensor, len(label_vocabulary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "00-colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpetkoff/identifier-suggestion/blob/master/notebooks/00-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeJd9knlVGS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utilities\n",
        "import os\n",
        "import shutil\n",
        "from subprocess import check_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntLDgQhocMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "a12cc32e-f6f0-44d2-ca40-d03584b7c349"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul  1 15:58:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_PqX9NDdEJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "51ba93a8-1d10-4e59-8fdb-e194e9bd4513"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvcl4iohdWEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7641857e-1688-4b75-e858-5d4b7a3c666a"
      },
      "source": [
        "%env WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
        "\n",
        "# TODO: how can one read an environment variable?!?\n",
        "%cd '/content/gdrive/My Drive/src'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
            "/content/gdrive/My Drive/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNR4ZqljgeyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = check_output(['date', '-u', \"+%Y-%m-%dT%H-%M-%S\"]).decode('utf-8').strip()\n",
        "\n",
        "os.environ['PROJECT_DIR'] = os.path.join(\n",
        "    os.environ['WORKSPACE_DIR'],\n",
        "    f'identifier-suggestion-{timestamp}',\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoI39h1dEJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "68f60336-72a2-4c15-abcd-8c31ca330833"
      },
      "source": [
        "!git clone https://github.com/antonpetkoff/identifier-suggestion.git --depth 1 \"${PROJECT_DIR}\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/114)\u001b[K\rremote: Counting objects:   1% (2/114)\u001b[K\rremote: Counting objects:   2% (3/114)\u001b[K\rremote: Counting objects:   3% (4/114)\u001b[K\rremote: Counting objects:   4% (5/114)\u001b[K\rremote: Counting objects:   5% (6/114)\u001b[K\rremote: Counting objects:   6% (7/114)\u001b[K\rremote: Counting objects:   7% (8/114)\u001b[K\rremote: Counting objects:   8% (10/114)\u001b[K\rremote: Counting objects:   9% (11/114)\u001b[K\rremote: Counting objects:  10% (12/114)\u001b[K\rremote: Counting objects:  11% (13/114)\u001b[K\rremote: Counting objects:  12% (14/114)\u001b[K\rremote: Counting objects:  13% (15/114)\u001b[K\rremote: Counting objects:  14% (16/114)\u001b[K\rremote: Counting objects:  15% (18/114)\u001b[K\rremote: Counting objects:  16% (19/114)\u001b[K\rremote: Counting objects:  17% (20/114)\u001b[K\rremote: Counting objects:  18% (21/114)\u001b[K\rremote: Counting objects:  19% (22/114)\u001b[K\rremote: Counting objects:  20% (23/114)\u001b[K\rremote: Counting objects:  21% (24/114)\u001b[K\rremote: Counting objects:  22% (26/114)\u001b[K\rremote: Counting objects:  23% (27/114)\u001b[K\rremote: Counting objects:  24% (28/114)\u001b[K\rremote: Counting objects:  25% (29/114)\u001b[K\rremote: Counting objects:  26% (30/114)\u001b[K\rremote: Counting objects:  27% (31/114)\u001b[K\rremote: Counting objects:  28% (32/114)\u001b[K\rremote: Counting objects:  29% (34/114)\u001b[K\rremote: Counting objects:  30% (35/114)\u001b[K\rremote: Counting objects:  31% (36/114)\u001b[K\rremote: Counting objects:  32% (37/114)\u001b[K\rremote: Counting objects:  33% (38/114)\u001b[K\rremote: Counting objects:  34% (39/114)\u001b[K\rremote: Counting objects:  35% (40/114)\u001b[K\rremote: Counting objects:  36% (42/114)\u001b[K\rremote: Counting objects:  37% (43/114)\u001b[K\rremote: Counting objects:  38% (44/114)\u001b[K\rremote: Counting objects:  39% (45/114)\u001b[K\rremote: Counting objects:  40% (46/114)\u001b[K\rremote: Counting objects:  41% (47/114)\u001b[K\rremote: Counting objects:  42% (48/114)\u001b[K\rremote: Counting objects:  43% (50/114)\u001b[K\rremote: Counting objects:  44% (51/114)\u001b[K\rremote: Counting objects:  45% (52/114)\u001b[K\rremote: Counting objects:  46% (53/114)\u001b[K\rremote: Counting objects:  47% (54/114)\u001b[K\rremote: Counting objects:  48% (55/114)\u001b[K\rremote: Counting objects:  49% (56/114)\u001b[K\rremote: Counting objects:  50% (57/114)\u001b[K\rremote: Counting objects:  51% (59/114)\u001b[K\rremote: Counting objects:  52% (60/114)\u001b[K\rremote: Counting objects:  53% (61/114)\u001b[K\rremote: Counting objects:  54% (62/114)\u001b[K\rremote: Counting objects:  55% (63/114)\u001b[K\rremote: Counting objects:  56% (64/114)\u001b[K\rremote: Counting objects:  57% (65/114)\u001b[K\rremote: Counting objects:  58% (67/114)\u001b[K\rremote: Counting objects:  59% (68/114)\u001b[K\rremote: Counting objects:  60% (69/114)\u001b[K\rremote: Counting objects:  61% (70/114)\u001b[K\rremote: Counting objects:  62% (71/114)\u001b[K\rremote: Counting objects:  63% (72/114)\u001b[K\rremote: Counting objects:  64% (73/114)\u001b[K\rremote: Counting objects:  65% (75/114)\u001b[K\rremote: Counting objects:  66% (76/114)\u001b[K\rremote: Counting objects:  67% (77/114)\u001b[K\rremote: Counting objects:  68% (78/114)\u001b[K\rremote: Counting objects:  69% (79/114)\u001b[K\rremote: Counting objects:  70% (80/114)\u001b[K\rremote: Counting objects:  71% (81/114)\u001b[K\rremote: Counting objects:  72% (83/114)\u001b[K\rremote: Counting objects:  73% (84/114)\u001b[K\rremote: Counting objects:  74% (85/114)\u001b[K\rremote: Counting objects:  75% (86/114)\u001b[K\rremote: Counting objects:  76% (87/114)\u001b[K\rremote: Counting objects:  77% (88/114)\u001b[K\rremote: Counting objects:  78% (89/114)\u001b[K\rremote: Counting objects:  79% (91/114)\u001b[K\rremote: Counting objects:  80% (92/114)\u001b[K\rremote: Counting objects:  81% (93/114)\u001b[K\rremote: Counting objects:  82% (94/114)\u001b[K\rremote: Counting objects:  83% (95/114)\u001b[K\rremote: Counting objects:  84% (96/114)\u001b[K\rremote: Counting objects:  85% (97/114)\u001b[K\rremote: Counting objects:  86% (99/114)\u001b[K\rremote: Counting objects:  87% (100/114)\u001b[K\rremote: Counting objects:  88% (101/114)\u001b[K\rremote: Counting objects:  89% (102/114)\u001b[K\rremote: Counting objects:  90% (103/114)\u001b[K\rremote: Counting objects:  91% (104/114)\u001b[K\rremote: Counting objects:  92% (105/114)\u001b[K\rremote: Counting objects:  93% (107/114)\u001b[K\rremote: Counting objects:  94% (108/114)\u001b[K\rremote: Counting objects:  95% (109/114)\u001b[K\rremote: Counting objects:  96% (110/114)\u001b[K\rremote: Counting objects:  97% (111/114)\u001b[K\rremote: Counting objects:  98% (112/114)\u001b[K\rremote: Counting objects:  99% (113/114)\u001b[K\rremote: Counting objects: 100% (114/114)\u001b[K\rremote: Counting objects: 100% (114/114), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/104)\u001b[K\rremote: Compressing objects:   1% (2/104)\u001b[K\rremote: Compressing objects:   2% (3/104)\u001b[K\rremote: Compressing objects:   3% (4/104)\u001b[K\rremote: Compressing objects:   4% (5/104)\u001b[K\rremote: Compressing objects:   5% (6/104)\u001b[K\rremote: Compressing objects:   6% (7/104)\u001b[K\rremote: Compressing objects:   7% (8/104)\u001b[K\rremote: Compressing objects:   8% (9/104)\u001b[K\rremote: Compressing objects:   9% (10/104)\u001b[K\rremote: Compressing objects:  10% (11/104)\u001b[K\rremote: Compressing objects:  11% (12/104)\u001b[K\rremote: Compressing objects:  12% (13/104)\u001b[K\rremote: Compressing objects:  13% (14/104)\u001b[K\rremote: Compressing objects:  14% (15/104)\u001b[K\rremote: Compressing objects:  15% (16/104)\u001b[K\rremote: Compressing objects:  16% (17/104)\u001b[K\rremote: Compressing objects:  17% (18/104)\u001b[K\rremote: Compressing objects:  18% (19/104)\u001b[K\rremote: Compressing objects:  19% (20/104)\u001b[K\rremote: Compressing objects:  20% (21/104)\u001b[K\rremote: Compressing objects:  21% (22/104)\u001b[K\rremote: Compressing objects:  22% (23/104)\u001b[K\rremote: Compressing objects:  23% (24/104)\u001b[K\rremote: Compressing objects:  24% (25/104)\u001b[K\rremote: Compressing objects:  25% (26/104)\u001b[K\rremote: Compressing objects:  26% (28/104)\u001b[K\rremote: Compressing objects:  27% (29/104)\u001b[K\rremote: Compressing objects:  28% (30/104)\u001b[K\rremote: Compressing objects:  29% (31/104)\u001b[K\rremote: Compressing objects:  30% (32/104)\u001b[K\rremote: Compressing objects:  31% (33/104)\u001b[K\rremote: Compressing objects:  32% (34/104)\u001b[K\rremote: Compressing objects:  33% (35/104)\u001b[K\rremote: Compressing objects:  34% (36/104)\u001b[K\rremote: Compressing objects:  35% (37/104)\u001b[K\rremote: Compressing objects:  36% (38/104)\u001b[K\rremote: Compressing objects:  37% (39/104)\u001b[K\rremote: Compressing objects:  38% (40/104)\u001b[K\rremote: Compressing objects:  39% (41/104)\u001b[K\rremote: Compressing objects:  40% (42/104)\u001b[K\rremote: Compressing objects:  41% (43/104)\u001b[K\rremote: Compressing objects:  42% (44/104)\u001b[K\rremote: Compressing objects:  43% (45/104)\u001b[K\rremote: Compressing objects:  44% (46/104)\u001b[K\rremote: Compressing objects:  45% (47/104)\u001b[K\rremote: Compressing objects:  46% (48/104)\u001b[K\rremote: Compressing objects:  47% (49/104)\u001b[K\rremote: Compressing objects:  48% (50/104)\u001b[K\rremote: Compressing objects:  49% (51/104)\u001b[K\rremote: Compressing objects:  50% (52/104)\u001b[K\rremote: Compressing objects:  51% (54/104)\u001b[K\rremote: Compressing objects:  52% (55/104)\u001b[K\rremote: Compressing objects:  53% (56/104)\u001b[K\rremote: Compressing objects:  54% (57/104)\u001b[K\rremote: Compressing objects:  55% (58/104)\u001b[K\rremote: Compressing objects:  56% (59/104)\u001b[K\rremote: Compressing objects:  57% (60/104)\u001b[K\rremote: Compressing objects:  58% (61/104)\u001b[K\rremote: Compressing objects:  59% (62/104)\u001b[K\rremote: Compressing objects:  60% (63/104)\u001b[K\rremote: Compressing objects:  61% (64/104)\u001b[K\rremote: Compressing objects:  62% (65/104)\u001b[K\rremote: Compressing objects:  63% (66/104)\u001b[K\rremote: Compressing objects:  64% (67/104)\u001b[K\rremote: Compressing objects:  65% (68/104)\u001b[K\rremote: Compressing objects:  66% (69/104)\u001b[K\rremote: Compressing objects:  67% (70/104)\u001b[K\rremote: Compressing objects:  68% (71/104)\u001b[K\rremote: Compressing objects:  69% (72/104)\u001b[K\rremote: Compressing objects:  70% (73/104)\u001b[K\rremote: Compressing objects:  71% (74/104)\u001b[K\rremote: Compressing objects:  72% (75/104)\u001b[K\rremote: Compressing objects:  73% (76/104)\u001b[K\rremote: Compressing objects:  74% (77/104)\u001b[K\rremote: Compressing objects:  75% (78/104)\u001b[K\rremote: Compressing objects:  76% (80/104)\u001b[K\rremote: Compressing objects:  77% (81/104)\u001b[K\rremote: Compressing objects:  78% (82/104)\u001b[K\rremote: Compressing objects:  79% (83/104)\u001b[K\rremote: Compressing objects:  80% (84/104)\u001b[K\rremote: Compressing objects:  81% (85/104)\u001b[K\rremote: Compressing objects:  82% (86/104)\u001b[K\rremote: Compressing objects:  83% (87/104)\u001b[K\rremote: Compressing objects:  84% (88/104)\u001b[K\rremote: Compressing objects:  85% (89/104)\u001b[K\rremote: Compressing objects:  86% (90/104)\u001b[K\rremote: Compressing objects:  87% (91/104)\u001b[K\rremote: Compressing objects:  88% (92/104)\u001b[K\rremote: Compressing objects:  89% (93/104)\u001b[K\rremote: Compressing objects:  90% (94/104)\u001b[K\rremote: Compressing objects:  91% (95/104)\u001b[K\rremote: Compressing objects:  92% (96/104)\u001b[K\rremote: Compressing objects:  93% (97/104)\u001b[K\rremote: Compressing objects:  94% (98/104)\u001b[K\rremote: Compressing objects:  95% (99/104)\u001b[K\rremote: Compressing objects:  96% (100/104)\u001b[K\rremote: Compressing objects:  97% (101/104)\u001b[K\rremote: Compressing objects:  98% (102/104)\u001b[K\rremote: Compressing objects:  99% (103/104)\u001b[K\rremote: Compressing objects: 100% (104/104)\u001b[K\rremote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 114 (delta 1), reused 67 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (114/114), 984.71 KiB | 11.45 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mki1yqMgixt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.environ['PROJECT_DIR'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjKIGx4i2dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f5cc62bc-6977-4327-d0a3-fee06b3660f6"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13\n",
            "total 69\n",
            "drwx------  4 root root  4096 Jul  1 15:59 data\n",
            "-rw-------  1 root root 35149 Jul  1 15:59 LICENSE\n",
            "drwx------  2 root root  4096 Jul  1 15:59 notebooks\n",
            "-rw-------  1 root root 10478 Jul  1 15:59 README.md\n",
            "drwx------  3 root root  4096 Jul  1 15:59 reports\n",
            "drwx------  2 root root  4096 Jul  1 15:59 requirements\n",
            "drwx------ 13 root root  4096 Jul  1 15:59 src\n",
            "drwx------  3 root root  4096 Jul  1 15:59 vscode-extension\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCr-lrn4SRhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67ed4874-6bcc-4994-aaeb-688630cf3f34"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7BkvE4dEJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffe5926a-7d1e-4c8c-dada-ff0e9a4fa1dd"
      },
      "source": [
        "# Google Colab has standard libraries like numpy, pandas, matplotlib and TF (of course) pre-installed\n",
        "!pip install -r requirements/colab.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting javalang==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e0/12344443d66b9a84844171be90112892a371da6db09866741774b8bc0a2f/javalang-0.13.0-py3-none-any.whl\n",
            "Collecting pydash==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/fc/19b89da8a38a89f4403451a4ed2a05e16f804e6c71e4d5eaedb9d56366c3/pydash-4.8.0-py2.py3-none-any.whl (84kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.7MB/s \n",
            "\u001b[?25hCollecting python-dotenv==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/2a/07f87440444fdf2c5870a710b6770d766a1c7df9c827b0c90e807f1fb4c5/python_dotenv-0.13.0-py2.py3-none-any.whl\n",
            "Collecting wandb==0.9.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/85/ee3bec131a1078f0450140abc6c3f17177e832109c44912dca2a82e9f7d9/wandb-0.9.1-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 19.8MB/s \n",
            "\u001b[?25hCollecting tables==3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 32.3MB/s \n",
            "\u001b[?25hCollecting rouge-score==0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from javalang==0.13.0->-r requirements/colab.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (7.1.2)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/6b/939519d77c95a9b2c85b771e9dccbf9e69cb90016c7cd63887c26400dd7a/sentry_sdk-0.15.1-py2.py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 50.5MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 51.8MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (7.352.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (5.4.8)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.8MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.0.4)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, gql, watchdog, graphql-core, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=6bff850157c2af465a8764b152cdea01fa918733995f57a3d5ac77cbb46f6eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=c1dc07ca37020105d0f903158cdb26553739ac82dca275d22854dd6c27219252\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=eeefa5c2c8c6ed4a2d8c8fa5275fc7484f45757b933262406352e9ab8de9dcae\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=63166d58b4ab740880dfe477c776599a3f390792fd4f2c4e7371eccbfac7e0e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=4c9f898741cb2fd716b41f14f666f6a34d548b3da17ba9d07128892ac6e26bc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 gql watchdog graphql-core pathtools\n",
            "\u001b[31mERROR: rouge-score 0.0.4 has requirement six>=1.14.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: javalang, pydash, python-dotenv, sentry-sdk, smmap, gitdb, GitPython, configparser, subprocess32, graphql-core, gql, docker-pycreds, pathtools, watchdog, shortuuid, wandb, tables, rouge-score\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "Successfully installed GitPython-3.1.3 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 javalang-0.13.0 pathtools-0.1.2 pydash-4.8.0 python-dotenv-0.13.0 rouge-score-0.0.4 sentry-sdk-0.15.1 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tables-3.6.1 wandb-0.9.1 watchdog-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrqt3o4TP5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f62a701-e03b-42e7-f6da-57e5f9ad35d1"
      },
      "source": [
        "# provide secrets to the project, e.g. access to wandb\n",
        "shutil.copy(\n",
        "    os.path.join(os.environ['WORKSPACE_DIR'], 'secrets/.env'),\n",
        "    os.environ['PROJECT_DIR']\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/.env'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhbZ4muuveC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# monkey-patch (mock) os.symlink to be a noop, because wandb.save() uses it, but it is not supported by Google Colab Notebooks\n",
        "os.symlink = lambda *x: print('Executing mocked noop symlink with arguments', x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHkwwGMPts7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8c8dc86f-91ab-4410-d162-bd09682b050b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY3fkLJIm7ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e842d289-31ea-4b7b-8205-2082ae2855d6"
      },
      "source": [
        "from argparse import Namespace\n",
        "from src.pipelines.baseline import run\n",
        "\n",
        "params = {\n",
        "  'dir_data': '../data/processed/subtoken/', # get the input data from the shared directory\n",
        "  'file_checkpoint_dir': 'models/checkpoints/baseline/',\n",
        "  'dir_preprocessed_data': 'data/processed/seq2seq/',\n",
        "  \n",
        "  'experiment_name': 'random_search_0_0',\n",
        "  'max_input_length': 120,\n",
        "  'max_output_length': 6,\n",
        "  'input_vocab_size': 5800,\n",
        "  'output_vocab_size': 5800,\n",
        "  'input_embedding_dim': 32,\n",
        "  'output_embedding_dim': 32,\n",
        "  'latent_dim': 640,\n",
        "  'learning_rate': 0.085805,\n",
        "  'dropout_rate': 0.118569,\n",
        "  'batch_size': 512,\n",
        "\n",
        "  'epochs': 200,\n",
        "  'early_stopping_patience': 4,\n",
        "  'early_stopping_min_delta': 0.0,\n",
        "  'evaluation_dataset': 'validation',\n",
        "  'random_seed': 1,\n",
        "}\n",
        "\n",
        "run(Namespace(**params))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing logger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/2of1m4ig\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/2of1m4ig</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Experiment parameters:  Namespace(batch_size=1024, dir_data='../data/processed/subtoken/', dir_preprocessed_data='../data/processed/seq2seq/', dropout_rate=0.05, early_stopping_min_delta=0.001, early_stopping_patience=3, epochs=50, evaluation_dataset='validation', file_checkpoint_dir='models/checkpoints/baseline/', input_embedding_dim=50, input_vocab_size=5000, latent_dim=320, learning_rate=0.001, max_input_length=128, max_output_length=8, output_embedding_dim=50, output_vocab_size=5000, random_seed=1)\n",
            "Loading preprocessed files...\n",
            "Loaded input vocabulary.\n",
            "Loaded output vocabulary.\n",
            "Loaded preprocessed files.\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "EncoderLSTM (LSTM)           multiple                  474880    \n",
            "=================================================================\n",
            "Total params: 724,880\n",
            "Trainable params: 724,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "DecoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "DecoderLSTM (LSTM)           multiple                  884480    \n",
            "_________________________________________________________________\n",
            "DenseOutput (Dense)          multiple                  1605000   \n",
            "_________________________________________________________________\n",
            "bahdanau_attention (Bahdanau multiple                  205761    \n",
            "=================================================================\n",
            "Total params: 2,945,241\n",
            "Trainable params: 2,945,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-1\n",
            "Executing mocked noop symlink with arguments ('/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/models/checkpoints/baseline/config.json', './reports/wandb/run-20200701_155943-2of1m4ig/config.json')\n",
            "Done saving model\n",
            "Restored from models/checkpoints/baseline/ckpt-1\n",
            "epoch 1 - batch 10 - loss 4.408756732940674\n",
            "epoch 1 - batch 20 - loss 3.08723783493042\n",
            "epoch 1 - batch 30 - loss 3.115360736846924\n",
            "epoch 1 - batch 40 - loss 3.1828439235687256\n",
            "epoch 1 - batch 50 - loss 3.0684542655944824\n",
            "epoch 1 - batch 60 - loss 3.129190444946289\n",
            "epoch 1 - batch 70 - loss 3.215830087661743\n",
            "epoch 1 - batch 80 - loss 3.1878738403320312\n",
            "epoch 1 - batch 90 - loss 3.1342785358428955\n",
            "epoch 1 - batch 100 - loss 3.0857670307159424\n",
            "epoch 1 - batch 110 - loss 3.153585195541382\n",
            "epoch 1 - batch 120 - loss 3.0321671962738037\n",
            "epoch 1 - batch 130 - loss 3.171478748321533\n",
            "epoch 1 - batch 140 - loss 3.09132981300354\n",
            "epoch 1 - batch 150 - loss 3.07035231590271\n",
            "epoch 1 - batch 160 - loss 2.955005168914795\n",
            "epoch 1 - batch 170 - loss 3.0259265899658203\n",
            "epoch 1 - batch 180 - loss 3.0205094814300537\n",
            "epoch 1 - batch 190 - loss 3.015817403793335\n",
            "epoch 1 - batch 200 - loss 2.8715548515319824\n",
            "epoch 1 - batch 210 - loss 3.036540985107422\n",
            "epoch 1 - batch 220 - loss 2.987924575805664\n",
            "epoch 1 - batch 230 - loss 2.899728298187256\n",
            "epoch 1 - batch 240 - loss 2.922208070755005\n",
            "epoch 1 - batch 250 - loss 2.9066812992095947\n",
            "epoch 1 - batch 260 - loss 2.892279863357544\n",
            "epoch 1 - batch 270 - loss 2.7935497760772705\n",
            "epoch 1 - batch 280 - loss 2.864741086959839\n",
            "epoch 1 - batch 290 - loss 2.8933217525482178\n",
            "epoch 1 - batch 300 - loss 3.1226744651794434\n",
            "epoch 1 - batch 310 - loss 3.356020212173462\n",
            "epoch 1 - batch 320 - loss 3.1966288089752197\n",
            "epoch 1 - batch 330 - loss 3.0519938468933105\n",
            "epoch 1 - batch 340 - loss 2.845076322555542\n",
            "epoch 1 - batch 350 - loss 2.913084030151367\n",
            "epoch 1 - batch 360 - loss 2.9394803047180176\n",
            "epoch 1 - batch 370 - loss 2.81483793258667\n",
            "epoch 1 - batch 380 - loss 2.7918739318847656\n",
            "epoch 1 - batch 390 - loss 2.909902811050415\n",
            "epoch 1 - batch 400 - loss 2.8098971843719482\n",
            "epoch 1 - batch 410 - loss 2.817505359649658\n",
            "epoch 1 - batch 420 - loss 2.834620952606201\n",
            "epoch 1 - batch 430 - loss 2.709188222885132\n",
            "epoch 1 - batch 440 - loss 2.8762238025665283\n",
            "epoch 1 - batch 450 - loss 2.699683427810669\n",
            "epoch 1 - batch 460 - loss 2.836540699005127\n",
            "epoch 1 - batch 470 - loss 2.88427996635437\n",
            "epoch 1 - batch 480 - loss 2.871614694595337\n",
            "epoch 1 - batch 490 - loss 2.79953932762146\n",
            "epoch 1 - batch 500 - loss 2.8305625915527344\n",
            "epoch 1 - batch 510 - loss 2.8399100303649902\n",
            "epoch 1 - batch 520 - loss 2.77593731880188\n",
            "epoch 1 - batch 530 - loss 2.865769624710083\n",
            "epoch 1 - batch 540 - loss 2.752047300338745\n",
            "epoch 1 - batch 550 - loss 2.741344451904297\n",
            "epoch 1 - batch 560 - loss 2.8243861198425293\n",
            "epoch 1 - batch 570 - loss 2.7210278511047363\n",
            "epoch 1 - batch 580 - loss 2.8254072666168213\n",
            "epoch 1 - batch 590 - loss 2.781780481338501\n",
            "epoch 1 - batch 600 - loss 2.787616491317749\n",
            "epoch 1 - batch 610 - loss 2.7368550300598145\n",
            "epoch 1 training time: 227.2587172985077 sec\n",
            "evaluation of batch 0 took: 0.2781193256378174\n",
            "evaluation of batch 50 took: 0.10710954666137695\n",
            "evaluation of batch 100 took: 0.10405468940734863\n",
            "evaluation of batch 150 took: 0.1044607162475586\n",
            "evaluation of batch 200 took: 0.11615228652954102\n",
            "evaluation of batch 250 took: 0.11797189712524414\n",
            "evaluation of batch 300 took: 0.12130308151245117\n",
            "evaluation of batch 350 took: 0.12482404708862305\n",
            "evaluation of batch 400 took: 0.12931180000305176\n",
            "evaluation of batch 450 took: 0.1286001205444336\n",
            "evaluation of batch 500 took: 0.1217341423034668\n",
            "evaluation of batch 550 took: 0.12198996543884277\n",
            "evaluation of batch 600 took: 0.11677384376525879\n",
            "epoch 1 evaluation on training data time: 72.58424067497253 sec\n",
            "evaluation of batch 0 took: 0.13846039772033691\n",
            "evaluation of batch 50 took: 0.12097907066345215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 evaluation on test data time: 19.938061714172363 sec\n",
            "epoch evaluation:  {'epoch': 1, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.7829003>, 'test_rouge_1_p': 0.19762961647727273, 'test_rouge_1_r': 0.08504144200293753, 'test_rouge_1_f1': 0.11387190565229344, 'test_rouge_2_p': 0.0014965503246753247, 'test_rouge_2_r': 0.0007055769751082251, 'test_rouge_2_f1': 0.0008794799880179346, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.19760213744588745, 'test_rouge_L_r': 0.08503165825989485, 'test_rouge_L_f1': 0.11385787578335285}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 - batch 10 - loss 2.770319700241089\n",
            "epoch 2 - batch 20 - loss 2.867374897003174\n",
            "epoch 2 - batch 30 - loss 2.7324159145355225\n",
            "epoch 2 - batch 40 - loss 2.857286214828491\n",
            "epoch 2 - batch 50 - loss 2.8507487773895264\n",
            "epoch 2 - batch 60 - loss 2.7810909748077393\n",
            "epoch 2 - batch 70 - loss 2.7317471504211426\n",
            "epoch 2 - batch 80 - loss 2.7718958854675293\n",
            "epoch 2 - batch 90 - loss 2.83500337600708\n",
            "epoch 2 - batch 100 - loss 2.7588024139404297\n",
            "epoch 2 - batch 110 - loss 2.811523914337158\n",
            "epoch 2 - batch 120 - loss 2.8340184688568115\n",
            "epoch 2 - batch 130 - loss 2.681711196899414\n",
            "epoch 2 - batch 140 - loss 2.7033309936523438\n",
            "epoch 2 - batch 150 - loss 2.7989630699157715\n",
            "epoch 2 - batch 160 - loss 2.6727137565612793\n",
            "epoch 2 - batch 170 - loss 2.693540334701538\n",
            "epoch 2 - batch 180 - loss 2.77941632270813\n",
            "epoch 2 - batch 190 - loss 2.7174746990203857\n",
            "epoch 2 - batch 200 - loss 2.753131628036499\n",
            "epoch 2 - batch 210 - loss 2.791276693344116\n",
            "epoch 2 - batch 220 - loss 2.761784076690674\n",
            "epoch 2 - batch 230 - loss 2.711261510848999\n",
            "epoch 2 - batch 240 - loss 2.6810812950134277\n",
            "epoch 2 - batch 250 - loss 2.698075771331787\n",
            "epoch 2 - batch 260 - loss 2.653942823410034\n",
            "epoch 2 - batch 270 - loss 2.724215030670166\n",
            "epoch 2 - batch 280 - loss 2.677424669265747\n",
            "epoch 2 - batch 290 - loss 2.7130534648895264\n",
            "epoch 2 - batch 300 - loss 2.8460352420806885\n",
            "epoch 2 - batch 310 - loss 2.6958611011505127\n",
            "epoch 2 - batch 320 - loss 2.700213670730591\n",
            "epoch 2 - batch 330 - loss 2.7596962451934814\n",
            "epoch 2 - batch 340 - loss 2.753901958465576\n",
            "epoch 2 - batch 350 - loss 2.7381699085235596\n",
            "epoch 2 - batch 360 - loss 2.6987555027008057\n",
            "epoch 2 - batch 370 - loss 2.6922895908355713\n",
            "epoch 2 - batch 380 - loss 2.6058616638183594\n",
            "epoch 2 - batch 390 - loss 2.690282106399536\n",
            "epoch 2 - batch 400 - loss 2.5772879123687744\n",
            "epoch 2 - batch 410 - loss 2.6749072074890137\n",
            "epoch 2 - batch 420 - loss 2.7454581260681152\n",
            "epoch 2 - batch 430 - loss 2.718248128890991\n",
            "epoch 2 - batch 440 - loss 2.62907075881958\n",
            "epoch 2 - batch 450 - loss 2.6341824531555176\n",
            "epoch 2 - batch 460 - loss 2.734668493270874\n",
            "epoch 2 - batch 470 - loss 2.805001974105835\n",
            "epoch 2 - batch 480 - loss 2.6816515922546387\n",
            "epoch 2 - batch 490 - loss 2.6564483642578125\n",
            "epoch 2 - batch 500 - loss 2.671968936920166\n",
            "epoch 2 - batch 510 - loss 2.7814574241638184\n",
            "epoch 2 - batch 520 - loss 2.6494712829589844\n",
            "epoch 2 - batch 530 - loss 2.742570400238037\n",
            "epoch 2 - batch 540 - loss 2.66795015335083\n",
            "epoch 2 - batch 550 - loss 2.6682286262512207\n",
            "epoch 2 - batch 560 - loss 2.6680424213409424\n",
            "epoch 2 - batch 570 - loss 2.728872537612915\n",
            "epoch 2 - batch 580 - loss 2.6487491130828857\n",
            "epoch 2 - batch 590 - loss 2.740243673324585\n",
            "epoch 2 - batch 600 - loss 2.633758068084717\n",
            "epoch 2 - batch 610 - loss 2.6599419116973877\n",
            "epoch 2 training time: 220.3332061767578 sec\n",
            "evaluation of batch 0 took: 0.12027192115783691\n",
            "evaluation of batch 50 took: 0.12336182594299316\n",
            "evaluation of batch 100 took: 0.12255430221557617\n",
            "evaluation of batch 150 took: 0.12273287773132324\n",
            "evaluation of batch 200 took: 0.12612175941467285\n",
            "evaluation of batch 250 took: 0.1383957862854004\n",
            "evaluation of batch 300 took: 0.12850713729858398\n",
            "evaluation of batch 350 took: 0.1273050308227539\n",
            "evaluation of batch 400 took: 0.1394791603088379\n",
            "evaluation of batch 450 took: 0.1249094009399414\n",
            "evaluation of batch 500 took: 0.11889243125915527\n",
            "evaluation of batch 550 took: 0.12599968910217285\n",
            "evaluation of batch 600 took: 0.1195826530456543\n",
            "epoch 2 evaluation on training data time: 77.93709516525269 sec\n",
            "evaluation of batch 0 took: 0.13930606842041016\n",
            "evaluation of batch 50 took: 0.12334680557250977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 evaluation on test data time: 19.833628177642822 sec\n",
            "epoch evaluation:  {'epoch': 2, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.675507>, 'test_rouge_1_p': 0.2540988145968615, 'test_rouge_1_r': 0.1136263081226808, 'test_rouge_1_f1': 0.15025285333409855, 'test_rouge_2_p': 0.003781537472943723, 'test_rouge_2_r': 0.0016914400703463208, 'test_rouge_2_f1': 0.002150490878169449, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.25404343377976196, 'test_rouge_L_r': 0.11359445056431658, 'test_rouge_L_f1': 0.15021275755663469}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 - batch 10 - loss 2.7174556255340576\n",
            "epoch 3 - batch 20 - loss 2.6554927825927734\n",
            "epoch 3 - batch 30 - loss 2.6248550415039062\n",
            "epoch 3 - batch 40 - loss 2.636136531829834\n",
            "epoch 3 - batch 50 - loss 2.684709310531616\n",
            "epoch 3 - batch 60 - loss 2.7498528957366943\n",
            "epoch 3 - batch 70 - loss 2.670579433441162\n",
            "epoch 3 - batch 80 - loss 2.629220485687256\n",
            "epoch 3 - batch 90 - loss 2.643371820449829\n",
            "epoch 3 - batch 100 - loss 2.6018595695495605\n",
            "epoch 3 - batch 110 - loss 2.7039599418640137\n",
            "epoch 3 - batch 120 - loss 2.6686007976531982\n",
            "epoch 3 - batch 130 - loss 2.6359152793884277\n",
            "epoch 3 - batch 140 - loss 2.652703046798706\n",
            "epoch 3 - batch 150 - loss 2.676136016845703\n",
            "epoch 3 - batch 160 - loss 2.7119832038879395\n",
            "epoch 3 - batch 170 - loss 2.6648507118225098\n",
            "epoch 3 - batch 180 - loss 2.6771767139434814\n",
            "epoch 3 - batch 190 - loss 2.6632297039031982\n",
            "epoch 3 - batch 200 - loss 2.629777193069458\n",
            "epoch 3 - batch 210 - loss 2.608025074005127\n",
            "epoch 3 - batch 220 - loss 2.656982660293579\n",
            "epoch 3 - batch 230 - loss 2.6502938270568848\n",
            "epoch 3 - batch 240 - loss 2.5189459323883057\n",
            "epoch 3 - batch 250 - loss 2.7463557720184326\n",
            "epoch 3 - batch 260 - loss 2.634328603744507\n",
            "epoch 3 - batch 270 - loss 2.6555914878845215\n",
            "epoch 3 - batch 280 - loss 2.5943315029144287\n",
            "epoch 3 - batch 290 - loss 2.7287113666534424\n",
            "epoch 3 - batch 300 - loss 2.6149935722351074\n",
            "epoch 3 - batch 310 - loss 2.5981714725494385\n",
            "epoch 3 - batch 320 - loss 2.5925674438476562\n",
            "epoch 3 - batch 330 - loss 2.6328792572021484\n",
            "epoch 3 - batch 340 - loss 2.6434519290924072\n",
            "epoch 3 - batch 350 - loss 2.662365436553955\n",
            "epoch 3 - batch 360 - loss 2.640160083770752\n",
            "epoch 3 - batch 370 - loss 2.6097865104675293\n",
            "epoch 3 - batch 380 - loss 2.5926566123962402\n",
            "epoch 3 - batch 390 - loss 2.594068765640259\n",
            "epoch 3 - batch 400 - loss 2.6038320064544678\n",
            "epoch 3 - batch 410 - loss 2.6584079265594482\n",
            "epoch 3 - batch 420 - loss 2.681452989578247\n",
            "epoch 3 - batch 430 - loss 2.556300401687622\n",
            "epoch 3 - batch 440 - loss 2.6461293697357178\n",
            "epoch 3 - batch 450 - loss 2.583423376083374\n",
            "epoch 3 - batch 460 - loss 2.5415070056915283\n",
            "epoch 3 - batch 470 - loss 2.5991156101226807\n",
            "epoch 3 - batch 480 - loss 2.634706735610962\n",
            "epoch 3 - batch 490 - loss 2.5719211101531982\n",
            "epoch 3 - batch 500 - loss 2.589726686477661\n",
            "epoch 3 - batch 510 - loss 2.60400652885437\n",
            "epoch 3 - batch 520 - loss 2.5227503776550293\n",
            "epoch 3 - batch 530 - loss 2.665184497833252\n",
            "epoch 3 - batch 540 - loss 2.6107242107391357\n",
            "epoch 3 - batch 550 - loss 2.508582592010498\n",
            "epoch 3 - batch 560 - loss 2.54596209526062\n",
            "epoch 3 - batch 570 - loss 2.5619430541992188\n",
            "epoch 3 - batch 580 - loss 2.6083645820617676\n",
            "epoch 3 - batch 590 - loss 2.5825893878936768\n",
            "epoch 3 - batch 600 - loss 2.5042364597320557\n",
            "epoch 3 - batch 610 - loss 2.5650227069854736\n",
            "epoch 3 training time: 220.1450798511505 sec\n",
            "evaluation of batch 0 took: 0.12132954597473145\n",
            "evaluation of batch 50 took: 0.12379312515258789\n",
            "evaluation of batch 100 took: 0.12181377410888672\n",
            "evaluation of batch 150 took: 0.12725186347961426\n",
            "evaluation of batch 200 took: 0.12293839454650879\n",
            "evaluation of batch 250 took: 0.12169837951660156\n",
            "evaluation of batch 300 took: 0.12622761726379395\n",
            "evaluation of batch 350 took: 0.12171053886413574\n",
            "evaluation of batch 400 took: 0.12647461891174316\n",
            "evaluation of batch 450 took: 0.12864923477172852\n",
            "evaluation of batch 500 took: 0.13219594955444336\n",
            "evaluation of batch 550 took: 0.1266167163848877\n",
            "evaluation of batch 600 took: 0.13082623481750488\n",
            "epoch 3 evaluation on training data time: 77.82557988166809 sec\n",
            "evaluation of batch 0 took: 0.15261411666870117\n",
            "evaluation of batch 50 took: 0.131486177444458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 evaluation on test data time: 20.0619056224823 sec\n",
            "epoch evaluation:  {'epoch': 3, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.575095>, 'test_rouge_1_p': 0.2543063567756649, 'test_rouge_1_r': 0.14031678189741803, 'test_rouge_1_f1': 0.17171247137321133, 'test_rouge_2_p': 0.02014339826839827, 'test_rouge_2_r': 0.018355147456709964, 'test_rouge_2_f1': 0.018832514873631963, 'test_rouge_3_p': 5.0730519480519484e-05, 'test_rouge_3_r': 1.5219155844155844e-05, 'test_rouge_3_f1': 2.282873376623377e-05, 'test_rouge_L_p': 0.2541095042420377, 'test_rouge_L_r': 0.1401784506416203, 'test_rouge_L_f1': 0.1715547948483473}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 saved checkpoint: models/checkpoints/baseline/ckpt-2\n",
            "epoch 4 - batch 10 - loss 2.6701066493988037\n",
            "epoch 4 - batch 20 - loss 2.664677381515503\n",
            "epoch 4 - batch 30 - loss 2.3907032012939453\n",
            "epoch 4 - batch 40 - loss 2.506490707397461\n",
            "epoch 4 - batch 50 - loss 2.563532590866089\n",
            "epoch 4 - batch 60 - loss 2.6291606426239014\n",
            "epoch 4 - batch 70 - loss 2.608062744140625\n",
            "epoch 4 - batch 80 - loss 2.538487195968628\n",
            "epoch 4 - batch 90 - loss 2.4677984714508057\n",
            "epoch 4 - batch 100 - loss 2.674129009246826\n",
            "epoch 4 - batch 110 - loss 2.5436859130859375\n",
            "epoch 4 - batch 120 - loss 2.548384428024292\n",
            "epoch 4 - batch 130 - loss 2.4882919788360596\n",
            "epoch 4 - batch 140 - loss 2.619309663772583\n",
            "epoch 4 - batch 150 - loss 2.4576377868652344\n",
            "epoch 4 - batch 160 - loss 2.528339385986328\n",
            "epoch 4 - batch 170 - loss 2.5225799083709717\n",
            "epoch 4 - batch 180 - loss 2.6411306858062744\n",
            "epoch 4 - batch 190 - loss 2.6219515800476074\n",
            "epoch 4 - batch 200 - loss 2.6063907146453857\n",
            "epoch 4 - batch 210 - loss 2.4721391201019287\n",
            "epoch 4 - batch 220 - loss 2.507155179977417\n",
            "epoch 4 - batch 230 - loss 2.5600650310516357\n",
            "epoch 4 - batch 240 - loss 2.4876110553741455\n",
            "epoch 4 - batch 250 - loss 2.4696671962738037\n",
            "epoch 4 - batch 260 - loss 2.5108044147491455\n",
            "epoch 4 - batch 270 - loss 2.586402416229248\n",
            "epoch 4 - batch 280 - loss 2.500401020050049\n",
            "epoch 4 - batch 290 - loss 2.5066983699798584\n",
            "epoch 4 - batch 300 - loss 2.502415895462036\n",
            "epoch 4 - batch 310 - loss 2.5002083778381348\n",
            "epoch 4 - batch 320 - loss 2.599377155303955\n",
            "epoch 4 - batch 330 - loss 2.55153489112854\n",
            "epoch 4 - batch 340 - loss 2.5162136554718018\n",
            "epoch 4 - batch 350 - loss 2.482954263687134\n",
            "epoch 4 - batch 360 - loss 2.466716766357422\n",
            "epoch 4 - batch 370 - loss 2.489793539047241\n",
            "epoch 4 - batch 380 - loss 2.415661096572876\n",
            "epoch 4 - batch 390 - loss 2.5154287815093994\n",
            "epoch 4 - batch 400 - loss 2.5331034660339355\n",
            "epoch 4 - batch 410 - loss 2.4878578186035156\n",
            "epoch 4 - batch 420 - loss 2.4543020725250244\n",
            "epoch 4 - batch 430 - loss 2.411207437515259\n",
            "epoch 4 - batch 440 - loss 2.5260701179504395\n",
            "epoch 4 - batch 450 - loss 2.4290428161621094\n",
            "epoch 4 - batch 460 - loss 2.4750921726226807\n",
            "epoch 4 - batch 470 - loss 2.369086742401123\n",
            "epoch 4 - batch 480 - loss 2.5034918785095215\n",
            "epoch 4 - batch 490 - loss 2.406470775604248\n",
            "epoch 4 - batch 500 - loss 2.4294116497039795\n",
            "epoch 4 - batch 510 - loss 2.475977659225464\n",
            "epoch 4 - batch 520 - loss 2.622910261154175\n",
            "epoch 4 - batch 530 - loss 2.4741992950439453\n",
            "epoch 4 - batch 540 - loss 2.398524045944214\n",
            "epoch 4 - batch 550 - loss 2.46203875541687\n",
            "epoch 4 - batch 560 - loss 2.4743387699127197\n",
            "epoch 4 - batch 570 - loss 2.455756664276123\n",
            "epoch 4 - batch 580 - loss 2.4597713947296143\n",
            "epoch 4 - batch 590 - loss 2.4329516887664795\n",
            "epoch 4 - batch 600 - loss 2.454101324081421\n",
            "epoch 4 - batch 610 - loss 2.4439120292663574\n",
            "epoch 4 training time: 223.98452019691467 sec\n",
            "evaluation of batch 0 took: 0.13175582885742188\n",
            "evaluation of batch 50 took: 0.13207030296325684\n",
            "evaluation of batch 100 took: 0.1282052993774414\n",
            "evaluation of batch 150 took: 0.13034367561340332\n",
            "evaluation of batch 200 took: 0.13869285583496094\n",
            "evaluation of batch 250 took: 0.1286463737487793\n",
            "evaluation of batch 300 took: 0.1340620517730713\n",
            "evaluation of batch 350 took: 0.13107800483703613\n",
            "evaluation of batch 400 took: 0.13955402374267578\n",
            "evaluation of batch 450 took: 0.13724446296691895\n",
            "evaluation of batch 500 took: 0.13171625137329102\n",
            "evaluation of batch 550 took: 0.13515615463256836\n",
            "evaluation of batch 600 took: 0.12836408615112305\n",
            "epoch 4 evaluation on training data time: 81.88489484786987 sec\n",
            "evaluation of batch 0 took: 0.1414928436279297\n",
            "evaluation of batch 50 took: 0.12975692749023438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 evaluation on test data time: 20.573442220687866 sec\n",
            "epoch evaluation:  {'epoch': 4, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.4674318>, 'test_rouge_1_p': 0.2767087729978354, 'test_rouge_1_r': 0.16005484935451456, 'test_rouge_1_f1': 0.19189622029643372, 'test_rouge_2_p': 0.02607379599567099, 'test_rouge_2_r': 0.02274122362012987, 'test_rouge_2_f1': 0.02356907267684966, 'test_rouge_3_p': 0.0016345796130952377, 'test_rouge_3_r': 0.0013646509740259743, 'test_rouge_3_f1': 0.0014407970811430636, 'test_rouge_L_p': 0.2765119506609462, 'test_rouge_L_r': 0.15992781167864872, 'test_rouge_L_f1': 0.19174556588568392}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 - batch 10 - loss 2.4344305992126465\n",
            "epoch 5 - batch 20 - loss 2.482940912246704\n",
            "epoch 5 - batch 30 - loss 2.3997650146484375\n",
            "epoch 5 - batch 40 - loss 2.3780055046081543\n",
            "epoch 5 - batch 50 - loss 2.47806978225708\n",
            "epoch 5 - batch 60 - loss 2.4466097354888916\n",
            "epoch 5 - batch 70 - loss 2.4552676677703857\n",
            "epoch 5 - batch 80 - loss 2.447864055633545\n",
            "epoch 5 - batch 90 - loss 2.488152027130127\n",
            "epoch 5 - batch 100 - loss 2.424271821975708\n",
            "epoch 5 - batch 110 - loss 2.4515058994293213\n",
            "epoch 5 - batch 120 - loss 2.5238375663757324\n",
            "epoch 5 - batch 130 - loss 2.3509907722473145\n",
            "epoch 5 - batch 140 - loss 2.4060451984405518\n",
            "epoch 5 - batch 150 - loss 2.455984354019165\n",
            "epoch 5 - batch 160 - loss 2.4463417530059814\n",
            "epoch 5 - batch 170 - loss 2.3688035011291504\n",
            "epoch 5 - batch 180 - loss 2.390287399291992\n",
            "epoch 5 - batch 190 - loss 2.511669635772705\n",
            "epoch 5 - batch 200 - loss 2.404114246368408\n",
            "epoch 5 - batch 210 - loss 2.429924488067627\n",
            "epoch 5 - batch 220 - loss 2.3798582553863525\n",
            "epoch 5 - batch 230 - loss 2.3509769439697266\n",
            "epoch 5 - batch 240 - loss 2.2899839878082275\n",
            "epoch 5 - batch 250 - loss 2.418393850326538\n",
            "epoch 5 - batch 260 - loss 2.2976748943328857\n",
            "epoch 5 - batch 270 - loss 2.441389560699463\n",
            "epoch 5 - batch 280 - loss 2.4365174770355225\n",
            "epoch 5 - batch 290 - loss 2.3914971351623535\n",
            "epoch 5 - batch 300 - loss 2.440019369125366\n",
            "epoch 5 - batch 310 - loss 2.45686411857605\n",
            "epoch 5 - batch 320 - loss 2.3959310054779053\n",
            "epoch 5 - batch 330 - loss 2.3648593425750732\n",
            "epoch 5 - batch 340 - loss 2.366007089614868\n",
            "epoch 5 - batch 350 - loss 2.35625958442688\n",
            "epoch 5 - batch 360 - loss 2.297119140625\n",
            "epoch 5 - batch 370 - loss 2.4029862880706787\n",
            "epoch 5 - batch 380 - loss 2.3765923976898193\n",
            "epoch 5 - batch 390 - loss 2.4194674491882324\n",
            "epoch 5 - batch 400 - loss 2.3328845500946045\n",
            "epoch 5 - batch 410 - loss 2.418274402618408\n",
            "epoch 5 - batch 420 - loss 2.3129117488861084\n",
            "epoch 5 - batch 430 - loss 2.272174119949341\n",
            "epoch 5 - batch 440 - loss 2.42317533493042\n",
            "epoch 5 - batch 450 - loss 2.363055467605591\n",
            "epoch 5 - batch 460 - loss 2.3702709674835205\n",
            "epoch 5 - batch 470 - loss 2.415248155593872\n",
            "epoch 5 - batch 480 - loss 2.35357666015625\n",
            "epoch 5 - batch 490 - loss 2.244393825531006\n",
            "epoch 5 - batch 500 - loss 2.2829625606536865\n",
            "epoch 5 - batch 510 - loss 2.3468017578125\n",
            "epoch 5 - batch 520 - loss 2.2808399200439453\n",
            "epoch 5 - batch 530 - loss 2.3198494911193848\n",
            "epoch 5 - batch 540 - loss 2.3409857749938965\n",
            "epoch 5 - batch 550 - loss 2.3380415439605713\n",
            "epoch 5 - batch 560 - loss 2.3441810607910156\n",
            "epoch 5 - batch 570 - loss 2.2681539058685303\n",
            "epoch 5 - batch 580 - loss 2.3195369243621826\n",
            "epoch 5 - batch 590 - loss 2.3374292850494385\n",
            "epoch 5 - batch 600 - loss 2.30269193649292\n",
            "epoch 5 - batch 610 - loss 2.3438613414764404\n",
            "epoch 5 training time: 222.37778234481812 sec\n",
            "evaluation of batch 0 took: 0.13068914413452148\n",
            "evaluation of batch 50 took: 0.13044953346252441\n",
            "evaluation of batch 100 took: 0.1354517936706543\n",
            "evaluation of batch 150 took: 0.12897157669067383\n",
            "evaluation of batch 200 took: 0.13080406188964844\n",
            "evaluation of batch 250 took: 0.13407206535339355\n",
            "evaluation of batch 300 took: 0.13082075119018555\n",
            "evaluation of batch 350 took: 0.1296076774597168\n",
            "evaluation of batch 400 took: 0.13730382919311523\n",
            "evaluation of batch 450 took: 0.14107489585876465\n",
            "evaluation of batch 500 took: 0.13320541381835938\n",
            "evaluation of batch 550 took: 0.14319372177124023\n",
            "evaluation of batch 600 took: 0.13395118713378906\n",
            "epoch 5 evaluation on training data time: 83.727059841156 sec\n",
            "evaluation of batch 0 took: 0.14302659034729004\n",
            "evaluation of batch 50 took: 0.13297605514526367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 evaluation on test data time: 20.745216846466064 sec\n",
            "epoch evaluation:  {'epoch': 5, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.3224921>, 'test_rouge_1_p': 0.27647212116380654, 'test_rouge_1_r': 0.18652651756725416, 'test_rouge_1_f1': 0.21175448590348178, 'test_rouge_2_p': 0.039848188920454546, 'test_rouge_2_r': 0.03554518398268399, 'test_rouge_2_f1': 0.03667894612433472, 'test_rouge_3_p': 0.004207885213744587, 'test_rouge_3_r': 0.0038730637851731586, 'test_rouge_3_f1': 0.003961560358044732, 'test_rouge_L_p': 0.27617269031192015, 'test_rouge_L_r': 0.1863383314973716, 'test_rouge_L_f1': 0.21152890972607435}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 - batch 10 - loss 2.3728880882263184\n",
            "epoch 6 - batch 20 - loss 2.4034371376037598\n",
            "epoch 6 - batch 30 - loss 2.282356023788452\n",
            "epoch 6 - batch 40 - loss 2.2355151176452637\n",
            "epoch 6 - batch 50 - loss 2.3599236011505127\n",
            "epoch 6 - batch 60 - loss 2.321645736694336\n",
            "epoch 6 - batch 70 - loss 2.3520872592926025\n",
            "epoch 6 - batch 80 - loss 2.2762866020202637\n",
            "epoch 6 - batch 90 - loss 2.3052399158477783\n",
            "epoch 6 - batch 100 - loss 2.3899552822113037\n",
            "epoch 6 - batch 110 - loss 2.281309127807617\n",
            "epoch 6 - batch 120 - loss 2.210811138153076\n",
            "epoch 6 - batch 130 - loss 2.2766451835632324\n",
            "epoch 6 - batch 140 - loss 2.300865650177002\n",
            "epoch 6 - batch 150 - loss 2.3283493518829346\n",
            "epoch 6 - batch 160 - loss 2.2724215984344482\n",
            "epoch 6 - batch 170 - loss 2.3418350219726562\n",
            "epoch 6 - batch 180 - loss 2.2927050590515137\n",
            "epoch 6 - batch 190 - loss 2.278414726257324\n",
            "epoch 6 - batch 200 - loss 2.1781952381134033\n",
            "epoch 6 - batch 210 - loss 2.337951898574829\n",
            "epoch 6 - batch 220 - loss 2.2676749229431152\n",
            "epoch 6 - batch 230 - loss 2.254373073577881\n",
            "epoch 6 - batch 240 - loss 2.230344295501709\n",
            "epoch 6 - batch 250 - loss 2.263017177581787\n",
            "epoch 6 - batch 260 - loss 2.2301526069641113\n",
            "epoch 6 - batch 270 - loss 2.2100930213928223\n",
            "epoch 6 - batch 280 - loss 2.2813520431518555\n",
            "epoch 6 - batch 290 - loss 2.2739970684051514\n",
            "epoch 6 - batch 300 - loss 2.3056070804595947\n",
            "epoch 6 - batch 310 - loss 2.3004555702209473\n",
            "epoch 6 - batch 320 - loss 2.2696101665496826\n",
            "epoch 6 - batch 330 - loss 2.2500903606414795\n",
            "epoch 6 - batch 340 - loss 2.271275281906128\n",
            "epoch 6 - batch 350 - loss 2.2654969692230225\n",
            "epoch 6 - batch 360 - loss 2.2290592193603516\n",
            "epoch 6 - batch 370 - loss 2.2150161266326904\n",
            "epoch 6 - batch 380 - loss 2.1999142169952393\n",
            "epoch 6 - batch 390 - loss 2.2476141452789307\n",
            "epoch 6 - batch 400 - loss 2.236830472946167\n",
            "epoch 6 - batch 410 - loss 2.179100513458252\n",
            "epoch 6 - batch 420 - loss 2.2278246879577637\n",
            "epoch 6 - batch 430 - loss 2.1882293224334717\n",
            "epoch 6 - batch 440 - loss 2.2404346466064453\n",
            "epoch 6 - batch 450 - loss 2.1378393173217773\n",
            "epoch 6 - batch 460 - loss 2.2144131660461426\n",
            "epoch 6 - batch 470 - loss 2.227475166320801\n",
            "epoch 6 - batch 480 - loss 2.2303245067596436\n",
            "epoch 6 - batch 490 - loss 2.1550557613372803\n",
            "epoch 6 - batch 500 - loss 2.1941988468170166\n",
            "epoch 6 - batch 510 - loss 2.3068175315856934\n",
            "epoch 6 - batch 520 - loss 2.252925395965576\n",
            "epoch 6 - batch 530 - loss 2.254448175430298\n",
            "epoch 6 - batch 540 - loss 2.20774507522583\n",
            "epoch 6 - batch 550 - loss 2.1610968112945557\n",
            "epoch 6 - batch 560 - loss 2.2317750453948975\n",
            "epoch 6 - batch 570 - loss 2.174208164215088\n",
            "epoch 6 - batch 580 - loss 2.2437551021575928\n",
            "epoch 6 - batch 590 - loss 2.2955322265625\n",
            "epoch 6 - batch 600 - loss 2.165379285812378\n",
            "epoch 6 - batch 610 - loss 2.1618916988372803\n",
            "epoch 6 training time: 222.66748356819153 sec\n",
            "evaluation of batch 0 took: 0.13045787811279297\n",
            "evaluation of batch 50 took: 0.13922524452209473\n",
            "evaluation of batch 100 took: 0.1387653350830078\n",
            "evaluation of batch 150 took: 0.14048099517822266\n",
            "evaluation of batch 200 took: 0.13789820671081543\n",
            "evaluation of batch 250 took: 0.1377882957458496\n",
            "evaluation of batch 300 took: 0.13826513290405273\n",
            "evaluation of batch 350 took: 0.1407628059387207\n",
            "evaluation of batch 400 took: 0.13977980613708496\n",
            "evaluation of batch 450 took: 0.1370835304260254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-724425d57f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/src/pipelines/baseline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mY_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/src/models/seq2seq/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, epochs, on_epoch_end)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mtrain_rouge_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {epoch} evaluation on training data time: {time.time() - start_time} sec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/src/evaluation/rouge.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         scores = list(map(\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         ))\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/src/identifier-suggestion-2020-07-01T15-59-13/src/evaluation/rouge.py\u001b[0m in \u001b[0;36mevaluate_batch\u001b[0;34m(self, batches)\u001b[0m\n\u001b[1;32m     75\u001b[0m             scores = self.scorer.score(\n\u001b[1;32m     76\u001b[0m                 \u001b[0mreference_method_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mpredicted_method_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             )\n\u001b[1;32m     79\u001b[0m             \u001b[0mbatch_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rouge_score/rouge_scorer.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, target, prediction)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mprediction_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rouge_score/tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, stemmer)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Only stem words more than 3 characters long.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;31m# One final check to drop any empty or invalid tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rouge_score/tokenize.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Only stem words more than 3 characters long.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;31m# One final check to drop any empty or invalid tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step4\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m'ize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_gt_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         ])\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'*d'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ends_double_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7fedf9c36488> (for post_run_cell):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_orig_post_run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_run_cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_orig_post_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shutting down system stats and metadata service\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensorboard_watchers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/meta.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# In case we never start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "00-colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpetkoff/identifier-suggestion/blob/master/notebooks/00-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeJd9knlVGS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utilities\n",
        "import os\n",
        "import shutil\n",
        "from subprocess import check_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntLDgQhocMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "6ec69b9d-feec-46b0-e5dc-51c9e4f2029f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 29 11:25:00 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_PqX9NDdEJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "cbf1012e-e54c-4d61-9812-62408d1ff6e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvcl4iohdWEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8b4724ee-1feb-4b82-8ffc-09a225a1f8ce"
      },
      "source": [
        "%env WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
        "\n",
        "# TODO: how can one read an environment variable?!?\n",
        "%cd '/content/gdrive/My Drive/src'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
            "/content/gdrive/My Drive/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNR4ZqljgeyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = check_output(['date', '-u', \"+%Y-%m-%dT%H-%M-%S\"]).decode('utf-8').strip()\n",
        "\n",
        "os.environ['PROJECT_DIR'] = os.path.join(\n",
        "    os.environ['WORKSPACE_DIR'],\n",
        "    f'identifier-suggestion-{timestamp}',\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoI39h1dEJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0dd7c1c8-e465-4c9d-a5f6-b373a717dcf4"
      },
      "source": [
        "!git clone https://github.com/antonpetkoff/identifier-suggestion.git --depth 1 \"${PROJECT_DIR}\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/src/identifier-suggestion-2020-06-29T11-25-25'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 112 (delta 1), reused 64 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (112/112), 961.76 KiB | 12.49 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mki1yqMgixt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.environ['PROJECT_DIR'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjKIGx4i2dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "8269d2df-9b7f-4cb3-d020-69343ac8f744"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/src/identifier-suggestion-2020-06-29T11-25-25\n",
            "total 69\n",
            "drwx------  4 root root  4096 Jun 29 11:25 data\n",
            "-rw-------  1 root root 35149 Jun 29 11:25 LICENSE\n",
            "drwx------  2 root root  4096 Jun 29 11:25 notebooks\n",
            "-rw-------  1 root root  9807 Jun 29 11:25 README.md\n",
            "drwx------  3 root root  4096 Jun 29 11:25 reports\n",
            "drwx------  2 root root  4096 Jun 29 11:25 requirements\n",
            "drwx------ 13 root root  4096 Jun 29 11:25 src\n",
            "drwx------  3 root root  4096 Jun 29 11:25 vscode-extension\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCr-lrn4SRhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "915562fa-9b4d-4252-fbdb-69e987766944"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7BkvE4dEJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a8c4d8f-a5f6-4a68-ec32-3c90842daf02"
      },
      "source": [
        "# Google Colab has standard libraries like numpy, pandas, matplotlib and TF (of course) pre-installed\n",
        "!pip install -r requirements/colab.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting javalang==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e0/12344443d66b9a84844171be90112892a371da6db09866741774b8bc0a2f/javalang-0.13.0-py3-none-any.whl\n",
            "Collecting pydash==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/fc/19b89da8a38a89f4403451a4ed2a05e16f804e6c71e4d5eaedb9d56366c3/pydash-4.8.0-py2.py3-none-any.whl (84kB)\n",
            "\r\u001b[K     |███▉                            | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 40kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 61kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.0MB/s \n",
            "\u001b[?25hCollecting python-dotenv==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/2a/07f87440444fdf2c5870a710b6770d766a1c7df9c827b0c90e807f1fb4c5/python_dotenv-0.13.0-py2.py3-none-any.whl\n",
            "Collecting wandb==0.8.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.2MB/s \n",
            "\u001b[?25hCollecting tables==3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 22.1MB/s \n",
            "\u001b[?25hCollecting rouge-score==0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting pathos==0.2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/ea/b2cf3a6561fc5deb64de8ae0af5e3e4e2db03ca588cb7415efce4a8de26e/pathos-0.2.6.zip (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from javalang==0.13.0->-r requirements/colab.txt (line 1)) (1.12.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (7.352.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.2MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (5.4.8)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/6b/939519d77c95a9b2c85b771e9dccbf9e69cb90016c7cd63887c26400dd7a/sentry_sdk-0.15.1-py2.py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 62.8MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 68.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (0.9.0)\n",
            "Collecting ppft>=1.6.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/fb/fa21f6e9aedc4823448473ed96e8eab64af1cb248c18165f045a90e1c6b4/ppft-1.6.6.2.zip (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from pathos==0.2.6->-r requirements/colab.txt (line 7)) (0.3.2)\n",
            "Collecting pox>=0.2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/0c/ec447fb0ed88bc1c09bf0dadf00e40ea05fda17e841d15bb351a52d9e192/pox-0.2.8.zip (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess>=0.70.10 in /usr/local/lib/python3.6/dist-packages (from pathos==0.2.6->-r requirements/colab.txt (line 7)) (0.70.10)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2020.6.20)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.9)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathos, watchdog, subprocess32, gql, ppft, pox, pathtools, graphql-core\n",
            "  Building wheel for pathos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathos: filename=pathos-0.2.6-cp36-none-any.whl size=77673 sha256=e6889cf7403de0583be032a2a39ff43f9adce0631fd549cbd9d48343d20b4b4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/e8/c8/04cdd0c4bc6fbce35f642fc004244228916daae74bb0f482da\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=8e0f62a1b26d697a5d4bfb22cb4ed46e0c6bb9f1d59388ec70414f76cf5446f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=bd8d8b468f87aafc461fc1dbe25efdfb45423c060ccfb4ae04352f0a453e5aa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=a8755b3c349f3b27c8d5af82a4970e3b6ddccff7d0965f67e317b63ff3163093\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for ppft (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ppft: filename=ppft-1.6.6.2-cp36-none-any.whl size=64743 sha256=81339d65731f7f2072b127864aef475fbd71a5f7341bb7a27229f0a981d89206\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/d2/2d/0ee21ede61786bb13247dbc69079373fd500c2bb0481913084\n",
            "  Building wheel for pox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pox: filename=pox-0.2.8-cp36-none-any.whl size=28290 sha256=114c768793cc1e6294baaefffd0368b8b817e4f78a610e53ef702bd0016b379c\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/ed/ce/a93103746b327e18bffaeb99ba0d57a88b392f31d719cea700\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=9de14365158a366576f2a2d582d6f2165b7ac69594c773b9381dcaf581b39a23\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=50fc857b1b260dd6229f2192447ed034d9df5a6123eda9a17689b4dd20e44fbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built pathos watchdog subprocess32 gql ppft pox pathtools graphql-core\n",
            "\u001b[31mERROR: rouge-score 0.0.4 has requirement six>=1.14.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: javalang, pydash, python-dotenv, docker-pycreds, pathtools, watchdog, subprocess32, graphql-core, gql, sentry-sdk, configparser, shortuuid, smmap, gitdb, GitPython, wandb, tables, rouge-score, ppft, pox, pathos\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "Successfully installed GitPython-3.1.3 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 javalang-0.13.0 pathos-0.2.6 pathtools-0.1.2 pox-0.2.8 ppft-1.6.6.2 pydash-4.8.0 python-dotenv-0.13.0 rouge-score-0.0.4 sentry-sdk-0.15.1 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tables-3.6.1 wandb-0.8.35 watchdog-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrqt3o4TP5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "749c82ad-04c4-4090-cd91-013f2a40749d"
      },
      "source": [
        "# provide secrets to the project, e.g. access to wandb\n",
        "shutil.copy(\n",
        "    os.path.join(os.environ['WORKSPACE_DIR'], 'secrets/.env'),\n",
        "    os.environ['PROJECT_DIR']\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/src/identifier-suggestion-2020-06-29T11-25-25/.env'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhbZ4muuveC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# monkey-patch (mock) os.symlink to be a noop, because wandb.save() uses it, but it is not supported by Google Colab Notebooks\n",
        "os.symlink = lambda *x: print('Executing mocked noop symlink with arguments', x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHkwwGMPts7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2a421a97-a3b3-49e0-834c-2174ab7586ad"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY3fkLJIm7ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93a9e7f6-dcd0-468d-a7ba-d1fb2f4d63a4"
      },
      "source": [
        "from argparse import Namespace\n",
        "from src.pipelines.baseline import run\n",
        "\n",
        "params = {\n",
        "  'dir_data': '../data/processed/subtoken/',\n",
        "  'file_checkpoint_dir': 'models/checkpoints/baseline/',\n",
        "  'dir_preprocessed_data': '../data/processed/seq2seq/',\n",
        "  'max_input_length': 128,\n",
        "  'max_output_length': 8,\n",
        "  'input_vocab_size': 5000,\n",
        "  'input_embedding_dim': 50,\n",
        "  'output_vocab_size': 5000,\n",
        "  'output_embedding_dim': 50,\n",
        "  'latent_dim': 320,\n",
        "  'learning_rate': 0.0001,\n",
        "  'epochs': 50,\n",
        "  'batch_size': 1024,\n",
        "  'random_seed': 1,\n",
        "  'eval_averaging': 'macro'\n",
        "}\n",
        "\n",
        "run(Namespace(**params))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing logger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/2h37hin6\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/2h37hin6</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Experiment parameters:  Namespace(batch_size=1024, dir_data='../data/processed/subtoken/', dir_preprocessed_data='../data/processed/seq2seq/', epochs=50, eval_averaging='macro', file_checkpoint_dir='models/checkpoints/baseline/', input_embedding_dim=50, input_vocab_size=5000, latent_dim=320, learning_rate=0.0001, max_input_length=128, max_output_length=8, output_embedding_dim=50, output_vocab_size=5000, random_seed=1)\n",
            "Loading preprocessed files...\n",
            "Loaded input vocabulary.\n",
            "Loaded output vocabulary.\n",
            "Loaded preprocessed files.\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "EncoderLSTM (LSTM)           multiple                  474880    \n",
            "=================================================================\n",
            "Total params: 724,880\n",
            "Trainable params: 724,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "DecoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "DecoderLSTM (LSTM)           multiple                  884480    \n",
            "_________________________________________________________________\n",
            "DenseOutput (Dense)          multiple                  1605000   \n",
            "_________________________________________________________________\n",
            "bahdanau_attention (Bahdanau multiple                  205761    \n",
            "=================================================================\n",
            "Total params: 2,945,241\n",
            "Trainable params: 2,945,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-1\n",
            "Executing mocked noop symlink with arguments ('/content/gdrive/My Drive/src/identifier-suggestion-2020-06-29T11-25-25/models/checkpoints/baseline/config.json', './reports/wandb/run-20200629_112600-2h37hin6/config.json')\n",
            "Done saving model\n",
            "Restored from models/checkpoints/baseline/ckpt-1\n",
            "epoch 1 - batch 10 - loss 4.408259868621826\n",
            "epoch 1 - batch 20 - loss 3.0870912075042725\n",
            "epoch 1 - batch 30 - loss 3.1153695583343506\n",
            "epoch 1 - batch 40 - loss 3.1828536987304688\n",
            "epoch 1 - batch 50 - loss 3.06841778755188\n",
            "epoch 1 - batch 60 - loss 3.1291606426239014\n",
            "epoch 1 - batch 70 - loss 3.215831756591797\n",
            "epoch 1 - batch 80 - loss 3.1878743171691895\n",
            "epoch 1 - batch 90 - loss 3.1343917846679688\n",
            "epoch 1 - batch 100 - loss 3.0858113765716553\n",
            "epoch 1 - batch 110 - loss 3.1536166667938232\n",
            "epoch 1 - batch 120 - loss 3.0320489406585693\n",
            "epoch 1 - batch 130 - loss 3.1714911460876465\n",
            "epoch 1 - batch 140 - loss 3.0913586616516113\n",
            "epoch 1 - batch 150 - loss 3.0708107948303223\n",
            "epoch 1 - batch 160 - loss 2.954719066619873\n",
            "epoch 1 - batch 170 - loss 3.0261034965515137\n",
            "epoch 1 - batch 180 - loss 3.020298480987549\n",
            "epoch 1 - batch 190 - loss 3.0154545307159424\n",
            "epoch 1 - batch 200 - loss 2.8704915046691895\n",
            "epoch 1 - batch 210 - loss 3.03546404838562\n",
            "epoch 1 - batch 220 - loss 2.987518787384033\n",
            "epoch 1 - batch 230 - loss 2.899250030517578\n",
            "epoch 1 - batch 240 - loss 2.921631336212158\n",
            "epoch 1 - batch 250 - loss 2.90614914894104\n",
            "epoch 1 - batch 260 - loss 2.9019463062286377\n",
            "epoch 1 - batch 270 - loss 2.7943034172058105\n",
            "epoch 1 - batch 280 - loss 2.868410348892212\n",
            "epoch 1 - batch 290 - loss 2.981353521347046\n",
            "epoch 1 - batch 300 - loss 2.9758529663085938\n",
            "epoch 1 - batch 310 - loss 2.859785795211792\n",
            "epoch 1 - batch 320 - loss 2.896019458770752\n",
            "epoch 1 - batch 330 - loss 2.890522003173828\n",
            "epoch 1 - batch 340 - loss 2.7593700885772705\n",
            "epoch 1 - batch 350 - loss 2.8528025150299072\n",
            "epoch 1 - batch 360 - loss 2.8862698078155518\n",
            "epoch 1 - batch 370 - loss 2.7590785026550293\n",
            "epoch 1 - batch 380 - loss 2.7456839084625244\n",
            "epoch 1 - batch 390 - loss 2.8651123046875\n",
            "epoch 1 - batch 400 - loss 2.7723159790039062\n",
            "epoch 1 - batch 410 - loss 2.776937961578369\n",
            "epoch 1 - batch 420 - loss 2.8070430755615234\n",
            "epoch 1 - batch 430 - loss 2.696237802505493\n",
            "epoch 1 - batch 440 - loss 2.861912250518799\n",
            "epoch 1 - batch 450 - loss 2.684831380844116\n",
            "epoch 1 - batch 460 - loss 2.8200526237487793\n",
            "epoch 1 - batch 470 - loss 2.8645055294036865\n",
            "epoch 1 - batch 480 - loss 2.8542463779449463\n",
            "epoch 1 - batch 490 - loss 2.776827573776245\n",
            "epoch 1 - batch 500 - loss 2.811004638671875\n",
            "epoch 1 - batch 510 - loss 2.812593698501587\n",
            "epoch 1 - batch 520 - loss 2.749652147293091\n",
            "epoch 1 - batch 530 - loss 2.8404810428619385\n",
            "epoch 1 - batch 540 - loss 2.7277698516845703\n",
            "epoch 1 - batch 550 - loss 2.7167904376983643\n",
            "epoch 1 - batch 560 - loss 2.794694185256958\n",
            "epoch 1 - batch 570 - loss 2.6892473697662354\n",
            "epoch 1 - batch 580 - loss 2.7937562465667725\n",
            "epoch 1 - batch 590 - loss 2.75030517578125\n",
            "epoch 1 - batch 600 - loss 2.7627787590026855\n",
            "epoch 1 - batch 610 - loss 2.710416555404663\n",
            "epoch 1 training time: 223.54421377182007 sec\n",
            "evaluation of batch 0 took: 0.29165172576904297\n",
            "evaluation of batch 50 took: 0.10315608978271484\n",
            "evaluation of batch 100 took: 0.10576796531677246\n",
            "evaluation of batch 150 took: 0.1105339527130127\n",
            "evaluation of batch 200 took: 0.11596131324768066\n",
            "evaluation of batch 250 took: 0.11387395858764648\n",
            "evaluation of batch 300 took: 0.1155693531036377\n",
            "evaluation of batch 350 took: 0.11754274368286133\n",
            "evaluation of batch 400 took: 0.12253856658935547\n",
            "evaluation of batch 450 took: 0.13714313507080078\n",
            "evaluation of batch 500 took: 0.12769722938537598\n",
            "evaluation of batch 550 took: 0.13134980201721191\n",
            "evaluation of batch 600 took: 0.13199543952941895\n",
            "epoch 1 evaluation on training data time: 74.00488638877869 sec\n",
            "evaluation of batch 0 took: 0.13593673706054688\n",
            "evaluation of batch 50 took: 0.12468481063842773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 evaluation on test data time: 20.33506417274475 sec\n",
            "epoch evaluation:  {'epoch': 1, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.7514312>, 'test_rouge_1_p': 0.20947096523268396, 'test_rouge_1_r': 0.09218176261982063, 'test_rouge_1_f1': 0.12280970461266753, 'test_rouge_2_p': 0.005089962121212122, 'test_rouge_2_r': 0.0019047196293290037, 'test_rouge_2_f1': 0.002560663233096267, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.2094646239177489, 'test_rouge_L_r': 0.0921792260938466, 'test_rouge_L_f1': 0.12280608100413322}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 - batch 10 - loss 2.7413787841796875\n",
            "epoch 2 - batch 20 - loss 2.8382728099823\n",
            "epoch 2 - batch 30 - loss 2.705202341079712\n",
            "epoch 2 - batch 40 - loss 2.8303396701812744\n",
            "epoch 2 - batch 50 - loss 2.8204433917999268\n",
            "epoch 2 - batch 60 - loss 2.747753143310547\n",
            "epoch 2 - batch 70 - loss 2.701320171356201\n",
            "epoch 2 - batch 80 - loss 2.7496707439422607\n",
            "epoch 2 - batch 90 - loss 2.8107378482818604\n",
            "epoch 2 - batch 100 - loss 2.737060070037842\n",
            "epoch 2 - batch 110 - loss 2.785961627960205\n",
            "epoch 2 - batch 120 - loss 2.807209014892578\n",
            "epoch 2 - batch 130 - loss 2.6590187549591064\n",
            "epoch 2 - batch 140 - loss 2.6760025024414062\n",
            "epoch 2 - batch 150 - loss 2.775890350341797\n",
            "epoch 2 - batch 160 - loss 2.6406872272491455\n",
            "epoch 2 - batch 170 - loss 2.6642134189605713\n",
            "epoch 2 - batch 180 - loss 2.7504327297210693\n",
            "epoch 2 - batch 190 - loss 2.683290719985962\n",
            "epoch 2 - batch 200 - loss 2.726677417755127\n",
            "epoch 2 - batch 210 - loss 2.7595503330230713\n",
            "epoch 2 - batch 220 - loss 2.7281992435455322\n",
            "epoch 2 - batch 230 - loss 2.6785659790039062\n",
            "epoch 2 - batch 240 - loss 2.6449508666992188\n",
            "epoch 2 - batch 250 - loss 2.663818120956421\n",
            "epoch 2 - batch 260 - loss 2.6263177394866943\n",
            "epoch 2 - batch 270 - loss 2.6817476749420166\n",
            "epoch 2 - batch 280 - loss 2.6435370445251465\n",
            "epoch 2 - batch 290 - loss 2.6824753284454346\n",
            "epoch 2 - batch 300 - loss 2.7997829914093018\n",
            "epoch 2 - batch 310 - loss 2.656900644302368\n",
            "epoch 2 - batch 320 - loss 2.663222074508667\n",
            "epoch 2 - batch 330 - loss 2.717859983444214\n",
            "epoch 2 - batch 340 - loss 2.705212116241455\n",
            "epoch 2 - batch 350 - loss 2.701692819595337\n",
            "epoch 2 - batch 360 - loss 2.6522793769836426\n",
            "epoch 2 - batch 370 - loss 2.6482722759246826\n",
            "epoch 2 - batch 380 - loss 2.556509494781494\n",
            "epoch 2 - batch 390 - loss 2.6431725025177\n",
            "epoch 2 - batch 400 - loss 2.5333919525146484\n",
            "epoch 2 - batch 410 - loss 2.6250994205474854\n",
            "epoch 2 - batch 420 - loss 2.6968154907226562\n",
            "epoch 2 - batch 430 - loss 2.6639480590820312\n",
            "epoch 2 - batch 440 - loss 2.5810868740081787\n",
            "epoch 2 - batch 450 - loss 2.578242778778076\n",
            "epoch 2 - batch 460 - loss 2.666372537612915\n",
            "epoch 2 - batch 470 - loss 2.7396328449249268\n",
            "epoch 2 - batch 480 - loss 2.6207823753356934\n",
            "epoch 2 - batch 490 - loss 2.5900371074676514\n",
            "epoch 2 - batch 500 - loss 2.6091558933258057\n",
            "epoch 2 - batch 510 - loss 2.708700656890869\n",
            "epoch 2 - batch 520 - loss 2.5846145153045654\n",
            "epoch 2 - batch 530 - loss 2.6733343601226807\n",
            "epoch 2 - batch 540 - loss 2.589860677719116\n",
            "epoch 2 - batch 550 - loss 2.600324869155884\n",
            "epoch 2 - batch 560 - loss 2.588625192642212\n",
            "epoch 2 - batch 570 - loss 2.64170503616333\n",
            "epoch 2 - batch 580 - loss 2.55226731300354\n",
            "epoch 2 - batch 590 - loss 2.6435534954071045\n",
            "epoch 2 - batch 600 - loss 2.5430848598480225\n",
            "epoch 2 - batch 610 - loss 2.5630462169647217\n",
            "epoch 2 training time: 216.77178382873535 sec\n",
            "evaluation of batch 0 took: 0.13039445877075195\n",
            "evaluation of batch 50 took: 0.12449145317077637\n",
            "evaluation of batch 100 took: 0.13625621795654297\n",
            "evaluation of batch 150 took: 0.12564563751220703\n",
            "evaluation of batch 200 took: 0.1199333667755127\n",
            "evaluation of batch 250 took: 0.12082409858703613\n",
            "evaluation of batch 300 took: 0.12964797019958496\n",
            "evaluation of batch 350 took: 0.13072538375854492\n",
            "evaluation of batch 400 took: 0.12889957427978516\n",
            "evaluation of batch 450 took: 0.12216997146606445\n",
            "evaluation of batch 500 took: 0.12091708183288574\n",
            "evaluation of batch 550 took: 0.1239769458770752\n",
            "evaluation of batch 600 took: 0.12261772155761719\n",
            "epoch 2 evaluation on training data time: 77.5037567615509 sec\n",
            "evaluation of batch 0 took: 0.1379077434539795\n",
            "evaluation of batch 50 took: 0.12356185913085938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 evaluation on test data time: 20.033992290496826 sec\n",
            "epoch evaluation:  {'epoch': 2, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.5725853>, 'test_rouge_1_p': 0.21934880131029694, 'test_rouge_1_r': 0.13988647838396714, 'test_rouge_1_f1': 0.16353522349933203, 'test_rouge_2_p': 0.008656106263528139, 'test_rouge_2_r': 0.006003111471861473, 'test_rouge_2_f1': 0.006743870391648394, 'test_rouge_3_p': 0.00010146103896103897, 'test_rouge_3_r': 4.7771239177489175e-05, 'test_rouge_3_f1': 5.918560606060606e-05, 'test_rouge_L_p': 0.21927798996018869, 'test_rouge_L_r': 0.139847554788961, 'test_rouge_L_f1': 0.16348565207001736}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 - batch 10 - loss 2.609348773956299\n",
            "epoch 3 - batch 20 - loss 2.567309856414795\n",
            "epoch 3 - batch 30 - loss 2.52130126953125\n",
            "epoch 3 - batch 40 - loss 2.5462117195129395\n",
            "epoch 3 - batch 50 - loss 2.5885977745056152\n",
            "epoch 3 - batch 60 - loss 2.643817901611328\n",
            "epoch 3 - batch 70 - loss 2.5536060333251953\n",
            "epoch 3 - batch 80 - loss 2.504415273666382\n",
            "epoch 3 - batch 90 - loss 2.5260612964630127\n",
            "epoch 3 - batch 100 - loss 2.493934392929077\n",
            "epoch 3 - batch 110 - loss 2.5743815898895264\n",
            "epoch 3 - batch 120 - loss 2.547872304916382\n",
            "epoch 3 - batch 130 - loss 2.51015305519104\n",
            "epoch 3 - batch 140 - loss 2.517061710357666\n",
            "epoch 3 - batch 150 - loss 2.5294196605682373\n",
            "epoch 3 - batch 160 - loss 2.5752923488616943\n",
            "epoch 3 - batch 170 - loss 2.5317535400390625\n",
            "epoch 3 - batch 180 - loss 2.5306942462921143\n",
            "epoch 3 - batch 190 - loss 2.509669542312622\n",
            "epoch 3 - batch 200 - loss 2.4962198734283447\n",
            "epoch 3 - batch 210 - loss 2.4519689083099365\n",
            "epoch 3 - batch 220 - loss 2.4943766593933105\n",
            "epoch 3 - batch 230 - loss 2.492823362350464\n",
            "epoch 3 - batch 240 - loss 2.366520404815674\n",
            "epoch 3 - batch 250 - loss 2.5773770809173584\n",
            "epoch 3 - batch 260 - loss 2.4642574787139893\n",
            "epoch 3 - batch 270 - loss 2.477057695388794\n",
            "epoch 3 - batch 280 - loss 2.417412281036377\n",
            "epoch 3 - batch 290 - loss 2.5300586223602295\n",
            "epoch 3 - batch 300 - loss 2.422466278076172\n",
            "epoch 3 - batch 310 - loss 2.410452127456665\n",
            "epoch 3 - batch 320 - loss 2.3847177028656006\n",
            "epoch 3 - batch 330 - loss 2.4327027797698975\n",
            "epoch 3 - batch 340 - loss 2.4415252208709717\n",
            "epoch 3 - batch 350 - loss 2.463654041290283\n",
            "epoch 3 - batch 360 - loss 2.432377338409424\n",
            "epoch 3 - batch 370 - loss 2.401907205581665\n",
            "epoch 3 - batch 380 - loss 2.3862719535827637\n",
            "epoch 3 - batch 390 - loss 2.3853726387023926\n",
            "epoch 3 - batch 400 - loss 2.38663911819458\n",
            "epoch 3 - batch 410 - loss 2.452526569366455\n",
            "epoch 3 - batch 420 - loss 2.477332830429077\n",
            "epoch 3 - batch 430 - loss 2.3501455783843994\n",
            "epoch 3 - batch 440 - loss 2.4246842861175537\n",
            "epoch 3 - batch 450 - loss 2.3605902194976807\n",
            "epoch 3 - batch 460 - loss 2.3160481452941895\n",
            "epoch 3 - batch 470 - loss 2.3746721744537354\n",
            "epoch 3 - batch 480 - loss 2.40484881401062\n",
            "epoch 3 - batch 490 - loss 2.3423869609832764\n",
            "epoch 3 - batch 500 - loss 2.3694779872894287\n",
            "epoch 3 - batch 510 - loss 2.3757667541503906\n",
            "epoch 3 - batch 520 - loss 2.3181378841400146\n",
            "epoch 3 - batch 530 - loss 2.424142599105835\n",
            "epoch 3 - batch 540 - loss 2.377344846725464\n",
            "epoch 3 - batch 550 - loss 2.286165714263916\n",
            "epoch 3 - batch 560 - loss 2.31837797164917\n",
            "epoch 3 - batch 570 - loss 2.330993413925171\n",
            "epoch 3 - batch 580 - loss 2.387131929397583\n",
            "epoch 3 - batch 590 - loss 2.3604960441589355\n",
            "epoch 3 - batch 600 - loss 2.285344123840332\n",
            "epoch 3 - batch 610 - loss 2.3247344493865967\n",
            "epoch 3 training time: 214.17413425445557 sec\n",
            "evaluation of batch 0 took: 0.12846088409423828\n",
            "evaluation of batch 50 took: 0.1280510425567627\n",
            "evaluation of batch 100 took: 0.12654781341552734\n",
            "evaluation of batch 150 took: 0.12507152557373047\n",
            "evaluation of batch 200 took: 0.12640666961669922\n",
            "evaluation of batch 250 took: 0.12738370895385742\n",
            "evaluation of batch 300 took: 0.1285872459411621\n",
            "evaluation of batch 350 took: 0.12657809257507324\n",
            "evaluation of batch 400 took: 0.12888550758361816\n",
            "evaluation of batch 450 took: 0.13425374031066895\n",
            "evaluation of batch 500 took: 0.13337254524230957\n",
            "evaluation of batch 550 took: 0.13609981536865234\n",
            "evaluation of batch 600 took: 0.133988618850708\n",
            "epoch 3 evaluation on training data time: 79.46548295021057 sec\n",
            "evaluation of batch 0 took: 0.13501858711242676\n",
            "evaluation of batch 50 took: 0.12897753715515137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 evaluation on test data time: 20.32614827156067 sec\n",
            "epoch evaluation:  {'epoch': 3, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.3352582>, 'test_rouge_1_p': 0.26667642021297167, 'test_rouge_1_r': 0.18093516881184296, 'test_rouge_1_f1': 0.20566658790456527, 'test_rouge_2_p': 0.034394869453463206, 'test_rouge_2_r': 0.030503838609307356, 'test_rouge_2_f1': 0.03157373739569863, 'test_rouge_3_p': 0.0011923785849567096, 'test_rouge_3_r': 0.0008962391774891775, 'test_rouge_3_f1': 0.0009723752190270051, 'test_rouge_L_p': 0.26646449950718926, 'test_rouge_L_r': 0.1808019106079932, 'test_rouge_L_f1': 0.2055054077838427}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 saved checkpoint: models/checkpoints/baseline/ckpt-2\n",
            "epoch 4 - batch 10 - loss 2.419205904006958\n",
            "epoch 4 - batch 20 - loss 2.4232499599456787\n",
            "epoch 4 - batch 30 - loss 2.154258966445923\n",
            "epoch 4 - batch 40 - loss 2.2709882259368896\n",
            "epoch 4 - batch 50 - loss 2.326040506362915\n",
            "epoch 4 - batch 60 - loss 2.3757855892181396\n",
            "epoch 4 - batch 70 - loss 2.379211187362671\n",
            "epoch 4 - batch 80 - loss 2.306725263595581\n",
            "epoch 4 - batch 90 - loss 2.222400188446045\n",
            "epoch 4 - batch 100 - loss 2.431427240371704\n",
            "epoch 4 - batch 110 - loss 2.2878363132476807\n",
            "epoch 4 - batch 120 - loss 2.3119664192199707\n",
            "epoch 4 - batch 130 - loss 2.2578423023223877\n",
            "epoch 4 - batch 140 - loss 2.3677685260772705\n",
            "epoch 4 - batch 150 - loss 2.2147843837738037\n",
            "epoch 4 - batch 160 - loss 2.2763478755950928\n",
            "epoch 4 - batch 170 - loss 2.260308027267456\n",
            "epoch 4 - batch 180 - loss 2.377349853515625\n",
            "epoch 4 - batch 190 - loss 2.3596322536468506\n",
            "epoch 4 - batch 200 - loss 2.341935873031616\n",
            "epoch 4 - batch 210 - loss 2.1947457790374756\n",
            "epoch 4 - batch 220 - loss 2.223919630050659\n",
            "epoch 4 - batch 230 - loss 2.292450189590454\n",
            "epoch 4 - batch 240 - loss 2.210909128189087\n",
            "epoch 4 - batch 250 - loss 2.1953155994415283\n",
            "epoch 4 - batch 260 - loss 2.2402918338775635\n",
            "epoch 4 - batch 270 - loss 2.31022310256958\n",
            "epoch 4 - batch 280 - loss 2.226071357727051\n",
            "epoch 4 - batch 290 - loss 2.235825777053833\n",
            "epoch 4 - batch 300 - loss 2.212937355041504\n",
            "epoch 4 - batch 310 - loss 2.222113609313965\n",
            "epoch 4 - batch 320 - loss 2.3087427616119385\n",
            "epoch 4 - batch 330 - loss 2.2580301761627197\n",
            "epoch 4 - batch 340 - loss 2.2279717922210693\n",
            "epoch 4 - batch 350 - loss 2.206894874572754\n",
            "epoch 4 - batch 360 - loss 2.1713714599609375\n",
            "epoch 4 - batch 370 - loss 2.1988277435302734\n",
            "epoch 4 - batch 380 - loss 2.1109707355499268\n",
            "epoch 4 - batch 390 - loss 2.2015302181243896\n",
            "epoch 4 - batch 400 - loss 2.244218111038208\n",
            "epoch 4 - batch 410 - loss 2.1765902042388916\n",
            "epoch 4 - batch 420 - loss 2.16330623626709\n",
            "epoch 4 - batch 430 - loss 2.1162269115448\n",
            "epoch 4 - batch 440 - loss 2.227620840072632\n",
            "epoch 4 - batch 450 - loss 2.139451265335083\n",
            "epoch 4 - batch 460 - loss 2.1526732444763184\n",
            "epoch 4 - batch 470 - loss 2.0654842853546143\n",
            "epoch 4 - batch 480 - loss 2.193223476409912\n",
            "epoch 4 - batch 490 - loss 2.0805811882019043\n",
            "epoch 4 - batch 500 - loss 2.1302883625030518\n",
            "epoch 4 - batch 510 - loss 2.1496617794036865\n",
            "epoch 4 - batch 520 - loss 2.286175489425659\n",
            "epoch 4 - batch 530 - loss 2.1446259021759033\n",
            "epoch 4 - batch 540 - loss 2.0723519325256348\n",
            "epoch 4 - batch 550 - loss 2.1268107891082764\n",
            "epoch 4 - batch 560 - loss 2.1342544555664062\n",
            "epoch 4 - batch 570 - loss 2.128181219100952\n",
            "epoch 4 - batch 580 - loss 2.1411359310150146\n",
            "epoch 4 - batch 590 - loss 2.1099841594696045\n",
            "epoch 4 - batch 600 - loss 2.1070427894592285\n",
            "epoch 4 - batch 610 - loss 2.109987258911133\n",
            "epoch 4 training time: 215.6707775592804 sec\n",
            "evaluation of batch 0 took: 0.13015460968017578\n",
            "evaluation of batch 50 took: 0.13124537467956543\n",
            "evaluation of batch 100 took: 0.12943124771118164\n",
            "evaluation of batch 150 took: 0.130173921585083\n",
            "evaluation of batch 200 took: 0.13129448890686035\n",
            "evaluation of batch 250 took: 0.13249635696411133\n",
            "evaluation of batch 300 took: 0.13498139381408691\n",
            "evaluation of batch 350 took: 0.12770771980285645\n",
            "evaluation of batch 400 took: 0.13239574432373047\n",
            "evaluation of batch 450 took: 0.1385326385498047\n",
            "evaluation of batch 500 took: 0.13433408737182617\n",
            "evaluation of batch 550 took: 0.13118982315063477\n",
            "evaluation of batch 600 took: 0.13419198989868164\n",
            "epoch 4 evaluation on training data time: 83.09339165687561 sec\n",
            "evaluation of batch 0 took: 0.1432332992553711\n",
            "evaluation of batch 50 took: 0.1337881088256836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 evaluation on test data time: 20.813167095184326 sec\n",
            "epoch evaluation:  {'epoch': 4, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.1380136>, 'test_rouge_1_p': 0.31695293174087813, 'test_rouge_1_r': 0.2319071414076994, 'test_rouge_1_f1': 0.2571202048734773, 'test_rouge_2_p': 0.06076945515422077, 'test_rouge_2_r': 0.05350082859848485, 'test_rouge_2_f1': 0.05550560466094152, 'test_rouge_3_p': 0.007016876352813854, 'test_rouge_3_r': 0.006123385078463202, 'test_rouge_3_f1': 0.00634949825100495, 'test_rouge_L_p': 0.3164954209666821, 'test_rouge_L_r': 0.23162516427025356, 'test_rouge_L_f1': 0.25677953020498206}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 - batch 10 - loss 2.08194637298584\n",
            "epoch 5 - batch 20 - loss 2.126910448074341\n",
            "epoch 5 - batch 30 - loss 2.0606770515441895\n",
            "epoch 5 - batch 40 - loss 2.057253122329712\n",
            "epoch 5 - batch 50 - loss 2.1229918003082275\n",
            "epoch 5 - batch 60 - loss 2.123793125152588\n",
            "epoch 5 - batch 70 - loss 2.1124207973480225\n",
            "epoch 5 - batch 80 - loss 2.1179516315460205\n",
            "epoch 5 - batch 90 - loss 2.1431660652160645\n",
            "epoch 5 - batch 100 - loss 2.0946285724639893\n",
            "epoch 5 - batch 110 - loss 2.115715503692627\n",
            "epoch 5 - batch 120 - loss 2.1950535774230957\n",
            "epoch 5 - batch 130 - loss 2.0040128231048584\n",
            "epoch 5 - batch 140 - loss 2.089099407196045\n",
            "epoch 5 - batch 150 - loss 2.0968596935272217\n",
            "epoch 5 - batch 160 - loss 2.1094746589660645\n",
            "epoch 5 - batch 170 - loss 2.027270793914795\n",
            "epoch 5 - batch 180 - loss 2.043839931488037\n",
            "epoch 5 - batch 190 - loss 2.145970582962036\n",
            "epoch 5 - batch 200 - loss 2.0676612854003906\n",
            "epoch 5 - batch 210 - loss 2.0679221153259277\n",
            "epoch 5 - batch 220 - loss 2.058824062347412\n",
            "epoch 5 - batch 230 - loss 1.9966689348220825\n",
            "epoch 5 - batch 240 - loss 1.9365640878677368\n",
            "epoch 5 - batch 250 - loss 2.0533692836761475\n",
            "epoch 5 - batch 260 - loss 1.959671974182129\n",
            "epoch 5 - batch 270 - loss 2.0875771045684814\n",
            "epoch 5 - batch 280 - loss 2.0747568607330322\n",
            "epoch 5 - batch 290 - loss 2.037418842315674\n",
            "epoch 5 - batch 300 - loss 2.0691497325897217\n",
            "epoch 5 - batch 310 - loss 2.0911614894866943\n",
            "epoch 5 - batch 320 - loss 2.042825937271118\n",
            "epoch 5 - batch 330 - loss 2.006603717803955\n",
            "epoch 5 - batch 340 - loss 1.9948227405548096\n",
            "epoch 5 - batch 350 - loss 1.9924427270889282\n",
            "epoch 5 - batch 360 - loss 1.9423972368240356\n",
            "epoch 5 - batch 370 - loss 2.039179563522339\n",
            "epoch 5 - batch 380 - loss 2.0132064819335938\n",
            "epoch 5 - batch 390 - loss 2.0772461891174316\n",
            "epoch 5 - batch 400 - loss 1.9732599258422852\n",
            "epoch 5 - batch 410 - loss 2.054918050765991\n",
            "epoch 5 - batch 420 - loss 1.9382888078689575\n",
            "epoch 5 - batch 430 - loss 1.928191900253296\n",
            "epoch 5 - batch 440 - loss 2.051306962966919\n",
            "epoch 5 - batch 450 - loss 1.9977493286132812\n",
            "epoch 5 - batch 460 - loss 2.011963129043579\n",
            "epoch 5 - batch 470 - loss 2.0324225425720215\n",
            "epoch 5 - batch 480 - loss 2.008005142211914\n",
            "epoch 5 - batch 490 - loss 1.8747366666793823\n",
            "epoch 5 - batch 500 - loss 1.937394142150879\n",
            "epoch 5 - batch 510 - loss 1.9948749542236328\n",
            "epoch 5 - batch 520 - loss 1.9224660396575928\n",
            "epoch 5 - batch 530 - loss 1.9560285806655884\n",
            "epoch 5 - batch 540 - loss 1.955431342124939\n",
            "epoch 5 - batch 550 - loss 1.964241862297058\n",
            "epoch 5 - batch 560 - loss 1.9483312368392944\n",
            "epoch 5 - batch 570 - loss 1.8874891996383667\n",
            "epoch 5 - batch 580 - loss 1.9569098949432373\n",
            "epoch 5 - batch 590 - loss 1.9611212015151978\n",
            "epoch 5 - batch 600 - loss 1.9428414106369019\n",
            "epoch 5 - batch 610 - loss 1.9700037240982056\n",
            "epoch 5 training time: 215.9334213733673 sec\n",
            "evaluation of batch 0 took: 0.13332891464233398\n",
            "evaluation of batch 50 took: 0.1319742202758789\n",
            "evaluation of batch 100 took: 0.13843822479248047\n",
            "evaluation of batch 150 took: 0.1335294246673584\n",
            "evaluation of batch 200 took: 0.13449740409851074\n",
            "evaluation of batch 250 took: 0.13994550704956055\n",
            "evaluation of batch 300 took: 0.137772798538208\n",
            "evaluation of batch 350 took: 0.1403505802154541\n",
            "evaluation of batch 400 took: 0.13949060440063477\n",
            "evaluation of batch 450 took: 0.13821864128112793\n",
            "evaluation of batch 500 took: 0.1405777931213379\n",
            "evaluation of batch 550 took: 0.14077019691467285\n",
            "evaluation of batch 600 took: 0.14023065567016602\n",
            "epoch 5 evaluation on training data time: 85.6715099811554 sec\n",
            "evaluation of batch 0 took: 0.14394712448120117\n",
            "evaluation of batch 50 took: 0.1452643871307373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 evaluation on test data time: 21.083606958389282 sec\n",
            "epoch evaluation:  {'epoch': 5, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.9636248>, 'test_rouge_1_p': 0.3634808806335032, 'test_rouge_1_r': 0.28327559717068623, 'test_rouge_1_f1': 0.3072801688892141, 'test_rouge_2_p': 0.0910171046401515, 'test_rouge_2_r': 0.08098577854437229, 'test_rouge_2_f1': 0.08370518694306237, 'test_rouge_3_p': 0.014716289569805197, 'test_rouge_3_r': 0.013062474634740267, 'test_rouge_3_f1': 0.013488902900175223, 'test_rouge_L_p': 0.3625679728567563, 'test_rouge_L_r': 0.2827026744646722, 'test_rouge_L_f1': 0.30658962369532083}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 - batch 10 - loss 2.027817964553833\n",
            "epoch 6 - batch 20 - loss 2.0440423488616943\n",
            "epoch 6 - batch 30 - loss 1.9113143682479858\n",
            "epoch 6 - batch 40 - loss 1.8644200563430786\n",
            "epoch 6 - batch 50 - loss 1.9796472787857056\n",
            "epoch 6 - batch 60 - loss 1.939319372177124\n",
            "epoch 6 - batch 70 - loss 1.9668878316879272\n",
            "epoch 6 - batch 80 - loss 1.881028175354004\n",
            "epoch 6 - batch 90 - loss 1.9114255905151367\n",
            "epoch 6 - batch 100 - loss 2.0324275493621826\n",
            "epoch 6 - batch 110 - loss 1.894841194152832\n",
            "epoch 6 - batch 120 - loss 1.8239896297454834\n",
            "epoch 6 - batch 130 - loss 1.9099276065826416\n",
            "epoch 6 - batch 140 - loss 1.9154123067855835\n",
            "epoch 6 - batch 150 - loss 1.968060851097107\n",
            "epoch 6 - batch 160 - loss 1.8896840810775757\n",
            "epoch 6 - batch 170 - loss 1.9770091772079468\n",
            "epoch 6 - batch 180 - loss 1.9238946437835693\n",
            "epoch 6 - batch 190 - loss 1.9087051153182983\n",
            "epoch 6 - batch 200 - loss 1.8092546463012695\n",
            "epoch 6 - batch 210 - loss 1.94648277759552\n",
            "epoch 6 - batch 220 - loss 1.8906280994415283\n",
            "epoch 6 - batch 230 - loss 1.8834612369537354\n",
            "epoch 6 - batch 240 - loss 1.855216145515442\n",
            "epoch 6 - batch 250 - loss 1.8834800720214844\n",
            "epoch 6 - batch 260 - loss 1.8654148578643799\n",
            "epoch 6 - batch 270 - loss 1.8153076171875\n",
            "epoch 6 - batch 280 - loss 1.9102165699005127\n",
            "epoch 6 - batch 290 - loss 1.8912094831466675\n",
            "epoch 6 - batch 300 - loss 1.9255369901657104\n",
            "epoch 6 - batch 310 - loss 1.9343585968017578\n",
            "epoch 6 - batch 320 - loss 1.8832148313522339\n",
            "epoch 6 - batch 330 - loss 1.8782602548599243\n",
            "epoch 6 - batch 340 - loss 1.8786860704421997\n",
            "epoch 6 - batch 350 - loss 1.8938039541244507\n",
            "epoch 6 - batch 360 - loss 1.8532698154449463\n",
            "epoch 6 - batch 370 - loss 1.8454482555389404\n",
            "epoch 6 - batch 380 - loss 1.8148326873779297\n",
            "epoch 6 - batch 390 - loss 1.849054217338562\n",
            "epoch 6 - batch 400 - loss 1.8802357912063599\n",
            "epoch 6 - batch 410 - loss 1.7859429121017456\n",
            "epoch 6 - batch 420 - loss 1.8474723100662231\n",
            "epoch 6 - batch 430 - loss 1.8086111545562744\n",
            "epoch 6 - batch 440 - loss 1.8354804515838623\n",
            "epoch 6 - batch 450 - loss 1.748496413230896\n",
            "epoch 6 - batch 460 - loss 1.8271293640136719\n",
            "epoch 6 - batch 470 - loss 1.8186959028244019\n",
            "epoch 6 - batch 480 - loss 1.8845402002334595\n",
            "epoch 6 - batch 490 - loss 1.7803938388824463\n",
            "epoch 6 - batch 500 - loss 1.8150509595870972\n",
            "epoch 6 - batch 510 - loss 1.9283138513565063\n",
            "epoch 6 - batch 520 - loss 1.864004135131836\n",
            "epoch 6 - batch 530 - loss 1.8727596998214722\n",
            "epoch 6 - batch 540 - loss 1.827502965927124\n",
            "epoch 6 - batch 550 - loss 1.7829424142837524\n",
            "epoch 6 - batch 560 - loss 1.8513597249984741\n",
            "epoch 6 - batch 570 - loss 1.7873390913009644\n",
            "epoch 6 - batch 580 - loss 1.8529222011566162\n",
            "epoch 6 - batch 590 - loss 1.9059604406356812\n",
            "epoch 6 - batch 600 - loss 1.79755437374115\n",
            "epoch 6 - batch 610 - loss 1.7873281240463257\n",
            "epoch 6 training time: 215.7528326511383 sec\n",
            "evaluation of batch 0 took: 0.14205527305603027\n",
            "evaluation of batch 50 took: 0.1418149471282959\n",
            "evaluation of batch 100 took: 0.1376667022705078\n",
            "evaluation of batch 150 took: 0.14182400703430176\n",
            "evaluation of batch 200 took: 0.1364607810974121\n",
            "evaluation of batch 250 took: 0.14493513107299805\n",
            "evaluation of batch 300 took: 0.14494776725769043\n",
            "evaluation of batch 350 took: 0.13662171363830566\n",
            "evaluation of batch 400 took: 0.14631175994873047\n",
            "evaluation of batch 450 took: 0.13762855529785156\n",
            "evaluation of batch 500 took: 0.14367151260375977\n",
            "evaluation of batch 550 took: 0.14574909210205078\n",
            "evaluation of batch 600 took: 0.1409621238708496\n",
            "epoch 6 evaluation on training data time: 88.21945691108704 sec\n",
            "evaluation of batch 0 took: 0.1513056755065918\n",
            "evaluation of batch 50 took: 0.14342117309570312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 evaluation on test data time: 21.44072699546814 sec\n",
            "epoch evaluation:  {'epoch': 6, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.8311441>, 'test_rouge_1_p': 0.3958792323747679, 'test_rouge_1_r': 0.3312522949520715, 'test_rouge_1_f1': 0.35057146594217914, 'test_rouge_2_p': 0.12148416362283551, 'test_rouge_2_r': 0.11123659868777053, 'test_rouge_2_f1': 0.11402812879197824, 'test_rouge_3_p': 0.026035325351731597, 'test_rouge_3_r': 0.023823051948051947, 'test_rouge_3_f1': 0.024437012020717373, 'test_rouge_L_p': 0.3945890465561225, 'test_rouge_L_r': 0.33037924686920184, 'test_rouge_L_f1': 0.3495543528835431}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 saved checkpoint: models/checkpoints/baseline/ckpt-3\n",
            "epoch 7 - batch 10 - loss 1.9123518466949463\n",
            "epoch 7 - batch 20 - loss 1.7989355325698853\n",
            "epoch 7 - batch 30 - loss 1.7393721342086792\n",
            "epoch 7 - batch 40 - loss 1.7835031747817993\n",
            "epoch 7 - batch 50 - loss 1.7789241075515747\n",
            "epoch 7 - batch 60 - loss 1.7820197343826294\n",
            "epoch 7 - batch 70 - loss 1.8033052682876587\n",
            "epoch 7 - batch 80 - loss 1.7662159204483032\n",
            "epoch 7 - batch 90 - loss 1.7424098253250122\n",
            "epoch 7 - batch 100 - loss 1.8306829929351807\n",
            "epoch 7 - batch 110 - loss 1.816308617591858\n",
            "epoch 7 - batch 120 - loss 1.8233476877212524\n",
            "epoch 7 - batch 130 - loss 1.763880729675293\n",
            "epoch 7 - batch 140 - loss 1.6696876287460327\n",
            "epoch 7 - batch 150 - loss 1.8028678894042969\n",
            "epoch 7 - batch 160 - loss 1.7252601385116577\n",
            "epoch 7 - batch 170 - loss 1.7951936721801758\n",
            "epoch 7 - batch 180 - loss 1.7248538732528687\n",
            "epoch 7 - batch 190 - loss 1.7860748767852783\n",
            "epoch 7 - batch 200 - loss 1.8131943941116333\n",
            "epoch 7 - batch 210 - loss 1.8220481872558594\n",
            "epoch 7 - batch 220 - loss 1.7578707933425903\n",
            "epoch 7 - batch 230 - loss 1.7570754289627075\n",
            "epoch 7 - batch 240 - loss 1.7241636514663696\n",
            "epoch 7 - batch 250 - loss 1.7984964847564697\n",
            "epoch 7 - batch 260 - loss 1.7242721319198608\n",
            "epoch 7 - batch 270 - loss 1.7148215770721436\n",
            "epoch 7 - batch 280 - loss 1.7208607196807861\n",
            "epoch 7 - batch 290 - loss 1.7590407133102417\n",
            "epoch 7 - batch 300 - loss 1.738291621208191\n",
            "epoch 7 - batch 310 - loss 1.7496473789215088\n",
            "epoch 7 - batch 320 - loss 1.6631580591201782\n",
            "epoch 7 - batch 330 - loss 1.8012336492538452\n",
            "epoch 7 - batch 340 - loss 1.7107255458831787\n",
            "epoch 7 - batch 350 - loss 1.7975292205810547\n",
            "epoch 7 - batch 360 - loss 1.7770044803619385\n",
            "epoch 7 - batch 370 - loss 1.780340552330017\n",
            "epoch 7 - batch 380 - loss 1.7297788858413696\n",
            "epoch 7 - batch 390 - loss 1.7398512363433838\n",
            "epoch 7 - batch 400 - loss 1.7181960344314575\n",
            "epoch 7 - batch 410 - loss 1.7903859615325928\n",
            "epoch 7 - batch 420 - loss 1.7607468366622925\n",
            "epoch 7 - batch 430 - loss 1.6490676403045654\n",
            "epoch 7 - batch 440 - loss 1.7089691162109375\n",
            "epoch 7 - batch 450 - loss 1.6665699481964111\n",
            "epoch 7 - batch 460 - loss 1.620296597480774\n",
            "epoch 7 - batch 470 - loss 1.6822340488433838\n",
            "epoch 7 - batch 480 - loss 1.6874988079071045\n",
            "epoch 7 - batch 490 - loss 1.7136456966400146\n",
            "epoch 7 - batch 500 - loss 1.6662876605987549\n",
            "epoch 7 - batch 510 - loss 1.6690305471420288\n",
            "epoch 7 - batch 520 - loss 1.7319097518920898\n",
            "epoch 7 - batch 530 - loss 1.7100061178207397\n",
            "epoch 7 - batch 540 - loss 1.7578566074371338\n",
            "epoch 7 - batch 550 - loss 1.7036043405532837\n",
            "epoch 7 - batch 560 - loss 1.6825037002563477\n",
            "epoch 7 - batch 570 - loss 1.7508716583251953\n",
            "epoch 7 - batch 580 - loss 1.679923415184021\n",
            "epoch 7 - batch 590 - loss 1.7457247972488403\n",
            "epoch 7 - batch 600 - loss 1.7160615921020508\n",
            "epoch 7 - batch 610 - loss 1.7336294651031494\n",
            "epoch 7 training time: 217.09940314292908 sec\n",
            "evaluation of batch 0 took: 0.14550089836120605\n",
            "evaluation of batch 50 took: 0.14301776885986328\n",
            "evaluation of batch 100 took: 0.14324569702148438\n",
            "evaluation of batch 150 took: 0.14532995223999023\n",
            "evaluation of batch 200 took: 0.14510035514831543\n",
            "evaluation of batch 250 took: 0.1422736644744873\n",
            "evaluation of batch 300 took: 0.15334296226501465\n",
            "evaluation of batch 350 took: 0.14390921592712402\n",
            "evaluation of batch 400 took: 0.14561963081359863\n",
            "evaluation of batch 450 took: 0.14194250106811523\n",
            "evaluation of batch 500 took: 0.14105725288391113\n",
            "evaluation of batch 550 took: 0.14414739608764648\n",
            "evaluation of batch 600 took: 0.14196395874023438\n",
            "epoch 7 evaluation on training data time: 90.26270151138306 sec\n",
            "evaluation of batch 0 took: 0.15169382095336914\n",
            "evaluation of batch 50 took: 0.14501047134399414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7 evaluation on test data time: 21.76093029975891 sec\n",
            "epoch evaluation:  {'epoch': 7, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.7302365>, 'test_rouge_1_p': 0.4305722946138681, 'test_rouge_1_r': 0.36273867501546053, 'test_rouge_1_f1': 0.3828804971408461, 'test_rouge_2_p': 0.1462522828733766, 'test_rouge_2_r': 0.13385247564935066, 'test_rouge_2_f1': 0.13731272166235645, 'test_rouge_3_p': 0.036863966112012966, 'test_rouge_3_r': 0.03370662540584415, 'test_rouge_3_f1': 0.034577982471397636, 'test_rouge_L_p': 0.4289582789792052, 'test_rouge_L_r': 0.3616155073535093, 'test_rouge_L_f1': 0.3815899217209331}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8 - batch 10 - loss 1.727301836013794\n",
            "epoch 8 - batch 20 - loss 1.642397165298462\n",
            "epoch 8 - batch 30 - loss 1.723271369934082\n",
            "epoch 8 - batch 40 - loss 1.6683145761489868\n",
            "epoch 8 - batch 50 - loss 1.613932728767395\n",
            "epoch 8 - batch 60 - loss 1.7854033708572388\n",
            "epoch 8 - batch 70 - loss 1.7183725833892822\n",
            "epoch 8 - batch 80 - loss 1.6348841190338135\n",
            "epoch 8 - batch 90 - loss 1.7213999032974243\n",
            "epoch 8 - batch 100 - loss 1.6414250135421753\n",
            "epoch 8 - batch 110 - loss 1.7956821918487549\n",
            "epoch 8 - batch 120 - loss 1.6539186239242554\n",
            "epoch 8 - batch 130 - loss 1.6618194580078125\n",
            "epoch 8 - batch 140 - loss 1.661171793937683\n",
            "epoch 8 - batch 150 - loss 1.686564326286316\n",
            "epoch 8 - batch 160 - loss 1.635148048400879\n",
            "epoch 8 - batch 170 - loss 1.65809166431427\n",
            "epoch 8 - batch 180 - loss 1.6296871900558472\n",
            "epoch 8 - batch 190 - loss 1.6545401811599731\n",
            "epoch 8 - batch 200 - loss 1.6550620794296265\n",
            "epoch 8 - batch 210 - loss 1.6928751468658447\n",
            "epoch 8 - batch 220 - loss 1.6624412536621094\n",
            "epoch 8 - batch 230 - loss 1.6929047107696533\n",
            "epoch 8 - batch 240 - loss 1.626090407371521\n",
            "epoch 8 - batch 250 - loss 1.7333908081054688\n",
            "epoch 8 - batch 260 - loss 1.6771379709243774\n",
            "epoch 8 - batch 270 - loss 1.568838357925415\n",
            "epoch 8 - batch 280 - loss 1.7495710849761963\n",
            "epoch 8 - batch 290 - loss 1.6272670030593872\n",
            "epoch 8 - batch 300 - loss 1.676120638847351\n",
            "epoch 8 - batch 310 - loss 1.7179737091064453\n",
            "epoch 8 - batch 320 - loss 1.600220799446106\n",
            "epoch 8 - batch 330 - loss 1.6141412258148193\n",
            "epoch 8 - batch 340 - loss 1.6562097072601318\n",
            "epoch 8 - batch 350 - loss 1.6449576616287231\n",
            "epoch 8 - batch 360 - loss 1.6195327043533325\n",
            "epoch 8 - batch 370 - loss 1.6888797283172607\n",
            "epoch 8 - batch 380 - loss 1.5565412044525146\n",
            "epoch 8 - batch 390 - loss 1.6460535526275635\n",
            "epoch 8 - batch 400 - loss 1.6262433528900146\n",
            "epoch 8 - batch 410 - loss 1.614908218383789\n",
            "epoch 8 - batch 420 - loss 1.6688976287841797\n",
            "epoch 8 - batch 430 - loss 1.6738475561141968\n",
            "epoch 8 - batch 440 - loss 1.648964762687683\n",
            "epoch 8 - batch 450 - loss 1.5540727376937866\n",
            "epoch 8 - batch 460 - loss 1.5892126560211182\n",
            "epoch 8 - batch 470 - loss 1.6311362981796265\n",
            "epoch 8 - batch 480 - loss 1.6193479299545288\n",
            "epoch 8 - batch 490 - loss 1.5543019771575928\n",
            "epoch 8 - batch 500 - loss 1.5972567796707153\n",
            "epoch 8 - batch 510 - loss 1.6330538988113403\n",
            "epoch 8 - batch 520 - loss 1.4925743341445923\n",
            "epoch 8 - batch 530 - loss 1.5804033279418945\n",
            "epoch 8 - batch 540 - loss 1.59296715259552\n",
            "epoch 8 - batch 550 - loss 1.524104118347168\n",
            "epoch 8 - batch 560 - loss 1.6287771463394165\n",
            "epoch 8 - batch 570 - loss 1.5884143114089966\n",
            "epoch 8 - batch 580 - loss 1.598997712135315\n",
            "epoch 8 - batch 590 - loss 1.605915904045105\n",
            "epoch 8 - batch 600 - loss 1.6743133068084717\n",
            "epoch 8 - batch 610 - loss 1.6209237575531006\n",
            "epoch 8 training time: 215.87511706352234 sec\n",
            "evaluation of batch 0 took: 0.14263558387756348\n",
            "evaluation of batch 50 took: 0.15276122093200684\n",
            "evaluation of batch 100 took: 0.14357995986938477\n",
            "evaluation of batch 150 took: 0.14967560768127441\n",
            "evaluation of batch 200 took: 0.1490468978881836\n",
            "evaluation of batch 250 took: 0.14328765869140625\n",
            "evaluation of batch 300 took: 0.15081572532653809\n",
            "evaluation of batch 350 took: 0.14969348907470703\n",
            "evaluation of batch 400 took: 0.1522655487060547\n",
            "evaluation of batch 450 took: 0.14690160751342773\n",
            "evaluation of batch 500 took: 0.14853763580322266\n",
            "evaluation of batch 550 took: 0.14738202095031738\n",
            "evaluation of batch 600 took: 0.14604711532592773\n",
            "epoch 8 evaluation on training data time: 91.62079453468323 sec\n",
            "evaluation of batch 0 took: 0.15259075164794922\n",
            "evaluation of batch 50 took: 0.15079450607299805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8 evaluation on test data time: 21.865572929382324 sec\n",
            "epoch evaluation:  {'epoch': 8, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.6563838>, 'test_rouge_1_p': 0.4517755379850804, 'test_rouge_1_r': 0.38800748637523164, 'test_rouge_1_f1': 0.4068248210179519, 'test_rouge_2_p': 0.16849338812229436, 'test_rouge_2_r': 0.15524130817099566, 'test_rouge_2_f1': 0.15885591624735296, 'test_rouge_3_p': 0.04738420758928572, 'test_rouge_3_r': 0.04356673599837662, 'test_rouge_3_f1': 0.04459396862438156, 'test_rouge_L_p': 0.44999402104591835, 'test_rouge_L_r': 0.3867157001294834, 'test_rouge_L_f1': 0.40536169665190036}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 - batch 10 - loss 1.6043351888656616\n",
            "epoch 9 - batch 20 - loss 1.6381882429122925\n",
            "epoch 9 - batch 30 - loss 1.6524533033370972\n",
            "epoch 9 - batch 40 - loss 1.5785925388336182\n",
            "epoch 9 - batch 50 - loss 1.5701569318771362\n",
            "epoch 9 - batch 60 - loss 1.589591145515442\n",
            "epoch 9 - batch 70 - loss 1.6236340999603271\n",
            "epoch 9 - batch 80 - loss 1.572344422340393\n",
            "epoch 9 - batch 90 - loss 1.5731170177459717\n",
            "epoch 9 - batch 100 - loss 1.608235478401184\n",
            "epoch 9 - batch 110 - loss 1.6248143911361694\n",
            "epoch 9 - batch 120 - loss 1.5900956392288208\n",
            "epoch 9 - batch 130 - loss 1.5783507823944092\n",
            "epoch 9 - batch 140 - loss 1.557273030281067\n",
            "epoch 9 - batch 150 - loss 1.714194655418396\n",
            "epoch 9 - batch 160 - loss 1.5885677337646484\n",
            "epoch 9 - batch 170 - loss 1.558644413948059\n",
            "epoch 9 - batch 180 - loss 1.4803791046142578\n",
            "epoch 9 - batch 190 - loss 1.5959889888763428\n",
            "epoch 9 - batch 200 - loss 1.522531509399414\n",
            "epoch 9 - batch 210 - loss 1.5662540197372437\n",
            "epoch 9 - batch 220 - loss 1.5954691171646118\n",
            "epoch 9 - batch 230 - loss 1.51012122631073\n",
            "epoch 9 - batch 240 - loss 1.5599658489227295\n",
            "epoch 9 - batch 250 - loss 1.622497797012329\n",
            "epoch 9 - batch 260 - loss 1.5286418199539185\n",
            "epoch 9 - batch 270 - loss 1.5788977146148682\n",
            "epoch 9 - batch 280 - loss 1.579837441444397\n",
            "epoch 9 - batch 290 - loss 1.6083582639694214\n",
            "epoch 9 - batch 300 - loss 1.6571859121322632\n",
            "epoch 9 - batch 310 - loss 1.5391935110092163\n",
            "epoch 9 - batch 320 - loss 1.620395302772522\n",
            "epoch 9 - batch 330 - loss 1.4463919401168823\n",
            "epoch 9 - batch 340 - loss 1.5535447597503662\n",
            "epoch 9 - batch 350 - loss 1.5452749729156494\n",
            "epoch 9 - batch 360 - loss 1.5607354640960693\n",
            "epoch 9 - batch 370 - loss 1.5614128112792969\n",
            "epoch 9 - batch 380 - loss 1.5971523523330688\n",
            "epoch 9 - batch 390 - loss 1.5684596300125122\n",
            "epoch 9 - batch 400 - loss 1.5623220205307007\n",
            "epoch 9 - batch 410 - loss 1.55282461643219\n",
            "epoch 9 - batch 420 - loss 1.526883840560913\n",
            "epoch 9 - batch 430 - loss 1.540172815322876\n",
            "epoch 9 - batch 440 - loss 1.5291364192962646\n",
            "epoch 9 - batch 450 - loss 1.5801626443862915\n",
            "epoch 9 - batch 460 - loss 1.510903000831604\n",
            "epoch 9 - batch 470 - loss 1.5907377004623413\n",
            "epoch 9 - batch 480 - loss 1.5479285717010498\n",
            "epoch 9 - batch 490 - loss 1.5684621334075928\n",
            "epoch 9 - batch 500 - loss 1.5462754964828491\n",
            "epoch 9 - batch 510 - loss 1.5379340648651123\n",
            "epoch 9 - batch 520 - loss 1.5312386751174927\n",
            "epoch 9 - batch 530 - loss 1.619709849357605\n",
            "epoch 9 - batch 540 - loss 1.5511654615402222\n",
            "epoch 9 - batch 550 - loss 1.5032079219818115\n",
            "epoch 9 - batch 560 - loss 1.5536315441131592\n",
            "epoch 9 - batch 570 - loss 1.4695428609848022\n",
            "epoch 9 - batch 580 - loss 1.5455514192581177\n",
            "epoch 9 - batch 590 - loss 1.5304561853408813\n",
            "epoch 9 - batch 600 - loss 1.4818803071975708\n",
            "epoch 9 - batch 610 - loss 1.6510450839996338\n",
            "epoch 9 training time: 216.0550239086151 sec\n",
            "evaluation of batch 0 took: 0.15588927268981934\n",
            "evaluation of batch 50 took: 0.14565038681030273\n",
            "evaluation of batch 100 took: 0.14776229858398438\n",
            "evaluation of batch 150 took: 0.14544987678527832\n",
            "evaluation of batch 200 took: 0.14941978454589844\n",
            "evaluation of batch 250 took: 0.14638757705688477\n",
            "evaluation of batch 300 took: 0.15308761596679688\n",
            "evaluation of batch 350 took: 0.14300918579101562\n",
            "evaluation of batch 400 took: 0.14980220794677734\n",
            "evaluation of batch 450 took: 0.14279627799987793\n",
            "evaluation of batch 500 took: 0.1442122459411621\n",
            "evaluation of batch 550 took: 0.1441059112548828\n",
            "evaluation of batch 600 took: 0.14879250526428223\n",
            "epoch 9 evaluation on training data time: 92.71074151992798 sec\n",
            "evaluation of batch 0 took: 0.15536069869995117\n",
            "evaluation of batch 50 took: 0.15191102027893066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 evaluation on test data time: 22.053342819213867 sec\n",
            "epoch evaluation:  {'epoch': 9, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.5960574>, 'test_rouge_1_p': 0.47262596267200047, 'test_rouge_1_r': 0.40752638590947726, 'test_rouge_1_f1': 0.42692300709751996, 'test_rouge_2_p': 0.18684853557900438, 'test_rouge_2_r': 0.17133302895021646, 'test_rouge_2_f1': 0.17572323566134448, 'test_rouge_3_p': 0.05724854572510825, 'test_rouge_3_r': 0.05205691118777056, 'test_rouge_3_f1': 0.05351590683621934, 'test_rouge_L_p': 0.47072208855132974, 'test_rouge_L_r': 0.40620887204313527, 'test_rouge_L_f1': 0.4254035751771902}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 saved checkpoint: models/checkpoints/baseline/ckpt-4\n",
            "epoch 10 - batch 10 - loss 1.5509049892425537\n",
            "epoch 10 - batch 20 - loss 1.6105964183807373\n",
            "epoch 10 - batch 30 - loss 1.5145151615142822\n",
            "epoch 10 - batch 40 - loss 1.527048110961914\n",
            "epoch 10 - batch 50 - loss 1.568987488746643\n",
            "epoch 10 - batch 60 - loss 1.5302437543869019\n",
            "epoch 10 - batch 70 - loss 1.4780654907226562\n",
            "epoch 10 - batch 80 - loss 1.5203129053115845\n",
            "epoch 10 - batch 90 - loss 1.5158804655075073\n",
            "epoch 10 - batch 100 - loss 1.6121629476547241\n",
            "epoch 10 - batch 110 - loss 1.6329928636550903\n",
            "epoch 10 - batch 120 - loss 1.5507166385650635\n",
            "epoch 10 - batch 130 - loss 1.5006686449050903\n",
            "epoch 10 - batch 140 - loss 1.559482216835022\n",
            "epoch 10 - batch 150 - loss 1.5221086740493774\n",
            "epoch 10 - batch 160 - loss 1.4969300031661987\n",
            "epoch 10 - batch 170 - loss 1.5378550291061401\n",
            "epoch 10 - batch 180 - loss 1.572669267654419\n",
            "epoch 10 - batch 190 - loss 1.3959264755249023\n",
            "epoch 10 - batch 200 - loss 1.5523946285247803\n",
            "epoch 10 - batch 210 - loss 1.5097428560256958\n",
            "epoch 10 - batch 220 - loss 1.6124221086502075\n",
            "epoch 10 - batch 230 - loss 1.5331236124038696\n",
            "epoch 10 - batch 240 - loss 1.501422643661499\n",
            "epoch 10 - batch 250 - loss 1.5222827196121216\n",
            "epoch 10 - batch 260 - loss 1.5331658124923706\n",
            "epoch 10 - batch 270 - loss 1.4971916675567627\n",
            "epoch 10 - batch 280 - loss 1.4696377515792847\n",
            "epoch 10 - batch 290 - loss 1.487192988395691\n",
            "epoch 10 - batch 300 - loss 1.5192936658859253\n",
            "epoch 10 - batch 310 - loss 1.4555693864822388\n",
            "epoch 10 - batch 320 - loss 1.5071622133255005\n",
            "epoch 10 - batch 330 - loss 1.4977210760116577\n",
            "epoch 10 - batch 340 - loss 1.4639787673950195\n",
            "epoch 10 - batch 350 - loss 1.4848556518554688\n",
            "epoch 10 - batch 360 - loss 1.551184058189392\n",
            "epoch 10 - batch 370 - loss 1.470224142074585\n",
            "epoch 10 - batch 380 - loss 1.4648072719573975\n",
            "epoch 10 - batch 390 - loss 1.4770787954330444\n",
            "epoch 10 - batch 400 - loss 1.494736671447754\n",
            "epoch 10 - batch 410 - loss 1.4357624053955078\n",
            "epoch 10 - batch 420 - loss 1.5090630054473877\n",
            "epoch 10 - batch 430 - loss 1.4602819681167603\n",
            "epoch 10 - batch 440 - loss 1.5279829502105713\n",
            "epoch 10 - batch 450 - loss 1.4888207912445068\n",
            "epoch 10 - batch 460 - loss 1.5074018239974976\n",
            "epoch 10 - batch 470 - loss 1.415926218032837\n",
            "epoch 10 - batch 480 - loss 1.4375406503677368\n",
            "epoch 10 - batch 490 - loss 1.5950758457183838\n",
            "epoch 10 - batch 500 - loss 1.5003234148025513\n",
            "epoch 10 - batch 510 - loss 1.4967328310012817\n",
            "epoch 10 - batch 520 - loss 1.5371447801589966\n",
            "epoch 10 - batch 530 - loss 1.5393389463424683\n",
            "epoch 10 - batch 540 - loss 1.4734952449798584\n",
            "epoch 10 - batch 550 - loss 1.4741652011871338\n",
            "epoch 10 - batch 560 - loss 1.4759989976882935\n",
            "epoch 10 - batch 570 - loss 1.471457600593567\n",
            "epoch 10 - batch 580 - loss 1.503806471824646\n",
            "epoch 10 - batch 590 - loss 1.4933301210403442\n",
            "epoch 10 - batch 600 - loss 1.4148722887039185\n",
            "epoch 10 - batch 610 - loss 1.4418631792068481\n",
            "epoch 10 training time: 216.1441788673401 sec\n",
            "evaluation of batch 0 took: 0.15021705627441406\n",
            "evaluation of batch 50 took: 0.15758919715881348\n",
            "evaluation of batch 100 took: 0.15319204330444336\n",
            "evaluation of batch 150 took: 0.15260100364685059\n",
            "evaluation of batch 200 took: 0.1570124626159668\n",
            "evaluation of batch 250 took: 0.14987540245056152\n",
            "evaluation of batch 300 took: 0.1541750431060791\n",
            "evaluation of batch 350 took: 0.14422059059143066\n",
            "evaluation of batch 400 took: 0.15004730224609375\n",
            "evaluation of batch 450 took: 0.15506505966186523\n",
            "evaluation of batch 500 took: 0.14696979522705078\n",
            "evaluation of batch 550 took: 0.14573454856872559\n",
            "evaluation of batch 600 took: 0.15040874481201172\n",
            "epoch 10 evaluation on training data time: 93.19942593574524 sec\n",
            "evaluation of batch 0 took: 0.15477633476257324\n",
            "evaluation of batch 50 took: 0.14922785758972168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 10 evaluation on test data time: 22.154114723205566 sec\n",
            "epoch evaluation:  {'epoch': 10, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.5492234>, 'test_rouge_1_p': 0.48531333342996263, 'test_rouge_1_r': 0.4224698757343844, 'test_rouge_1_f1': 0.44111267569630497, 'test_rouge_2_p': 0.20200470102813856, 'test_rouge_2_r': 0.1863441896645021, 'test_rouge_2_f1': 0.19076291124662675, 'test_rouge_3_p': 0.06556877367424241, 'test_rouge_3_r': 0.06020993979978354, 'test_rouge_3_f1': 0.0616914219523036, 'test_rouge_L_p': 0.4833821010648577, 'test_rouge_L_r': 0.4210850533395177, 'test_rouge_L_f1': 0.43953800795952785}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11 - batch 10 - loss 1.4960540533065796\n",
            "epoch 11 - batch 20 - loss 1.5073477029800415\n",
            "epoch 11 - batch 30 - loss 1.4539144039154053\n",
            "epoch 11 - batch 40 - loss 1.4810922145843506\n",
            "epoch 11 - batch 50 - loss 1.4031181335449219\n",
            "epoch 11 - batch 60 - loss 1.4897698163986206\n",
            "epoch 11 - batch 70 - loss 1.4558006525039673\n",
            "epoch 11 - batch 80 - loss 1.5077762603759766\n",
            "epoch 11 - batch 90 - loss 1.4939711093902588\n",
            "epoch 11 - batch 100 - loss 1.499247670173645\n",
            "epoch 11 - batch 110 - loss 1.4297993183135986\n",
            "epoch 11 - batch 120 - loss 1.5285570621490479\n",
            "epoch 11 - batch 130 - loss 1.4000862836837769\n",
            "epoch 11 - batch 140 - loss 1.4621922969818115\n",
            "epoch 11 - batch 150 - loss 1.4686402082443237\n",
            "epoch 11 - batch 160 - loss 1.4684654474258423\n",
            "epoch 11 - batch 170 - loss 1.458369493484497\n",
            "epoch 11 - batch 180 - loss 1.511784553527832\n",
            "epoch 11 - batch 190 - loss 1.453255295753479\n",
            "epoch 11 - batch 200 - loss 1.4534980058670044\n",
            "epoch 11 - batch 210 - loss 1.5083067417144775\n",
            "epoch 11 - batch 220 - loss 1.477229118347168\n",
            "epoch 11 - batch 230 - loss 1.3997809886932373\n",
            "epoch 11 - batch 240 - loss 1.4349825382232666\n",
            "epoch 11 - batch 250 - loss 1.449161410331726\n",
            "epoch 11 - batch 260 - loss 1.4522351026535034\n",
            "epoch 11 - batch 270 - loss 1.4584896564483643\n",
            "epoch 11 - batch 280 - loss 1.4725383520126343\n",
            "epoch 11 - batch 290 - loss 1.5009119510650635\n",
            "epoch 11 - batch 300 - loss 1.451015591621399\n",
            "epoch 11 - batch 310 - loss 1.44145667552948\n",
            "epoch 11 - batch 320 - loss 1.4466686248779297\n",
            "epoch 11 - batch 330 - loss 1.3741368055343628\n",
            "epoch 11 - batch 340 - loss 1.433609127998352\n",
            "epoch 11 - batch 350 - loss 1.5127531290054321\n",
            "epoch 11 - batch 360 - loss 1.410095453262329\n",
            "epoch 11 - batch 370 - loss 1.4542192220687866\n",
            "epoch 11 - batch 380 - loss 1.420338749885559\n",
            "epoch 11 - batch 390 - loss 1.4136502742767334\n",
            "epoch 11 - batch 400 - loss 1.402957558631897\n",
            "epoch 11 - batch 410 - loss 1.4298527240753174\n",
            "epoch 11 - batch 420 - loss 1.4566923379898071\n",
            "epoch 11 - batch 430 - loss 1.378554105758667\n",
            "epoch 11 - batch 440 - loss 1.418623685836792\n",
            "epoch 11 - batch 450 - loss 1.4102123975753784\n",
            "epoch 11 - batch 460 - loss 1.4260705709457397\n",
            "epoch 11 - batch 470 - loss 1.440543293952942\n",
            "epoch 11 - batch 480 - loss 1.462586522102356\n",
            "epoch 11 - batch 490 - loss 1.3796846866607666\n",
            "epoch 11 - batch 500 - loss 1.3488705158233643\n",
            "epoch 11 - batch 510 - loss 1.4481029510498047\n",
            "epoch 11 - batch 520 - loss 1.4279289245605469\n",
            "epoch 11 - batch 530 - loss 1.4565709829330444\n",
            "epoch 11 - batch 540 - loss 1.4270833730697632\n",
            "epoch 11 - batch 550 - loss 1.4452178478240967\n",
            "epoch 11 - batch 560 - loss 1.4528615474700928\n",
            "epoch 11 - batch 570 - loss 1.3863306045532227\n",
            "epoch 11 - batch 580 - loss 1.410584807395935\n",
            "epoch 11 - batch 590 - loss 1.3924640417099\n",
            "epoch 11 - batch 600 - loss 1.382546305656433\n",
            "epoch 11 - batch 610 - loss 1.416279673576355\n",
            "epoch 11 training time: 216.17674446105957 sec\n",
            "evaluation of batch 0 took: 0.14851140975952148\n",
            "evaluation of batch 50 took: 0.15157246589660645\n",
            "evaluation of batch 100 took: 0.15218496322631836\n",
            "evaluation of batch 150 took: 0.15133285522460938\n",
            "evaluation of batch 200 took: 0.14625763893127441\n",
            "evaluation of batch 250 took: 0.14864206314086914\n",
            "evaluation of batch 300 took: 0.15212512016296387\n",
            "evaluation of batch 350 took: 0.14935779571533203\n",
            "evaluation of batch 400 took: 0.15022897720336914\n",
            "evaluation of batch 450 took: 0.15271282196044922\n",
            "evaluation of batch 500 took: 0.15348243713378906\n",
            "evaluation of batch 550 took: 0.1504967212677002\n",
            "evaluation of batch 600 took: 0.14559173583984375\n",
            "epoch 11 evaluation on training data time: 94.0450029373169 sec\n",
            "evaluation of batch 0 took: 0.15562081336975098\n",
            "evaluation of batch 50 took: 0.15183019638061523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11 evaluation on test data time: 22.076698541641235 sec\n",
            "epoch evaluation:  {'epoch': 11, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.5115741>, 'test_rouge_1_p': 0.49604265711966594, 'test_rouge_1_r': 0.43387023133116887, 'test_rouge_1_f1': 0.4521761096778852, 'test_rouge_2_p': 0.21144522794913423, 'test_rouge_2_r': 0.19584242255140688, 'test_rouge_2_f1': 0.2001858801371789, 'test_rouge_3_p': 0.07235799682088744, 'test_rouge_3_r': 0.06665821158008657, 'test_rouge_3_f1': 0.06821687659438774, 'test_rouge_L_p': 0.4940085444689241, 'test_rouge_L_r': 0.4324202745728972, 'test_rouge_L_f1': 0.4505253963490061}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 - batch 10 - loss 1.4440526962280273\n",
            "epoch 12 - batch 20 - loss 1.401509404182434\n",
            "epoch 12 - batch 30 - loss 1.3953335285186768\n",
            "epoch 12 - batch 40 - loss 1.3480714559555054\n",
            "epoch 12 - batch 50 - loss 1.4173295497894287\n",
            "epoch 12 - batch 60 - loss 1.4348279237747192\n",
            "epoch 12 - batch 70 - loss 1.3743034601211548\n",
            "epoch 12 - batch 80 - loss 1.4675146341323853\n",
            "epoch 12 - batch 90 - loss 1.4233474731445312\n",
            "epoch 12 - batch 100 - loss 1.4186569452285767\n",
            "epoch 12 - batch 110 - loss 1.4049443006515503\n",
            "epoch 12 - batch 120 - loss 1.408285140991211\n",
            "epoch 12 - batch 130 - loss 1.3809579610824585\n",
            "epoch 12 - batch 140 - loss 1.3914477825164795\n",
            "epoch 12 - batch 150 - loss 1.4681793451309204\n",
            "epoch 12 - batch 160 - loss 1.423653244972229\n",
            "epoch 12 - batch 170 - loss 1.393585443496704\n",
            "epoch 12 - batch 180 - loss 1.3860784769058228\n",
            "epoch 12 - batch 190 - loss 1.3950960636138916\n",
            "epoch 12 - batch 200 - loss 1.452431559562683\n",
            "epoch 12 - batch 210 - loss 1.3858128786087036\n",
            "epoch 12 - batch 220 - loss 1.4333850145339966\n",
            "epoch 12 - batch 230 - loss 1.3957903385162354\n",
            "epoch 12 - batch 240 - loss 1.3800671100616455\n",
            "epoch 12 - batch 250 - loss 1.4351937770843506\n",
            "epoch 12 - batch 260 - loss 1.3125697374343872\n",
            "epoch 12 - batch 270 - loss 1.4789631366729736\n",
            "epoch 12 - batch 280 - loss 1.4607325792312622\n",
            "epoch 12 - batch 290 - loss 1.422514796257019\n",
            "epoch 12 - batch 300 - loss 1.333386778831482\n",
            "epoch 12 - batch 310 - loss 1.3643563985824585\n",
            "epoch 12 - batch 320 - loss 1.4043630361557007\n",
            "epoch 12 - batch 330 - loss 1.4418460130691528\n",
            "epoch 12 - batch 340 - loss 1.4012386798858643\n",
            "epoch 12 - batch 350 - loss 1.3832157850265503\n",
            "epoch 12 - batch 360 - loss 1.4226518869400024\n",
            "epoch 12 - batch 370 - loss 1.3970059156417847\n",
            "epoch 12 - batch 380 - loss 1.3294353485107422\n",
            "epoch 12 - batch 390 - loss 1.3471148014068604\n",
            "epoch 12 - batch 400 - loss 1.3515568971633911\n",
            "epoch 12 - batch 410 - loss 1.3891832828521729\n",
            "epoch 12 - batch 420 - loss 1.3916587829589844\n",
            "epoch 12 - batch 430 - loss 1.3475183248519897\n",
            "epoch 12 - batch 440 - loss 1.3776681423187256\n",
            "epoch 12 - batch 450 - loss 1.412530779838562\n",
            "epoch 12 - batch 460 - loss 1.3217469453811646\n",
            "epoch 12 - batch 470 - loss 1.3893539905548096\n",
            "epoch 12 - batch 480 - loss 1.4380509853363037\n",
            "epoch 12 - batch 490 - loss 1.3121058940887451\n",
            "epoch 12 - batch 500 - loss 1.3708274364471436\n",
            "epoch 12 - batch 510 - loss 1.3803781270980835\n",
            "epoch 12 - batch 520 - loss 1.3788126707077026\n",
            "epoch 12 - batch 530 - loss 1.3732664585113525\n",
            "epoch 12 - batch 540 - loss 1.405824065208435\n",
            "epoch 12 - batch 550 - loss 1.280895471572876\n",
            "epoch 12 - batch 560 - loss 1.3794212341308594\n",
            "epoch 12 - batch 570 - loss 1.381264567375183\n",
            "epoch 12 - batch 580 - loss 1.4225385189056396\n",
            "epoch 12 - batch 590 - loss 1.3742740154266357\n",
            "epoch 12 - batch 600 - loss 1.3548164367675781\n",
            "epoch 12 - batch 610 - loss 1.3955966234207153\n",
            "epoch 12 training time: 217.63981437683105 sec\n",
            "evaluation of batch 0 took: 0.154005765914917\n",
            "evaluation of batch 50 took: 0.1554415225982666\n",
            "evaluation of batch 100 took: 0.15116357803344727\n",
            "evaluation of batch 150 took: 0.15589022636413574\n",
            "evaluation of batch 200 took: 0.1549530029296875\n",
            "evaluation of batch 250 took: 0.15317273139953613\n",
            "evaluation of batch 300 took: 0.15473437309265137\n",
            "evaluation of batch 350 took: 0.16266393661499023\n",
            "evaluation of batch 400 took: 0.15020203590393066\n",
            "evaluation of batch 450 took: 0.14669132232666016\n",
            "evaluation of batch 500 took: 0.16888689994812012\n",
            "evaluation of batch 550 took: 0.15684914588928223\n",
            "evaluation of batch 600 took: 0.14929413795471191\n",
            "epoch 12 evaluation on training data time: 95.56808066368103 sec\n",
            "evaluation of batch 0 took: 0.15989971160888672\n",
            "evaluation of batch 50 took: 0.15839385986328125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 evaluation on test data time: 22.80469274520874 sec\n",
            "epoch evaluation:  {'epoch': 12, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4795775>, 'test_rouge_1_p': 0.5046828798991188, 'test_rouge_1_r': 0.44649856746675903, 'test_rouge_1_f1': 0.46322736100029105, 'test_rouge_2_p': 0.22172936113365807, 'test_rouge_2_r': 0.207178579883658, 'test_rouge_2_f1': 0.21115961164725885, 'test_rouge_3_p': 0.078611590232684, 'test_rouge_3_r': 0.07334470542478355, 'test_rouge_3_f1': 0.0747541985544218, 'test_rouge_L_p': 0.5026272973678105, 'test_rouge_L_r': 0.44497716522688624, 'test_rouge_L_f1': 0.46152489271528435}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 saved checkpoint: models/checkpoints/baseline/ckpt-5\n",
            "epoch 13 - batch 10 - loss 1.4022127389907837\n",
            "epoch 13 - batch 20 - loss 1.3193705081939697\n",
            "epoch 13 - batch 30 - loss 1.3732894659042358\n",
            "epoch 13 - batch 40 - loss 1.3797714710235596\n",
            "epoch 13 - batch 50 - loss 1.3525208234786987\n",
            "epoch 13 - batch 60 - loss 1.369628667831421\n",
            "epoch 13 - batch 70 - loss 1.333535075187683\n",
            "epoch 13 - batch 80 - loss 1.3661013841629028\n",
            "epoch 13 - batch 90 - loss 1.3422783613204956\n",
            "epoch 13 - batch 100 - loss 1.367806077003479\n",
            "epoch 13 - batch 110 - loss 1.3918969631195068\n",
            "epoch 13 - batch 120 - loss 1.3718332052230835\n",
            "epoch 13 - batch 130 - loss 1.3766435384750366\n",
            "epoch 13 - batch 140 - loss 1.4112790822982788\n",
            "epoch 13 - batch 150 - loss 1.3692747354507446\n",
            "epoch 13 - batch 160 - loss 1.3242913484573364\n",
            "epoch 13 - batch 170 - loss 1.3083146810531616\n",
            "epoch 13 - batch 180 - loss 1.3374794721603394\n",
            "epoch 13 - batch 190 - loss 1.2779273986816406\n",
            "epoch 13 - batch 200 - loss 1.3537598848342896\n",
            "epoch 13 - batch 210 - loss 1.3798630237579346\n",
            "epoch 13 - batch 220 - loss 1.3519686460494995\n",
            "epoch 13 - batch 230 - loss 1.3670597076416016\n",
            "epoch 13 - batch 240 - loss 1.305734634399414\n",
            "epoch 13 - batch 250 - loss 1.3981717824935913\n",
            "epoch 13 - batch 260 - loss 1.3763093948364258\n",
            "epoch 13 - batch 270 - loss 1.3046480417251587\n",
            "epoch 13 - batch 280 - loss 1.4126836061477661\n",
            "epoch 13 - batch 290 - loss 1.3498731851577759\n",
            "epoch 13 - batch 300 - loss 1.3488308191299438\n",
            "epoch 13 - batch 310 - loss 1.3623006343841553\n",
            "epoch 13 - batch 320 - loss 1.3854950666427612\n",
            "epoch 13 - batch 330 - loss 1.3570712804794312\n",
            "epoch 13 - batch 340 - loss 1.4176005125045776\n",
            "epoch 13 - batch 350 - loss 1.3570137023925781\n",
            "epoch 13 - batch 360 - loss 1.4000250101089478\n",
            "epoch 13 - batch 370 - loss 1.3189226388931274\n",
            "epoch 13 - batch 380 - loss 1.3224750757217407\n",
            "epoch 13 - batch 390 - loss 1.384823203086853\n",
            "epoch 13 - batch 400 - loss 1.3529245853424072\n",
            "epoch 13 - batch 410 - loss 1.2981361150741577\n",
            "epoch 13 - batch 420 - loss 1.296826958656311\n",
            "epoch 13 - batch 430 - loss 1.3702362775802612\n",
            "epoch 13 - batch 440 - loss 1.3983968496322632\n",
            "epoch 13 - batch 450 - loss 1.3544769287109375\n",
            "epoch 13 - batch 460 - loss 1.3519456386566162\n",
            "epoch 13 - batch 470 - loss 1.4083815813064575\n",
            "epoch 13 - batch 480 - loss 1.3652265071868896\n",
            "epoch 13 - batch 490 - loss 1.3362890481948853\n",
            "epoch 13 - batch 500 - loss 1.3342372179031372\n",
            "epoch 13 - batch 510 - loss 1.3972342014312744\n",
            "epoch 13 - batch 520 - loss 1.3679113388061523\n",
            "epoch 13 - batch 530 - loss 1.3071950674057007\n",
            "epoch 13 - batch 540 - loss 1.3589369058609009\n",
            "epoch 13 - batch 550 - loss 1.3529638051986694\n",
            "epoch 13 - batch 560 - loss 1.4087954759597778\n",
            "epoch 13 - batch 570 - loss 1.2673295736312866\n",
            "epoch 13 - batch 580 - loss 1.3560916185379028\n",
            "epoch 13 - batch 590 - loss 1.2791475057601929\n",
            "epoch 13 - batch 600 - loss 1.3115322589874268\n",
            "epoch 13 - batch 610 - loss 1.3197462558746338\n",
            "epoch 13 training time: 217.29252576828003 sec\n",
            "evaluation of batch 0 took: 0.1579282283782959\n",
            "evaluation of batch 50 took: 0.15306544303894043\n",
            "evaluation of batch 100 took: 0.16152501106262207\n",
            "evaluation of batch 150 took: 0.15214180946350098\n",
            "evaluation of batch 200 took: 0.15151166915893555\n",
            "evaluation of batch 250 took: 0.16261863708496094\n",
            "evaluation of batch 300 took: 0.15475153923034668\n",
            "evaluation of batch 350 took: 0.1525866985321045\n",
            "evaluation of batch 400 took: 0.1642005443572998\n",
            "evaluation of batch 450 took: 0.15299534797668457\n",
            "evaluation of batch 500 took: 0.1538999080657959\n",
            "evaluation of batch 550 took: 0.15057969093322754\n",
            "evaluation of batch 600 took: 0.15262985229492188\n",
            "epoch 13 evaluation on training data time: 95.28123092651367 sec\n",
            "evaluation of batch 0 took: 0.16295456886291504\n",
            "evaluation of batch 50 took: 0.15528225898742676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 13 evaluation on test data time: 22.30320429801941 sec\n",
            "epoch evaluation:  {'epoch': 13, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.452715>, 'test_rouge_1_p': 0.5090523780534943, 'test_rouge_1_r': 0.4616264989660636, 'test_rouge_1_f1': 0.47417179970383583, 'test_rouge_2_p': 0.22981411492153678, 'test_rouge_2_r': 0.2178554518398268, 'test_rouge_2_f1': 0.22071239356839503, 'test_rouge_3_p': 0.08319488129058442, 'test_rouge_3_r': 0.07899185775162339, 'test_rouge_3_f1': 0.07997444953360135, 'test_rouge_L_p': 0.5068461741941094, 'test_rouge_L_r': 0.4599066437654606, 'test_rouge_L_f1': 0.4722915844104715}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14 - batch 10 - loss 1.365286946296692\n",
            "epoch 14 - batch 20 - loss 1.3848459720611572\n",
            "epoch 14 - batch 30 - loss 1.286874771118164\n",
            "epoch 14 - batch 40 - loss 1.277880311012268\n",
            "epoch 14 - batch 50 - loss 1.2893753051757812\n",
            "epoch 14 - batch 60 - loss 1.342388391494751\n",
            "epoch 14 - batch 70 - loss 1.3309357166290283\n",
            "epoch 14 - batch 80 - loss 1.3918875455856323\n",
            "epoch 14 - batch 90 - loss 1.3432600498199463\n",
            "epoch 14 - batch 100 - loss 1.313494324684143\n",
            "epoch 14 - batch 110 - loss 1.3701952695846558\n",
            "epoch 14 - batch 120 - loss 1.353742003440857\n",
            "epoch 14 - batch 130 - loss 1.3604475259780884\n",
            "epoch 14 - batch 140 - loss 1.401295781135559\n",
            "epoch 14 - batch 150 - loss 1.3177443742752075\n",
            "epoch 14 - batch 160 - loss 1.2800170183181763\n",
            "epoch 14 - batch 170 - loss 1.363898515701294\n",
            "epoch 14 - batch 180 - loss 1.3687002658843994\n",
            "epoch 14 - batch 190 - loss 1.3491308689117432\n",
            "epoch 14 - batch 200 - loss 1.2338899374008179\n",
            "epoch 14 - batch 210 - loss 1.3171555995941162\n",
            "epoch 14 - batch 220 - loss 1.3480256795883179\n",
            "epoch 14 - batch 230 - loss 1.3034793138504028\n",
            "epoch 14 - batch 240 - loss 1.358957290649414\n",
            "epoch 14 - batch 250 - loss 1.310916543006897\n",
            "epoch 14 - batch 260 - loss 1.3370698690414429\n",
            "epoch 14 - batch 270 - loss 1.337552785873413\n",
            "epoch 14 - batch 280 - loss 1.2927604913711548\n",
            "epoch 14 - batch 290 - loss 1.3442567586898804\n",
            "epoch 14 - batch 300 - loss 1.269728422164917\n",
            "epoch 14 - batch 310 - loss 1.2743197679519653\n",
            "epoch 14 - batch 320 - loss 1.3258612155914307\n",
            "epoch 14 - batch 330 - loss 1.2824821472167969\n",
            "epoch 14 - batch 340 - loss 1.3828814029693604\n",
            "epoch 14 - batch 350 - loss 1.310464859008789\n",
            "epoch 14 - batch 360 - loss 1.2707769870758057\n",
            "epoch 14 - batch 370 - loss 1.3085931539535522\n",
            "epoch 14 - batch 380 - loss 1.2557907104492188\n",
            "epoch 14 - batch 390 - loss 1.2962405681610107\n",
            "epoch 14 - batch 400 - loss 1.2902361154556274\n",
            "epoch 14 - batch 410 - loss 1.286588430404663\n",
            "epoch 14 - batch 420 - loss 1.3276814222335815\n",
            "epoch 14 - batch 430 - loss 1.333935022354126\n",
            "epoch 14 - batch 440 - loss 1.263210415840149\n",
            "epoch 14 - batch 450 - loss 1.2724430561065674\n",
            "epoch 14 - batch 460 - loss 1.2969419956207275\n",
            "epoch 14 - batch 470 - loss 1.2895020246505737\n",
            "epoch 14 - batch 480 - loss 1.2949539422988892\n",
            "epoch 14 - batch 490 - loss 1.2889336347579956\n",
            "epoch 14 - batch 500 - loss 1.240208625793457\n",
            "epoch 14 - batch 510 - loss 1.3134708404541016\n",
            "epoch 14 - batch 520 - loss 1.3393598794937134\n",
            "epoch 14 - batch 530 - loss 1.3558733463287354\n",
            "epoch 14 - batch 540 - loss 1.2618162631988525\n",
            "epoch 14 - batch 550 - loss 1.2737566232681274\n",
            "epoch 14 - batch 560 - loss 1.2925798892974854\n",
            "epoch 14 - batch 570 - loss 1.2958552837371826\n",
            "epoch 14 - batch 580 - loss 1.3444445133209229\n",
            "epoch 14 - batch 590 - loss 1.3372942209243774\n",
            "epoch 14 - batch 600 - loss 1.2458248138427734\n",
            "epoch 14 - batch 610 - loss 1.3023537397384644\n",
            "epoch 14 training time: 215.9677014350891 sec\n",
            "evaluation of batch 0 took: 0.15281438827514648\n",
            "evaluation of batch 50 took: 0.1531989574432373\n",
            "evaluation of batch 100 took: 0.15237808227539062\n",
            "evaluation of batch 150 took: 0.15469884872436523\n",
            "evaluation of batch 200 took: 0.1508030891418457\n",
            "evaluation of batch 250 took: 0.15051007270812988\n",
            "evaluation of batch 300 took: 0.15467405319213867\n",
            "evaluation of batch 350 took: 0.16216397285461426\n",
            "evaluation of batch 400 took: 0.15992069244384766\n",
            "evaluation of batch 450 took: 0.15584063529968262\n",
            "evaluation of batch 500 took: 0.15523219108581543\n",
            "evaluation of batch 550 took: 0.1555500030517578\n",
            "evaluation of batch 600 took: 0.15130400657653809\n",
            "epoch 14 evaluation on training data time: 95.02928400039673 sec\n",
            "evaluation of batch 0 took: 0.160689115524292\n",
            "evaluation of batch 50 took: 0.15045380592346191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14 evaluation on test data time: 22.265100717544556 sec\n",
            "epoch evaluation:  {'epoch': 14, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4319819>, 'test_rouge_1_p': 0.5178089790603739, 'test_rouge_1_r': 0.4655827849605752, 'test_rouge_1_f1': 0.479892942203337, 'test_rouge_2_p': 0.2359032568993508, 'test_rouge_2_r': 0.22251378179112563, 'test_rouge_2_f1': 0.22587194910462466, 'test_rouge_3_p': 0.08702482413419917, 'test_rouge_3_r': 0.08211072781385284, 'test_rouge_3_f1': 0.08329943246237889, 'test_rouge_L_p': 0.5156501538825758, 'test_rouge_L_r': 0.46387917560490094, 'test_rouge_L_f1': 0.47804012921771133}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 - batch 10 - loss 1.274368405342102\n",
            "epoch 15 - batch 20 - loss 1.2383829355239868\n",
            "epoch 15 - batch 30 - loss 1.2803136110305786\n",
            "epoch 15 - batch 40 - loss 1.2563352584838867\n",
            "epoch 15 - batch 50 - loss 1.2756121158599854\n",
            "epoch 15 - batch 60 - loss 1.3246463537216187\n",
            "epoch 15 - batch 70 - loss 1.292094111442566\n",
            "epoch 15 - batch 80 - loss 1.3015673160552979\n",
            "epoch 15 - batch 90 - loss 1.2727749347686768\n",
            "epoch 15 - batch 100 - loss 1.2733961343765259\n",
            "epoch 15 - batch 110 - loss 1.3074966669082642\n",
            "epoch 15 - batch 120 - loss 1.2872461080551147\n",
            "epoch 15 - batch 130 - loss 1.2267686128616333\n",
            "epoch 15 - batch 140 - loss 1.291195273399353\n",
            "epoch 15 - batch 150 - loss 1.323857069015503\n",
            "epoch 15 - batch 160 - loss 1.337677001953125\n",
            "epoch 15 - batch 170 - loss 1.3039251565933228\n",
            "epoch 15 - batch 180 - loss 1.2962497472763062\n",
            "epoch 15 - batch 190 - loss 1.2500860691070557\n",
            "epoch 15 - batch 200 - loss 1.2404193878173828\n",
            "epoch 15 - batch 210 - loss 1.2109596729278564\n",
            "epoch 15 - batch 220 - loss 1.3479325771331787\n",
            "epoch 15 - batch 230 - loss 1.2680590152740479\n",
            "epoch 15 - batch 240 - loss 1.2705628871917725\n",
            "epoch 15 - batch 250 - loss 1.366486668586731\n",
            "epoch 15 - batch 260 - loss 1.3586057424545288\n",
            "epoch 15 - batch 270 - loss 1.2716305255889893\n",
            "epoch 15 - batch 280 - loss 1.3072251081466675\n",
            "epoch 15 - batch 290 - loss 1.212723970413208\n",
            "epoch 15 - batch 300 - loss 1.3377182483673096\n",
            "epoch 15 - batch 310 - loss 1.2234249114990234\n",
            "epoch 15 - batch 320 - loss 1.274871826171875\n",
            "epoch 15 - batch 330 - loss 1.2521896362304688\n",
            "epoch 15 - batch 340 - loss 1.2961763143539429\n",
            "epoch 15 - batch 350 - loss 1.2988792657852173\n",
            "epoch 15 - batch 360 - loss 1.2510945796966553\n",
            "epoch 15 - batch 370 - loss 1.2816294431686401\n",
            "epoch 15 - batch 380 - loss 1.278360366821289\n",
            "epoch 15 - batch 390 - loss 1.2955992221832275\n",
            "epoch 15 - batch 400 - loss 1.2722223997116089\n",
            "epoch 15 - batch 410 - loss 1.199756383895874\n",
            "epoch 15 - batch 420 - loss 1.2741819620132446\n",
            "epoch 15 - batch 430 - loss 1.210182547569275\n",
            "epoch 15 - batch 440 - loss 1.2546498775482178\n",
            "epoch 15 - batch 450 - loss 1.2351493835449219\n",
            "epoch 15 - batch 460 - loss 1.3068374395370483\n",
            "epoch 15 - batch 470 - loss 1.2650619745254517\n",
            "epoch 15 - batch 480 - loss 1.3177236318588257\n",
            "epoch 15 - batch 490 - loss 1.2427079677581787\n",
            "epoch 15 - batch 500 - loss 1.2633285522460938\n",
            "epoch 15 - batch 510 - loss 1.2434090375900269\n",
            "epoch 15 - batch 520 - loss 1.2277947664260864\n",
            "epoch 15 - batch 530 - loss 1.2844189405441284\n",
            "epoch 15 - batch 540 - loss 1.3040698766708374\n",
            "epoch 15 - batch 550 - loss 1.2259693145751953\n",
            "epoch 15 - batch 560 - loss 1.2892335653305054\n",
            "epoch 15 - batch 570 - loss 1.2685643434524536\n",
            "epoch 15 - batch 580 - loss 1.2347090244293213\n",
            "epoch 15 - batch 590 - loss 1.230219841003418\n",
            "epoch 15 - batch 600 - loss 1.233389139175415\n",
            "epoch 15 - batch 610 - loss 1.275208830833435\n",
            "epoch 15 training time: 216.03525114059448 sec\n",
            "evaluation of batch 0 took: 0.1474907398223877\n",
            "evaluation of batch 50 took: 0.1549689769744873\n",
            "evaluation of batch 100 took: 0.14877963066101074\n",
            "evaluation of batch 150 took: 0.14854097366333008\n",
            "evaluation of batch 200 took: 0.1531074047088623\n",
            "evaluation of batch 250 took: 0.15471839904785156\n",
            "evaluation of batch 300 took: 0.15179705619812012\n",
            "evaluation of batch 350 took: 0.14795684814453125\n",
            "evaluation of batch 400 took: 0.1540987491607666\n",
            "evaluation of batch 450 took: 0.15718507766723633\n",
            "evaluation of batch 500 took: 0.15302181243896484\n",
            "evaluation of batch 550 took: 0.1511836051940918\n",
            "evaluation of batch 600 took: 0.15593695640563965\n",
            "epoch 15 evaluation on training data time: 95.47470664978027 sec\n",
            "evaluation of batch 0 took: 0.15677952766418457\n",
            "evaluation of batch 50 took: 0.15299344062805176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 evaluation on test data time: 22.062241077423096 sec\n",
            "epoch evaluation:  {'epoch': 15, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4105427>, 'test_rouge_1_p': 0.5243177651032003, 'test_rouge_1_r': 0.4715218491515925, 'test_rouge_1_f1': 0.48615436001926465, 'test_rouge_2_p': 0.2439294592126624, 'test_rouge_2_r': 0.22982912270021652, 'test_rouge_2_f1': 0.23351383570602832, 'test_rouge_3_p': 0.09191672585227274, 'test_rouge_3_r': 0.08659572849025975, 'test_rouge_3_f1': 0.08792718318387963, 'test_rouge_L_p': 0.52216346943607, 'test_rouge_L_r': 0.46983548213319376, 'test_rouge_L_f1': 0.48431570599539353}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 saved checkpoint: models/checkpoints/baseline/ckpt-6\n",
            "epoch 16 - batch 10 - loss 1.2810933589935303\n",
            "epoch 16 - batch 20 - loss 1.2652480602264404\n",
            "epoch 16 - batch 30 - loss 1.282388687133789\n",
            "epoch 16 - batch 40 - loss 1.2405983209609985\n",
            "epoch 16 - batch 50 - loss 1.392696738243103\n",
            "epoch 16 - batch 60 - loss 1.3052583932876587\n",
            "epoch 16 - batch 70 - loss 1.2651337385177612\n",
            "epoch 16 - batch 80 - loss 1.2415729761123657\n",
            "epoch 16 - batch 90 - loss 1.3067389726638794\n",
            "epoch 16 - batch 100 - loss 1.265557050704956\n",
            "epoch 16 - batch 110 - loss 1.234548807144165\n",
            "epoch 16 - batch 120 - loss 1.2605931758880615\n",
            "epoch 16 - batch 130 - loss 1.2413052320480347\n",
            "epoch 16 - batch 140 - loss 1.208445429801941\n",
            "epoch 16 - batch 150 - loss 1.2154791355133057\n",
            "epoch 16 - batch 160 - loss 1.177460789680481\n",
            "epoch 16 - batch 170 - loss 1.225303053855896\n",
            "epoch 16 - batch 180 - loss 1.1935322284698486\n",
            "epoch 16 - batch 190 - loss 1.287955403327942\n",
            "epoch 16 - batch 200 - loss 1.2191619873046875\n",
            "epoch 16 - batch 210 - loss 1.22301185131073\n",
            "epoch 16 - batch 220 - loss 1.3007545471191406\n",
            "epoch 16 - batch 230 - loss 1.2273807525634766\n",
            "epoch 16 - batch 240 - loss 1.297291874885559\n",
            "epoch 16 - batch 250 - loss 1.288903832435608\n",
            "epoch 16 - batch 260 - loss 1.2569893598556519\n",
            "epoch 16 - batch 270 - loss 1.2427202463150024\n",
            "epoch 16 - batch 280 - loss 1.2568944692611694\n",
            "epoch 16 - batch 290 - loss 1.3054074048995972\n",
            "epoch 16 - batch 300 - loss 1.2028785943984985\n",
            "epoch 16 - batch 310 - loss 1.3131344318389893\n",
            "epoch 16 - batch 320 - loss 1.2317513227462769\n",
            "epoch 16 - batch 330 - loss 1.244480013847351\n",
            "epoch 16 - batch 340 - loss 1.1993143558502197\n",
            "epoch 16 - batch 350 - loss 1.3367365598678589\n",
            "epoch 16 - batch 360 - loss 1.2425450086593628\n",
            "epoch 16 - batch 370 - loss 1.2463029623031616\n",
            "epoch 16 - batch 380 - loss 1.2416632175445557\n",
            "epoch 16 - batch 390 - loss 1.3205677270889282\n",
            "epoch 16 - batch 400 - loss 1.2518922090530396\n",
            "epoch 16 - batch 410 - loss 1.1699111461639404\n",
            "epoch 16 - batch 420 - loss 1.2715349197387695\n",
            "epoch 16 - batch 430 - loss 1.2698116302490234\n",
            "epoch 16 - batch 440 - loss 1.219213604927063\n",
            "epoch 16 - batch 450 - loss 1.2780171632766724\n",
            "epoch 16 - batch 460 - loss 1.2501938343048096\n",
            "epoch 16 - batch 470 - loss 1.250497579574585\n",
            "epoch 16 - batch 480 - loss 1.2253316640853882\n",
            "epoch 16 - batch 490 - loss 1.18915855884552\n",
            "epoch 16 - batch 500 - loss 1.1783250570297241\n",
            "epoch 16 - batch 510 - loss 1.1896169185638428\n",
            "epoch 16 - batch 520 - loss 1.1886647939682007\n",
            "epoch 16 - batch 530 - loss 1.2062528133392334\n",
            "epoch 16 - batch 540 - loss 1.2444438934326172\n",
            "epoch 16 - batch 550 - loss 1.2006555795669556\n",
            "epoch 16 - batch 560 - loss 1.2616878747940063\n",
            "epoch 16 - batch 570 - loss 1.237384557723999\n",
            "epoch 16 - batch 580 - loss 1.2741233110427856\n",
            "epoch 16 - batch 590 - loss 1.228568434715271\n",
            "epoch 16 - batch 600 - loss 1.2488492727279663\n",
            "epoch 16 - batch 610 - loss 1.2329095602035522\n",
            "epoch 16 training time: 216.17661142349243 sec\n",
            "evaluation of batch 0 took: 0.16055917739868164\n",
            "evaluation of batch 50 took: 0.15183806419372559\n",
            "evaluation of batch 100 took: 0.1533372402191162\n",
            "evaluation of batch 150 took: 0.15994668006896973\n",
            "evaluation of batch 200 took: 0.15764164924621582\n",
            "evaluation of batch 250 took: 0.15392374992370605\n",
            "evaluation of batch 300 took: 0.15413427352905273\n",
            "evaluation of batch 350 took: 0.15256595611572266\n",
            "evaluation of batch 400 took: 0.1589641571044922\n",
            "evaluation of batch 450 took: 0.15156960487365723\n",
            "evaluation of batch 500 took: 0.1584782600402832\n",
            "evaluation of batch 550 took: 0.1547071933746338\n",
            "evaluation of batch 600 took: 0.15105438232421875\n",
            "epoch 16 evaluation on training data time: 95.5070698261261 sec\n",
            "evaluation of batch 0 took: 0.15611577033996582\n",
            "evaluation of batch 50 took: 0.15075302124023438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 16 evaluation on test data time: 22.099212646484375 sec\n",
            "epoch evaluation:  {'epoch': 16, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.395839>, 'test_rouge_1_p': 0.5277105195771491, 'test_rouge_1_r': 0.4773112583101422, 'test_rouge_1_f1': 0.49113039977361206, 'test_rouge_2_p': 0.2497581845238095, 'test_rouge_2_r': 0.2356316372429654, 'test_rouge_2_f1': 0.23925215381434944, 'test_rouge_3_p': 0.0956701501623377, 'test_rouge_3_r': 0.09015066964285715, 'test_rouge_3_f1': 0.09153410298778603, 'test_rouge_L_p': 0.5254936562693258, 'test_rouge_L_r': 0.475562233060838, 'test_rouge_L_f1': 0.4892299211318084}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 17 - batch 10 - loss 1.235133171081543\n",
            "epoch 17 - batch 20 - loss 1.2678658962249756\n",
            "epoch 17 - batch 30 - loss 1.2880762815475464\n",
            "epoch 17 - batch 40 - loss 1.2310069799423218\n",
            "epoch 17 - batch 50 - loss 1.2233248949050903\n",
            "epoch 17 - batch 60 - loss 1.1629019975662231\n",
            "epoch 17 - batch 70 - loss 1.2188745737075806\n",
            "epoch 17 - batch 80 - loss 1.2633631229400635\n",
            "epoch 17 - batch 90 - loss 1.261789083480835\n",
            "epoch 17 - batch 100 - loss 1.281328797340393\n",
            "epoch 17 - batch 110 - loss 1.2565151453018188\n",
            "epoch 17 - batch 120 - loss 1.2647384405136108\n",
            "epoch 17 - batch 130 - loss 1.246338963508606\n",
            "epoch 17 - batch 140 - loss 1.1969534158706665\n",
            "epoch 17 - batch 150 - loss 1.2669578790664673\n",
            "epoch 17 - batch 160 - loss 1.2372273206710815\n",
            "epoch 17 - batch 170 - loss 1.2094696760177612\n",
            "epoch 17 - batch 180 - loss 1.1814569234848022\n",
            "epoch 17 - batch 190 - loss 1.1861213445663452\n",
            "epoch 17 - batch 200 - loss 1.2355210781097412\n",
            "epoch 17 - batch 210 - loss 1.1784169673919678\n",
            "epoch 17 - batch 220 - loss 1.2201273441314697\n",
            "epoch 17 - batch 230 - loss 1.1601094007492065\n",
            "epoch 17 - batch 240 - loss 1.2198704481124878\n",
            "epoch 17 - batch 250 - loss 1.1883232593536377\n",
            "epoch 17 - batch 260 - loss 1.1535199880599976\n",
            "epoch 17 - batch 270 - loss 1.2730226516723633\n",
            "epoch 17 - batch 280 - loss 1.1831963062286377\n",
            "epoch 17 - batch 290 - loss 1.284027338027954\n",
            "epoch 17 - batch 300 - loss 1.2699947357177734\n",
            "epoch 17 - batch 310 - loss 1.194828748703003\n",
            "epoch 17 - batch 320 - loss 1.182527780532837\n",
            "epoch 17 - batch 330 - loss 1.241263508796692\n",
            "epoch 17 - batch 340 - loss 1.3082095384597778\n",
            "epoch 17 - batch 350 - loss 1.2441902160644531\n",
            "epoch 17 - batch 360 - loss 1.2165738344192505\n",
            "epoch 17 - batch 370 - loss 1.2754541635513306\n",
            "epoch 17 - batch 380 - loss 1.225813865661621\n",
            "epoch 17 - batch 390 - loss 1.240722894668579\n",
            "epoch 17 - batch 400 - loss 1.2433072328567505\n",
            "epoch 17 - batch 410 - loss 1.2372589111328125\n",
            "epoch 17 - batch 420 - loss 1.2771703004837036\n",
            "epoch 17 - batch 430 - loss 1.1868864297866821\n",
            "epoch 17 - batch 440 - loss 1.2301443815231323\n",
            "epoch 17 - batch 450 - loss 1.1783292293548584\n",
            "epoch 17 - batch 460 - loss 1.2283681631088257\n",
            "epoch 17 - batch 470 - loss 1.243499755859375\n",
            "epoch 17 - batch 480 - loss 1.2777408361434937\n",
            "epoch 17 - batch 490 - loss 1.2232837677001953\n",
            "epoch 17 - batch 500 - loss 1.1317952871322632\n",
            "epoch 17 - batch 510 - loss 1.1840546131134033\n",
            "epoch 17 - batch 520 - loss 1.1727622747421265\n",
            "epoch 17 - batch 530 - loss 1.2702760696411133\n",
            "epoch 17 - batch 540 - loss 1.2189414501190186\n",
            "epoch 17 - batch 550 - loss 1.1932671070098877\n",
            "epoch 17 - batch 560 - loss 1.1826194524765015\n",
            "epoch 17 - batch 570 - loss 1.2358227968215942\n",
            "epoch 17 - batch 580 - loss 1.136043667793274\n",
            "epoch 17 - batch 590 - loss 1.2675440311431885\n",
            "epoch 17 - batch 600 - loss 1.2561604976654053\n",
            "epoch 17 - batch 610 - loss 1.2602177858352661\n",
            "epoch 17 training time: 216.1998906135559 sec\n",
            "evaluation of batch 0 took: 0.14848637580871582\n",
            "evaluation of batch 50 took: 0.15472912788391113\n",
            "evaluation of batch 100 took: 0.15519165992736816\n",
            "evaluation of batch 150 took: 0.15276026725769043\n",
            "evaluation of batch 200 took: 0.15314459800720215\n",
            "evaluation of batch 250 took: 0.1528151035308838\n",
            "evaluation of batch 300 took: 0.15380620956420898\n",
            "evaluation of batch 350 took: 0.16138887405395508\n",
            "evaluation of batch 400 took: 0.156219482421875\n",
            "evaluation of batch 450 took: 0.1535956859588623\n",
            "evaluation of batch 500 took: 0.15073132514953613\n",
            "evaluation of batch 550 took: 0.15261173248291016\n",
            "evaluation of batch 600 took: 0.15507841110229492\n",
            "epoch 17 evaluation on training data time: 95.45020723342896 sec\n",
            "evaluation of batch 0 took: 0.15720820426940918\n",
            "evaluation of batch 50 took: 0.15081262588500977\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 17 evaluation on test data time: 22.129155158996582 sec\n",
            "epoch evaluation:  {'epoch': 17, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.383167>, 'test_rouge_1_p': 0.5298675329506806, 'test_rouge_1_r': 0.4829972738385124, 'test_rouge_1_f1': 0.4955087188244655, 'test_rouge_2_p': 0.2537527901785715, 'test_rouge_2_r': 0.24116633691829, 'test_rouge_2_f1': 0.24419009457691482, 'test_rouge_3_p': 0.09843813413149351, 'test_rouge_3_r': 0.09373647186147183, 'test_rouge_3_f1': 0.09478961328849722, 'test_rouge_L_p': 0.5277007962275819, 'test_rouge_L_r': 0.48125277809987616, 'test_rouge_L_f1': 0.49362905939039786}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 - batch 10 - loss 1.2250550985336304\n",
            "epoch 18 - batch 20 - loss 1.1951671838760376\n",
            "epoch 18 - batch 30 - loss 1.2190138101577759\n",
            "epoch 18 - batch 40 - loss 1.2071627378463745\n",
            "epoch 18 - batch 50 - loss 1.182253122329712\n",
            "epoch 18 - batch 60 - loss 1.225638747215271\n",
            "epoch 18 - batch 70 - loss 1.1685224771499634\n",
            "epoch 18 - batch 80 - loss 1.1753193140029907\n",
            "epoch 18 - batch 90 - loss 1.2008593082427979\n",
            "epoch 18 - batch 100 - loss 1.1999062299728394\n",
            "epoch 18 - batch 110 - loss 1.2362439632415771\n",
            "epoch 18 - batch 120 - loss 1.1866114139556885\n",
            "epoch 18 - batch 130 - loss 1.172324538230896\n",
            "epoch 18 - batch 140 - loss 1.1495410203933716\n",
            "epoch 18 - batch 150 - loss 1.1873910427093506\n",
            "epoch 18 - batch 160 - loss 1.2310856580734253\n",
            "epoch 18 - batch 170 - loss 1.231699824333191\n",
            "epoch 18 - batch 180 - loss 1.1813174486160278\n",
            "epoch 18 - batch 190 - loss 1.2333996295928955\n",
            "epoch 18 - batch 200 - loss 1.2079895734786987\n",
            "epoch 18 - batch 210 - loss 1.2783464193344116\n",
            "epoch 18 - batch 220 - loss 1.1485024690628052\n",
            "epoch 18 - batch 230 - loss 1.1512407064437866\n",
            "epoch 18 - batch 240 - loss 1.204451322555542\n",
            "epoch 18 - batch 250 - loss 1.168650507926941\n",
            "epoch 18 - batch 260 - loss 1.2490384578704834\n",
            "epoch 18 - batch 270 - loss 1.2133150100708008\n",
            "epoch 18 - batch 280 - loss 1.1829744577407837\n",
            "epoch 18 - batch 290 - loss 1.3088054656982422\n",
            "epoch 18 - batch 300 - loss 1.119585633277893\n",
            "epoch 18 - batch 310 - loss 1.152059555053711\n",
            "epoch 18 - batch 320 - loss 1.204589605331421\n",
            "epoch 18 - batch 330 - loss 1.1888285875320435\n",
            "epoch 18 - batch 340 - loss 1.1365243196487427\n",
            "epoch 18 - batch 350 - loss 1.182679533958435\n",
            "epoch 18 - batch 360 - loss 1.182924509048462\n",
            "epoch 18 - batch 370 - loss 1.13102126121521\n",
            "epoch 18 - batch 380 - loss 1.1796971559524536\n",
            "epoch 18 - batch 390 - loss 1.149729609489441\n",
            "epoch 18 - batch 400 - loss 1.2140434980392456\n",
            "epoch 18 - batch 410 - loss 1.1715266704559326\n",
            "epoch 18 - batch 420 - loss 1.2013180255889893\n",
            "epoch 18 - batch 430 - loss 1.1359196901321411\n",
            "epoch 18 - batch 440 - loss 1.2221683263778687\n",
            "epoch 18 - batch 450 - loss 1.1645766496658325\n",
            "epoch 18 - batch 460 - loss 1.1620854139328003\n",
            "epoch 18 - batch 470 - loss 1.1519306898117065\n",
            "epoch 18 - batch 480 - loss 1.2305282354354858\n",
            "epoch 18 - batch 490 - loss 1.1356728076934814\n",
            "epoch 18 - batch 500 - loss 1.1448554992675781\n",
            "epoch 18 - batch 510 - loss 1.227690577507019\n",
            "epoch 18 - batch 520 - loss 1.132649302482605\n",
            "epoch 18 - batch 530 - loss 1.2077304124832153\n",
            "epoch 18 - batch 540 - loss 1.206249475479126\n",
            "epoch 18 - batch 550 - loss 1.1388083696365356\n",
            "epoch 18 - batch 560 - loss 1.1982988119125366\n",
            "epoch 18 - batch 570 - loss 1.193971037864685\n",
            "epoch 18 - batch 580 - loss 1.1625230312347412\n",
            "epoch 18 - batch 590 - loss 1.1528511047363281\n",
            "epoch 18 - batch 600 - loss 1.189837098121643\n",
            "epoch 18 - batch 610 - loss 1.203717589378357\n",
            "epoch 18 training time: 217.52629041671753 sec\n",
            "evaluation of batch 0 took: 0.1567988395690918\n",
            "evaluation of batch 50 took: 0.15764141082763672\n",
            "evaluation of batch 100 took: 0.15896010398864746\n",
            "evaluation of batch 150 took: 0.15088438987731934\n",
            "evaluation of batch 200 took: 0.1565535068511963\n",
            "evaluation of batch 250 took: 0.1557173728942871\n",
            "evaluation of batch 300 took: 0.1577129364013672\n",
            "evaluation of batch 350 took: 0.15422296524047852\n",
            "evaluation of batch 400 took: 0.16068720817565918\n",
            "evaluation of batch 450 took: 0.15747833251953125\n",
            "evaluation of batch 500 took: 0.15297341346740723\n",
            "evaluation of batch 550 took: 0.1555614471435547\n",
            "evaluation of batch 600 took: 0.15524578094482422\n",
            "epoch 18 evaluation on training data time: 96.01879215240479 sec\n",
            "evaluation of batch 0 took: 0.1595907211303711\n",
            "evaluation of batch 50 took: 0.1531665325164795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 evaluation on test data time: 22.35367441177368 sec\n",
            "epoch evaluation:  {'epoch': 18, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3735939>, 'test_rouge_1_p': 0.5351516600958566, 'test_rouge_1_r': 0.4865822003517316, 'test_rouge_1_f1': 0.4998430438327455, 'test_rouge_2_p': 0.2579473586309525, 'test_rouge_2_r': 0.24357815036525982, 'test_rouge_2_f1': 0.24731894877871952, 'test_rouge_3_p': 0.10113847740800869, 'test_rouge_3_r': 0.09548265861742425, 'test_rouge_3_f1': 0.09688797453166872, 'test_rouge_L_p': 0.533021068867888, 'test_rouge_L_r': 0.48492038328115317, 'test_rouge_L_f1': 0.4980293704649425}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 saved checkpoint: models/checkpoints/baseline/ckpt-7\n",
            "epoch 19 - batch 10 - loss 1.2001533508300781\n",
            "epoch 19 - batch 20 - loss 1.1968114376068115\n",
            "epoch 19 - batch 30 - loss 1.152133584022522\n",
            "epoch 19 - batch 40 - loss 1.2294379472732544\n",
            "epoch 19 - batch 50 - loss 1.2318452596664429\n",
            "epoch 19 - batch 60 - loss 1.22284996509552\n",
            "epoch 19 - batch 70 - loss 1.203696608543396\n",
            "epoch 19 - batch 80 - loss 1.2057408094406128\n",
            "epoch 19 - batch 90 - loss 1.1484358310699463\n",
            "epoch 19 - batch 100 - loss 1.1817504167556763\n",
            "epoch 19 - batch 110 - loss 1.2000519037246704\n",
            "epoch 19 - batch 120 - loss 1.152748703956604\n",
            "epoch 19 - batch 130 - loss 1.126262903213501\n",
            "epoch 19 - batch 140 - loss 1.2445164918899536\n",
            "epoch 19 - batch 150 - loss 1.2507654428482056\n",
            "epoch 19 - batch 160 - loss 1.2268532514572144\n",
            "epoch 19 - batch 170 - loss 1.1817471981048584\n",
            "epoch 19 - batch 180 - loss 1.178783655166626\n",
            "epoch 19 - batch 190 - loss 1.202349305152893\n",
            "epoch 19 - batch 200 - loss 1.1920626163482666\n",
            "epoch 19 - batch 210 - loss 1.1782315969467163\n",
            "epoch 19 - batch 220 - loss 1.2649585008621216\n",
            "epoch 19 - batch 230 - loss 1.1775017976760864\n",
            "epoch 19 - batch 240 - loss 1.1579889059066772\n",
            "epoch 19 - batch 250 - loss 1.1974314451217651\n",
            "epoch 19 - batch 260 - loss 1.183540940284729\n",
            "epoch 19 - batch 270 - loss 1.2290477752685547\n",
            "epoch 19 - batch 280 - loss 1.1560258865356445\n",
            "epoch 19 - batch 290 - loss 1.1756715774536133\n",
            "epoch 19 - batch 300 - loss 1.1910438537597656\n",
            "epoch 19 - batch 310 - loss 1.1767628192901611\n",
            "epoch 19 - batch 320 - loss 1.1799514293670654\n",
            "epoch 19 - batch 330 - loss 1.1361466646194458\n",
            "epoch 19 - batch 340 - loss 1.177285075187683\n",
            "epoch 19 - batch 350 - loss 1.1930501461029053\n",
            "epoch 19 - batch 360 - loss 1.1173226833343506\n",
            "epoch 19 - batch 370 - loss 1.1557742357254028\n",
            "epoch 19 - batch 380 - loss 1.137174367904663\n",
            "epoch 19 - batch 390 - loss 1.1292798519134521\n",
            "epoch 19 - batch 400 - loss 1.1219562292099\n",
            "epoch 19 - batch 410 - loss 1.1603995561599731\n",
            "epoch 19 - batch 420 - loss 1.2319344282150269\n",
            "epoch 19 - batch 430 - loss 1.0625313520431519\n",
            "epoch 19 - batch 440 - loss 1.104630947113037\n",
            "epoch 19 - batch 450 - loss 1.1446131467819214\n",
            "epoch 19 - batch 460 - loss 1.1799174547195435\n",
            "epoch 19 - batch 470 - loss 1.1238499879837036\n",
            "epoch 19 - batch 480 - loss 1.1883447170257568\n",
            "epoch 19 - batch 490 - loss 1.0886472463607788\n",
            "epoch 19 - batch 500 - loss 1.1501914262771606\n",
            "epoch 19 - batch 510 - loss 1.2021808624267578\n",
            "epoch 19 - batch 520 - loss 1.1284451484680176\n",
            "epoch 19 - batch 530 - loss 1.2142016887664795\n",
            "epoch 19 - batch 540 - loss 1.175382375717163\n",
            "epoch 19 - batch 550 - loss 1.1440412998199463\n",
            "epoch 19 - batch 560 - loss 1.1758440732955933\n",
            "epoch 19 - batch 570 - loss 1.101945161819458\n",
            "epoch 19 - batch 580 - loss 1.1483418941497803\n",
            "epoch 19 - batch 590 - loss 1.1549631357192993\n",
            "epoch 19 - batch 600 - loss 1.1502845287322998\n",
            "epoch 19 - batch 610 - loss 1.0980314016342163\n",
            "epoch 19 training time: 216.45890069007874 sec\n",
            "evaluation of batch 0 took: 0.14916396141052246\n",
            "evaluation of batch 50 took: 0.15190839767456055\n",
            "evaluation of batch 100 took: 0.1590428352355957\n",
            "evaluation of batch 150 took: 0.15586233139038086\n",
            "evaluation of batch 200 took: 0.15739798545837402\n",
            "evaluation of batch 250 took: 0.1543886661529541\n",
            "evaluation of batch 300 took: 0.15901970863342285\n",
            "evaluation of batch 350 took: 0.1520850658416748\n",
            "evaluation of batch 400 took: 0.15316152572631836\n",
            "evaluation of batch 450 took: 0.15222573280334473\n",
            "evaluation of batch 500 took: 0.15285348892211914\n",
            "evaluation of batch 550 took: 0.15024495124816895\n",
            "evaluation of batch 600 took: 0.1617140769958496\n",
            "epoch 19 evaluation on training data time: 95.96738576889038 sec\n",
            "evaluation of batch 0 took: 0.1626298427581787\n",
            "evaluation of batch 50 took: 0.15165066719055176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 19 evaluation on test data time: 22.178317308425903 sec\n",
            "epoch evaluation:  {'epoch': 19, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3630778>, 'test_rouge_1_p': 0.5385138252744279, 'test_rouge_1_r': 0.4893759723349566, 'test_rouge_1_f1': 0.5028261771654122, 'test_rouge_2_p': 0.26175996854707795, 'test_rouge_2_r': 0.24725463338744597, 'test_rouge_2_f1': 0.2509909690890819, 'test_rouge_3_p': 0.10380986201298703, 'test_rouge_3_r': 0.0978114008387446, 'test_rouge_3_f1': 0.09930405578424549, 'test_rouge_L_p': 0.5363889412299013, 'test_rouge_L_r': 0.4877202550054112, 'test_rouge_L_f1': 0.5010162509382527}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 20 - batch 10 - loss 1.1259615421295166\n",
            "epoch 20 - batch 20 - loss 1.1659923791885376\n",
            "epoch 20 - batch 30 - loss 1.1076008081436157\n",
            "epoch 20 - batch 40 - loss 1.1498298645019531\n",
            "epoch 20 - batch 50 - loss 1.0954195261001587\n",
            "epoch 20 - batch 60 - loss 1.2038543224334717\n",
            "epoch 20 - batch 70 - loss 1.1506184339523315\n",
            "epoch 20 - batch 80 - loss 1.1233844757080078\n",
            "epoch 20 - batch 90 - loss 1.1501301527023315\n",
            "epoch 20 - batch 100 - loss 1.2190200090408325\n",
            "epoch 20 - batch 110 - loss 1.1173713207244873\n",
            "epoch 20 - batch 120 - loss 1.1769276857376099\n",
            "epoch 20 - batch 130 - loss 1.1807568073272705\n",
            "epoch 20 - batch 140 - loss 1.1521413326263428\n",
            "epoch 20 - batch 150 - loss 1.1389497518539429\n",
            "epoch 20 - batch 160 - loss 1.1412321329116821\n",
            "epoch 20 - batch 170 - loss 1.1503230333328247\n",
            "epoch 20 - batch 180 - loss 1.1142481565475464\n",
            "epoch 20 - batch 190 - loss 1.0941076278686523\n",
            "epoch 20 - batch 200 - loss 1.1228924989700317\n",
            "epoch 20 - batch 210 - loss 1.182660460472107\n",
            "epoch 20 - batch 220 - loss 1.1395695209503174\n",
            "epoch 20 - batch 230 - loss 1.1355619430541992\n",
            "epoch 20 - batch 240 - loss 1.1342440843582153\n",
            "epoch 20 - batch 250 - loss 1.2304900884628296\n",
            "epoch 20 - batch 260 - loss 1.1309901475906372\n",
            "epoch 20 - batch 270 - loss 1.135911226272583\n",
            "epoch 20 - batch 280 - loss 1.113251805305481\n",
            "epoch 20 - batch 290 - loss 1.1928986310958862\n",
            "epoch 20 - batch 300 - loss 1.1743929386138916\n",
            "epoch 20 - batch 310 - loss 1.1716420650482178\n",
            "epoch 20 - batch 320 - loss 1.1390317678451538\n",
            "epoch 20 - batch 330 - loss 1.0991666316986084\n",
            "epoch 20 - batch 340 - loss 1.156636357307434\n",
            "epoch 20 - batch 350 - loss 1.1357345581054688\n",
            "epoch 20 - batch 360 - loss 1.1629083156585693\n",
            "epoch 20 - batch 370 - loss 1.1024550199508667\n",
            "epoch 20 - batch 380 - loss 1.0945528745651245\n",
            "epoch 20 - batch 390 - loss 1.1035746335983276\n",
            "epoch 20 - batch 400 - loss 1.1523126363754272\n",
            "epoch 20 - batch 410 - loss 1.1954443454742432\n",
            "epoch 20 - batch 420 - loss 1.1460357904434204\n",
            "epoch 20 - batch 430 - loss 1.1207256317138672\n",
            "epoch 20 - batch 440 - loss 1.196106195449829\n",
            "epoch 20 - batch 450 - loss 1.1553670167922974\n",
            "epoch 20 - batch 460 - loss 1.0807511806488037\n",
            "epoch 20 - batch 470 - loss 1.0958054065704346\n",
            "epoch 20 - batch 480 - loss 1.1267451047897339\n",
            "epoch 20 - batch 490 - loss 1.0655852556228638\n",
            "epoch 20 - batch 500 - loss 1.1700232028961182\n",
            "epoch 20 - batch 510 - loss 1.1833583116531372\n",
            "epoch 20 - batch 520 - loss 1.0880839824676514\n",
            "epoch 20 - batch 530 - loss 1.1429027318954468\n",
            "epoch 20 - batch 540 - loss 1.1070830821990967\n",
            "epoch 20 - batch 550 - loss 1.1481664180755615\n",
            "epoch 20 - batch 560 - loss 1.1566985845565796\n",
            "epoch 20 - batch 570 - loss 1.1010459661483765\n",
            "epoch 20 - batch 580 - loss 1.1168327331542969\n",
            "epoch 20 - batch 590 - loss 1.1892917156219482\n",
            "epoch 20 - batch 600 - loss 1.052962064743042\n",
            "epoch 20 - batch 610 - loss 1.180618166923523\n",
            "epoch 20 training time: 216.74725532531738 sec\n",
            "evaluation of batch 0 took: 0.15668249130249023\n",
            "evaluation of batch 50 took: 0.15816450119018555\n",
            "evaluation of batch 100 took: 0.1569652557373047\n",
            "evaluation of batch 150 took: 0.15214276313781738\n",
            "evaluation of batch 200 took: 0.1521592140197754\n",
            "evaluation of batch 250 took: 0.15361475944519043\n",
            "evaluation of batch 300 took: 0.1568737030029297\n",
            "evaluation of batch 350 took: 0.19028139114379883\n",
            "evaluation of batch 400 took: 0.1505908966064453\n",
            "evaluation of batch 450 took: 0.154494047164917\n",
            "evaluation of batch 500 took: 0.15961003303527832\n",
            "evaluation of batch 550 took: 0.15682482719421387\n",
            "evaluation of batch 600 took: 0.15354657173156738\n",
            "epoch 20 evaluation on training data time: 95.92729830741882 sec\n",
            "evaluation of batch 0 took: 0.15776991844177246\n",
            "evaluation of batch 50 took: 0.15797042846679688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 20 evaluation on test data time: 22.145678997039795 sec\n",
            "epoch evaluation:  {'epoch': 20, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3557446>, 'test_rouge_1_p': 0.5412080084164349, 'test_rouge_1_r': 0.4920871562403368, 'test_rouge_1_f1': 0.5055872576253562, 'test_rouge_2_p': 0.264205602340368, 'test_rouge_2_r': 0.2497894683441559, 'test_rouge_2_f1': 0.2535450155763925, 'test_rouge_3_p': 0.10550193621482684, 'test_rouge_3_r': 0.09956879058441559, 'test_rouge_3_f1': 0.10109694682797363, 'test_rouge_L_p': 0.5390854797174551, 'test_rouge_L_r': 0.49046861109500645, 'test_rouge_L_f1': 0.5038037507282128}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 - batch 10 - loss 1.1643840074539185\n",
            "epoch 21 - batch 20 - loss 1.152817964553833\n",
            "epoch 21 - batch 30 - loss 1.1375200748443604\n",
            "epoch 21 - batch 40 - loss 1.1733828783035278\n",
            "epoch 21 - batch 50 - loss 1.15724778175354\n",
            "epoch 21 - batch 60 - loss 1.120429515838623\n",
            "epoch 21 - batch 70 - loss 1.149619460105896\n",
            "epoch 21 - batch 80 - loss 1.1598845720291138\n",
            "epoch 21 - batch 90 - loss 1.0946308374404907\n",
            "epoch 21 - batch 100 - loss 1.111506462097168\n",
            "epoch 21 - batch 110 - loss 1.1076220273971558\n",
            "epoch 21 - batch 120 - loss 1.1684786081314087\n",
            "epoch 21 - batch 130 - loss 1.1014564037322998\n",
            "epoch 21 - batch 140 - loss 1.147698163986206\n",
            "epoch 21 - batch 150 - loss 1.092316746711731\n",
            "epoch 21 - batch 160 - loss 1.1494920253753662\n",
            "epoch 21 - batch 170 - loss 1.0458000898361206\n",
            "epoch 21 - batch 180 - loss 1.2003356218338013\n",
            "epoch 21 - batch 190 - loss 1.015886902809143\n",
            "epoch 21 - batch 200 - loss 1.1129705905914307\n",
            "epoch 21 - batch 210 - loss 1.1556605100631714\n",
            "epoch 21 - batch 220 - loss 1.1062908172607422\n",
            "epoch 21 - batch 230 - loss 1.0739437341690063\n",
            "epoch 21 - batch 240 - loss 1.1644145250320435\n",
            "epoch 21 - batch 250 - loss 1.1905343532562256\n",
            "epoch 21 - batch 260 - loss 1.0500233173370361\n",
            "epoch 21 - batch 270 - loss 1.1101806163787842\n",
            "epoch 21 - batch 280 - loss 1.0971052646636963\n",
            "epoch 21 - batch 290 - loss 1.1915228366851807\n",
            "epoch 21 - batch 300 - loss 1.1225950717926025\n",
            "epoch 21 - batch 310 - loss 1.115090250968933\n",
            "epoch 21 - batch 320 - loss 1.1795673370361328\n",
            "epoch 21 - batch 330 - loss 1.084821343421936\n",
            "epoch 21 - batch 340 - loss 1.1544164419174194\n",
            "epoch 21 - batch 350 - loss 1.1493178606033325\n",
            "epoch 21 - batch 360 - loss 1.0976883172988892\n",
            "epoch 21 - batch 370 - loss 1.1855310201644897\n",
            "epoch 21 - batch 380 - loss 1.1185096502304077\n",
            "epoch 21 - batch 390 - loss 1.0756913423538208\n",
            "epoch 21 - batch 400 - loss 1.0926697254180908\n",
            "epoch 21 - batch 410 - loss 1.1736786365509033\n",
            "epoch 21 - batch 420 - loss 1.1257957220077515\n",
            "epoch 21 - batch 430 - loss 1.0768595933914185\n",
            "epoch 21 - batch 440 - loss 1.1793049573898315\n",
            "epoch 21 - batch 450 - loss 1.1540030241012573\n",
            "epoch 21 - batch 460 - loss 1.1217418909072876\n",
            "epoch 21 - batch 470 - loss 1.1145797967910767\n",
            "epoch 21 - batch 480 - loss 1.1358312368392944\n",
            "epoch 21 - batch 490 - loss 1.0681618452072144\n",
            "epoch 21 - batch 500 - loss 1.1622850894927979\n",
            "epoch 21 - batch 510 - loss 1.1812409162521362\n",
            "epoch 21 - batch 520 - loss 1.1150014400482178\n",
            "epoch 21 - batch 530 - loss 1.155898928642273\n",
            "epoch 21 - batch 540 - loss 1.0671519041061401\n",
            "epoch 21 - batch 550 - loss 1.1222056150436401\n",
            "epoch 21 - batch 560 - loss 1.1196404695510864\n",
            "epoch 21 - batch 570 - loss 1.1414549350738525\n",
            "epoch 21 - batch 580 - loss 1.1484425067901611\n",
            "epoch 21 - batch 590 - loss 1.182997465133667\n",
            "epoch 21 - batch 600 - loss 1.100520372390747\n",
            "epoch 21 - batch 610 - loss 1.0877728462219238\n",
            "epoch 21 training time: 217.17393684387207 sec\n",
            "evaluation of batch 0 took: 0.15208005905151367\n",
            "evaluation of batch 50 took: 0.15778374671936035\n",
            "evaluation of batch 100 took: 0.1534264087677002\n",
            "evaluation of batch 150 took: 0.15680432319641113\n",
            "evaluation of batch 200 took: 0.15079665184020996\n",
            "evaluation of batch 250 took: 0.16028904914855957\n",
            "evaluation of batch 300 took: 0.1523897647857666\n",
            "evaluation of batch 350 took: 0.1492917537689209\n",
            "evaluation of batch 400 took: 0.15449976921081543\n",
            "evaluation of batch 450 took: 0.16651415824890137\n",
            "evaluation of batch 500 took: 0.15375947952270508\n",
            "evaluation of batch 550 took: 0.15117835998535156\n",
            "evaluation of batch 600 took: 0.15511775016784668\n",
            "epoch 21 evaluation on training data time: 96.15641641616821 sec\n",
            "evaluation of batch 0 took: 0.1635749340057373\n",
            "evaluation of batch 50 took: 0.15416789054870605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 evaluation on test data time: 22.297763347625732 sec\n",
            "epoch evaluation:  {'epoch': 21, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.348316>, 'test_rouge_1_p': 0.5415338614138837, 'test_rouge_1_r': 0.4976945696602504, 'test_rouge_1_f1': 0.5091520072978067, 'test_rouge_2_p': 0.2674343885281387, 'test_rouge_2_r': 0.254103253517316, 'test_rouge_2_f1': 0.25734546261255886, 'test_rouge_3_p': 0.10765734916125537, 'test_rouge_3_r': 0.1021065848214286, 'test_rouge_3_f1': 0.10344441102672126, 'test_rouge_L_p': 0.5393163639745671, 'test_rouge_L_r': 0.49595378812036156, 'test_rouge_L_f1': 0.5072574295490542}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 saved checkpoint: models/checkpoints/baseline/ckpt-8\n",
            "epoch 22 - batch 10 - loss 1.127246618270874\n",
            "epoch 22 - batch 20 - loss 1.1636269092559814\n",
            "epoch 22 - batch 30 - loss 1.0549319982528687\n",
            "epoch 22 - batch 40 - loss 1.1311609745025635\n",
            "epoch 22 - batch 50 - loss 1.168149709701538\n",
            "epoch 22 - batch 60 - loss 1.1287740468978882\n",
            "epoch 22 - batch 70 - loss 1.1574748754501343\n",
            "epoch 22 - batch 80 - loss 1.020426630973816\n",
            "epoch 22 - batch 90 - loss 1.08103609085083\n",
            "epoch 22 - batch 100 - loss 1.0869197845458984\n",
            "epoch 22 - batch 110 - loss 1.1280860900878906\n",
            "epoch 22 - batch 120 - loss 1.2036811113357544\n",
            "epoch 22 - batch 130 - loss 1.1131302118301392\n",
            "epoch 22 - batch 140 - loss 1.0806843042373657\n",
            "epoch 22 - batch 150 - loss 1.1016219854354858\n",
            "epoch 22 - batch 160 - loss 1.0875908136367798\n",
            "epoch 22 - batch 170 - loss 1.1195870637893677\n",
            "epoch 22 - batch 180 - loss 1.0646356344223022\n",
            "epoch 22 - batch 190 - loss 1.1541707515716553\n",
            "epoch 22 - batch 200 - loss 1.1321932077407837\n",
            "epoch 22 - batch 210 - loss 1.0955666303634644\n",
            "epoch 22 - batch 220 - loss 1.1672831773757935\n",
            "epoch 22 - batch 230 - loss 1.052791953086853\n",
            "epoch 22 - batch 240 - loss 1.1094236373901367\n",
            "epoch 22 - batch 250 - loss 1.127189040184021\n",
            "epoch 22 - batch 260 - loss 1.06450355052948\n",
            "epoch 22 - batch 270 - loss 1.1326650381088257\n",
            "epoch 22 - batch 280 - loss 1.147735357284546\n",
            "epoch 22 - batch 290 - loss 1.1032062768936157\n",
            "epoch 22 - batch 300 - loss 1.1017416715621948\n",
            "epoch 22 - batch 310 - loss 1.1015809774398804\n",
            "epoch 22 - batch 320 - loss 1.0990575551986694\n",
            "epoch 22 - batch 330 - loss 1.0714279413223267\n",
            "epoch 22 - batch 340 - loss 1.0943812131881714\n",
            "epoch 22 - batch 350 - loss 1.1212003231048584\n",
            "epoch 22 - batch 360 - loss 1.086836338043213\n",
            "epoch 22 - batch 370 - loss 1.0661430358886719\n",
            "epoch 22 - batch 380 - loss 1.172647476196289\n",
            "epoch 22 - batch 390 - loss 1.0832270383834839\n",
            "epoch 22 - batch 400 - loss 1.1311472654342651\n",
            "epoch 22 - batch 410 - loss 1.124626636505127\n",
            "epoch 22 - batch 420 - loss 1.0929279327392578\n",
            "epoch 22 - batch 430 - loss 1.0983383655548096\n",
            "epoch 22 - batch 440 - loss 1.1152828931808472\n",
            "epoch 22 - batch 450 - loss 1.1155372858047485\n",
            "epoch 22 - batch 460 - loss 1.0785870552062988\n",
            "epoch 22 - batch 470 - loss 1.1325105428695679\n",
            "epoch 22 - batch 480 - loss 1.1511156558990479\n",
            "epoch 22 - batch 490 - loss 1.0947190523147583\n",
            "epoch 22 - batch 500 - loss 1.074062466621399\n",
            "epoch 22 - batch 510 - loss 1.0815731287002563\n",
            "epoch 22 - batch 520 - loss 1.0837011337280273\n",
            "epoch 22 - batch 530 - loss 1.1206693649291992\n",
            "epoch 22 - batch 540 - loss 1.144224762916565\n",
            "epoch 22 - batch 550 - loss 1.078119158744812\n",
            "epoch 22 - batch 560 - loss 1.082663655281067\n",
            "epoch 22 - batch 570 - loss 1.0637266635894775\n",
            "epoch 22 - batch 580 - loss 1.1020323038101196\n",
            "epoch 22 - batch 590 - loss 1.0738716125488281\n",
            "epoch 22 - batch 600 - loss 1.0634238719940186\n",
            "epoch 22 - batch 610 - loss 1.0756851434707642\n",
            "epoch 22 training time: 217.11552262306213 sec\n",
            "evaluation of batch 0 took: 0.15109038352966309\n",
            "evaluation of batch 50 took: 0.15310311317443848\n",
            "evaluation of batch 100 took: 0.15975689888000488\n",
            "evaluation of batch 150 took: 0.1542797088623047\n",
            "evaluation of batch 200 took: 0.15366148948669434\n",
            "evaluation of batch 250 took: 0.15602517127990723\n",
            "evaluation of batch 300 took: 0.16792607307434082\n",
            "evaluation of batch 350 took: 0.14719271659851074\n",
            "evaluation of batch 400 took: 0.14945006370544434\n",
            "evaluation of batch 450 took: 0.15969204902648926\n",
            "evaluation of batch 500 took: 0.15108728408813477\n",
            "evaluation of batch 550 took: 0.15284371376037598\n",
            "evaluation of batch 600 took: 0.1633303165435791\n",
            "epoch 22 evaluation on training data time: 96.41579985618591 sec\n",
            "evaluation of batch 0 took: 0.16405034065246582\n",
            "evaluation of batch 50 took: 0.15640854835510254\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 22 evaluation on test data time: 22.24324631690979 sec\n",
            "epoch evaluation:  {'epoch': 22, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3455291>, 'test_rouge_1_p': 0.5434704989950527, 'test_rouge_1_r': 0.5007209169179034, 'test_rouge_1_f1': 0.5117780677959753, 'test_rouge_2_p': 0.26941287878787884, 'test_rouge_2_r': 0.2566213896780304, 'test_rouge_2_f1': 0.25971438113597706, 'test_rouge_3_p': 0.10893935166396103, 'test_rouge_3_r': 0.10379464285714285, 'test_rouge_3_f1': 0.10493916967828804, 'test_rouge_L_p': 0.5412399263682743, 'test_rouge_L_r': 0.49895425677373206, 'test_rouge_L_f1': 0.5098608428458266}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 23 - batch 10 - loss 1.0999118089675903\n",
            "epoch 23 - batch 20 - loss 1.090757966041565\n",
            "epoch 23 - batch 30 - loss 1.0885201692581177\n",
            "epoch 23 - batch 40 - loss 1.0333136320114136\n",
            "epoch 23 - batch 50 - loss 1.1210768222808838\n",
            "epoch 23 - batch 60 - loss 1.0760643482208252\n",
            "epoch 23 - batch 70 - loss 1.1466598510742188\n",
            "epoch 23 - batch 80 - loss 1.0312213897705078\n",
            "epoch 23 - batch 90 - loss 1.0789834260940552\n",
            "epoch 23 - batch 100 - loss 1.1676969528198242\n",
            "epoch 23 - batch 110 - loss 1.0522288084030151\n",
            "epoch 23 - batch 120 - loss 1.1284611225128174\n",
            "epoch 23 - batch 130 - loss 1.0510810613632202\n",
            "epoch 23 - batch 140 - loss 1.070475697517395\n",
            "epoch 23 - batch 150 - loss 1.1479276418685913\n",
            "epoch 23 - batch 160 - loss 1.1354089975357056\n",
            "epoch 23 - batch 170 - loss 1.0979737043380737\n",
            "epoch 23 - batch 180 - loss 1.1252763271331787\n",
            "epoch 23 - batch 190 - loss 1.059792160987854\n",
            "epoch 23 - batch 200 - loss 1.1130796670913696\n",
            "epoch 23 - batch 210 - loss 1.1249935626983643\n",
            "epoch 23 - batch 220 - loss 1.0923205614089966\n",
            "epoch 23 - batch 230 - loss 1.0589754581451416\n",
            "epoch 23 - batch 240 - loss 1.0692436695098877\n",
            "epoch 23 - batch 250 - loss 1.0892095565795898\n",
            "epoch 23 - batch 260 - loss 1.056031346321106\n",
            "epoch 23 - batch 270 - loss 1.1462279558181763\n",
            "epoch 23 - batch 280 - loss 1.0765043497085571\n",
            "epoch 23 - batch 290 - loss 1.16938054561615\n",
            "epoch 23 - batch 300 - loss 1.1084480285644531\n",
            "epoch 23 - batch 310 - loss 1.054322600364685\n",
            "epoch 23 - batch 320 - loss 1.057188868522644\n",
            "epoch 23 - batch 330 - loss 1.0781439542770386\n",
            "epoch 23 - batch 340 - loss 1.0732282400131226\n",
            "epoch 23 - batch 350 - loss 1.1736465692520142\n",
            "epoch 23 - batch 360 - loss 1.0579434633255005\n",
            "epoch 23 - batch 370 - loss 1.093639612197876\n",
            "epoch 23 - batch 380 - loss 1.0375603437423706\n",
            "epoch 23 - batch 390 - loss 1.0850318670272827\n",
            "epoch 23 - batch 400 - loss 1.0899322032928467\n",
            "epoch 23 - batch 410 - loss 1.0928319692611694\n",
            "epoch 23 - batch 420 - loss 1.0904104709625244\n",
            "epoch 23 - batch 430 - loss 1.09430730342865\n",
            "epoch 23 - batch 440 - loss 1.0642043352127075\n",
            "epoch 23 - batch 450 - loss 1.0346603393554688\n",
            "epoch 23 - batch 460 - loss 1.0393650531768799\n",
            "epoch 23 - batch 470 - loss 1.0505586862564087\n",
            "epoch 23 - batch 480 - loss 1.0535929203033447\n",
            "epoch 23 - batch 490 - loss 1.0640387535095215\n",
            "epoch 23 - batch 500 - loss 1.0344369411468506\n",
            "epoch 23 - batch 510 - loss 1.0712850093841553\n",
            "epoch 23 - batch 520 - loss 1.0591799020767212\n",
            "epoch 23 - batch 530 - loss 1.086628794670105\n",
            "epoch 23 - batch 540 - loss 1.0238145589828491\n",
            "epoch 23 - batch 550 - loss 1.0238174200057983\n",
            "epoch 23 - batch 560 - loss 1.0685886144638062\n",
            "epoch 23 - batch 570 - loss 1.0750083923339844\n",
            "epoch 23 - batch 580 - loss 1.1544524431228638\n",
            "epoch 23 - batch 590 - loss 1.056890845298767\n",
            "epoch 23 - batch 600 - loss 1.085797905921936\n",
            "epoch 23 - batch 610 - loss 1.1075859069824219\n",
            "epoch 23 training time: 217.5292248725891 sec\n",
            "evaluation of batch 0 took: 0.15816545486450195\n",
            "evaluation of batch 50 took: 0.14998555183410645\n",
            "evaluation of batch 100 took: 0.15796327590942383\n",
            "evaluation of batch 150 took: 0.15832161903381348\n",
            "evaluation of batch 200 took: 0.15477919578552246\n",
            "evaluation of batch 250 took: 0.15804839134216309\n",
            "evaluation of batch 300 took: 0.15383005142211914\n",
            "evaluation of batch 350 took: 0.15426397323608398\n",
            "evaluation of batch 400 took: 0.16005969047546387\n",
            "evaluation of batch 450 took: 0.14800524711608887\n",
            "evaluation of batch 500 took: 0.15401887893676758\n",
            "evaluation of batch 550 took: 0.15597319602966309\n",
            "evaluation of batch 600 took: 0.1560382843017578\n",
            "epoch 23 evaluation on training data time: 96.72642707824707 sec\n",
            "evaluation of batch 0 took: 0.15935611724853516\n",
            "evaluation of batch 50 took: 0.15119457244873047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 23 evaluation on test data time: 22.657551527023315 sec\n",
            "epoch evaluation:  {'epoch': 23, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3402809>, 'test_rouge_1_p': 0.5466987718382806, 'test_rouge_1_r': 0.5016289026263914, 'test_rouge_1_f1': 0.5136377638077111, 'test_rouge_2_p': 0.27165051745129865, 'test_rouge_2_r': 0.2579725125135281, 'test_rouge_2_f1': 0.2613927606597047, 'test_rouge_3_p': 0.11066968513257575, 'test_rouge_3_r': 0.10512843276515146, 'test_rouge_3_f1': 0.10647571054937126, 'test_rouge_L_p': 0.5445062772978507, 'test_rouge_L_r': 0.49991707975803956, 'test_rouge_L_f1': 0.5117718739452961}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 - batch 10 - loss 1.0755441188812256\n",
            "epoch 24 - batch 20 - loss 1.123632788658142\n",
            "epoch 24 - batch 30 - loss 1.0514936447143555\n",
            "epoch 24 - batch 40 - loss 1.0569164752960205\n",
            "epoch 24 - batch 50 - loss 1.099369764328003\n",
            "epoch 24 - batch 60 - loss 1.152979850769043\n",
            "epoch 24 - batch 70 - loss 1.0528309345245361\n",
            "epoch 24 - batch 80 - loss 1.13351309299469\n",
            "epoch 24 - batch 90 - loss 1.0334081649780273\n",
            "epoch 24 - batch 100 - loss 1.0854135751724243\n",
            "epoch 24 - batch 110 - loss 1.0622999668121338\n",
            "epoch 24 - batch 120 - loss 1.0941005945205688\n",
            "epoch 24 - batch 130 - loss 1.0706666707992554\n",
            "epoch 24 - batch 140 - loss 1.0993367433547974\n",
            "epoch 24 - batch 150 - loss 1.0391091108322144\n",
            "epoch 24 - batch 160 - loss 1.0285675525665283\n",
            "epoch 24 - batch 170 - loss 1.131252646446228\n",
            "epoch 24 - batch 180 - loss 1.1293948888778687\n",
            "epoch 24 - batch 190 - loss 1.0884904861450195\n",
            "epoch 24 - batch 200 - loss 1.102545142173767\n",
            "epoch 24 - batch 210 - loss 1.1021684408187866\n",
            "epoch 24 - batch 220 - loss 1.1458381414413452\n",
            "epoch 24 - batch 230 - loss 1.0704454183578491\n",
            "epoch 24 - batch 240 - loss 1.0816527605056763\n",
            "epoch 24 - batch 250 - loss 1.1041783094406128\n",
            "epoch 24 - batch 260 - loss 1.0756489038467407\n",
            "epoch 24 - batch 270 - loss 1.1477059125900269\n",
            "epoch 24 - batch 280 - loss 1.1540353298187256\n",
            "epoch 24 - batch 290 - loss 1.0662338733673096\n",
            "epoch 24 - batch 300 - loss 1.01655912399292\n",
            "epoch 24 - batch 310 - loss 1.0711733102798462\n",
            "epoch 24 - batch 320 - loss 1.10448157787323\n",
            "epoch 24 - batch 330 - loss 1.12450110912323\n",
            "epoch 24 - batch 340 - loss 1.0516669750213623\n",
            "epoch 24 - batch 350 - loss 1.0696507692337036\n",
            "epoch 24 - batch 360 - loss 1.085899829864502\n",
            "epoch 24 - batch 370 - loss 1.1198605298995972\n",
            "epoch 24 - batch 380 - loss 1.015569806098938\n",
            "epoch 24 - batch 390 - loss 1.0940992832183838\n",
            "epoch 24 - batch 400 - loss 1.0671483278274536\n",
            "epoch 24 - batch 410 - loss 1.0402042865753174\n",
            "epoch 24 - batch 420 - loss 1.0260883569717407\n",
            "epoch 24 - batch 430 - loss 1.0207597017288208\n",
            "epoch 24 - batch 440 - loss 1.1506348848342896\n",
            "epoch 24 - batch 450 - loss 1.0545308589935303\n",
            "epoch 24 - batch 460 - loss 1.0097094774246216\n",
            "epoch 24 - batch 470 - loss 1.0348371267318726\n",
            "epoch 24 - batch 480 - loss 1.071081519126892\n",
            "epoch 24 - batch 490 - loss 1.066243290901184\n",
            "epoch 24 - batch 500 - loss 1.0203789472579956\n",
            "epoch 24 - batch 510 - loss 1.0726295709609985\n",
            "epoch 24 - batch 520 - loss 1.0786349773406982\n",
            "epoch 24 - batch 530 - loss 1.0751756429672241\n",
            "epoch 24 - batch 540 - loss 1.0974876880645752\n",
            "epoch 24 - batch 550 - loss 1.0217126607894897\n",
            "epoch 24 - batch 560 - loss 1.0826513767242432\n",
            "epoch 24 - batch 570 - loss 1.0595142841339111\n",
            "epoch 24 - batch 580 - loss 1.07297682762146\n",
            "epoch 24 - batch 590 - loss 1.0798321962356567\n",
            "epoch 24 - batch 600 - loss 1.004215121269226\n",
            "epoch 24 - batch 610 - loss 1.0963537693023682\n",
            "epoch 24 training time: 216.92185974121094 sec\n",
            "evaluation of batch 0 took: 0.157393217086792\n",
            "evaluation of batch 50 took: 0.16006159782409668\n",
            "evaluation of batch 100 took: 0.16592884063720703\n",
            "evaluation of batch 150 took: 0.15583300590515137\n",
            "evaluation of batch 200 took: 0.15850114822387695\n",
            "evaluation of batch 250 took: 0.15798616409301758\n",
            "evaluation of batch 300 took: 0.15950417518615723\n",
            "evaluation of batch 350 took: 0.15128493309020996\n",
            "evaluation of batch 400 took: 0.1585218906402588\n",
            "evaluation of batch 450 took: 0.15386962890625\n",
            "evaluation of batch 500 took: 0.17188215255737305\n",
            "evaluation of batch 550 took: 0.1683177947998047\n",
            "evaluation of batch 600 took: 0.15367460250854492\n",
            "epoch 24 evaluation on training data time: 97.78317165374756 sec\n",
            "evaluation of batch 0 took: 0.16343307495117188\n",
            "evaluation of batch 50 took: 0.1509723663330078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 evaluation on test data time: 22.327449321746826 sec\n",
            "epoch evaluation:  {'epoch': 24, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3396621>, 'test_rouge_1_p': 0.5480838962005256, 'test_rouge_1_r': 0.5030308163748455, 'test_rouge_1_f1': 0.5150477388181579, 'test_rouge_2_p': 0.27414138595779225, 'test_rouge_2_r': 0.260191549986472, 'test_rouge_2_f1': 0.2636762205850833, 'test_rouge_3_p': 0.11290563277867965, 'test_rouge_3_r': 0.10683149857954546, 'test_rouge_3_f1': 0.10830417335021128, 'test_rouge_L_p': 0.5459420717861005, 'test_rouge_L_r': 0.5013702373705166, 'test_rouge_L_f1': 0.5132308743801742}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 saved checkpoint: models/checkpoints/baseline/ckpt-9\n",
            "epoch 25 - batch 10 - loss 1.075804352760315\n",
            "epoch 25 - batch 20 - loss 1.1000607013702393\n",
            "epoch 25 - batch 30 - loss 1.08112370967865\n",
            "epoch 25 - batch 40 - loss 1.0309566259384155\n",
            "epoch 25 - batch 50 - loss 1.0970298051834106\n",
            "epoch 25 - batch 60 - loss 1.0573946237564087\n",
            "epoch 25 - batch 70 - loss 1.0828202962875366\n",
            "epoch 25 - batch 80 - loss 1.068273901939392\n",
            "epoch 25 - batch 90 - loss 1.063886284828186\n",
            "epoch 25 - batch 100 - loss 1.0607701539993286\n",
            "epoch 25 - batch 110 - loss 1.0629479885101318\n",
            "epoch 25 - batch 120 - loss 1.0783919095993042\n",
            "epoch 25 - batch 130 - loss 1.0067743062973022\n",
            "epoch 25 - batch 140 - loss 1.069196105003357\n",
            "epoch 25 - batch 150 - loss 1.0507581233978271\n",
            "epoch 25 - batch 160 - loss 1.0313456058502197\n",
            "epoch 25 - batch 170 - loss 1.0454742908477783\n",
            "epoch 25 - batch 180 - loss 1.088234782218933\n",
            "epoch 25 - batch 190 - loss 1.0820187330245972\n",
            "epoch 25 - batch 200 - loss 1.0623588562011719\n",
            "epoch 25 - batch 210 - loss 1.0958832502365112\n",
            "epoch 25 - batch 220 - loss 1.1083009243011475\n",
            "epoch 25 - batch 230 - loss 1.0811587572097778\n",
            "epoch 25 - batch 240 - loss 1.035321831703186\n",
            "epoch 25 - batch 250 - loss 1.0654442310333252\n",
            "epoch 25 - batch 260 - loss 1.061229944229126\n",
            "epoch 25 - batch 270 - loss 1.0872749090194702\n",
            "epoch 25 - batch 280 - loss 1.0539630651474\n",
            "epoch 25 - batch 290 - loss 1.1154567003250122\n",
            "epoch 25 - batch 300 - loss 1.1033847332000732\n",
            "epoch 25 - batch 310 - loss 1.0632809400558472\n",
            "epoch 25 - batch 320 - loss 1.1277904510498047\n",
            "epoch 25 - batch 330 - loss 1.055909514427185\n",
            "epoch 25 - batch 340 - loss 1.080689787864685\n",
            "epoch 25 - batch 350 - loss 1.1251088380813599\n",
            "epoch 25 - batch 360 - loss 0.9973918199539185\n",
            "epoch 25 - batch 370 - loss 1.063286542892456\n",
            "epoch 25 - batch 380 - loss 0.9880254864692688\n",
            "epoch 25 - batch 390 - loss 1.0511572360992432\n",
            "epoch 25 - batch 400 - loss 1.035449504852295\n",
            "epoch 25 - batch 410 - loss 1.0474320650100708\n",
            "epoch 25 - batch 420 - loss 1.1271785497665405\n",
            "epoch 25 - batch 430 - loss 1.0514098405838013\n",
            "epoch 25 - batch 440 - loss 1.02902352809906\n",
            "epoch 25 - batch 450 - loss 1.1184996366500854\n",
            "epoch 25 - batch 460 - loss 0.998446524143219\n",
            "epoch 25 - batch 470 - loss 1.0400348901748657\n",
            "epoch 25 - batch 480 - loss 1.0555830001831055\n",
            "epoch 25 - batch 490 - loss 1.0265228748321533\n",
            "epoch 25 - batch 500 - loss 1.0267871618270874\n",
            "epoch 25 - batch 510 - loss 1.0441579818725586\n",
            "epoch 25 - batch 520 - loss 1.0251086950302124\n",
            "epoch 25 - batch 530 - loss 1.0894373655319214\n",
            "epoch 25 - batch 540 - loss 1.023801565170288\n",
            "epoch 25 - batch 550 - loss 1.0255918502807617\n",
            "epoch 25 - batch 560 - loss 1.004289150238037\n",
            "epoch 25 - batch 570 - loss 1.056014895439148\n",
            "epoch 25 - batch 580 - loss 1.0850374698638916\n",
            "epoch 25 - batch 590 - loss 1.0901020765304565\n",
            "epoch 25 - batch 600 - loss 1.0513496398925781\n",
            "epoch 25 - batch 610 - loss 1.0206263065338135\n",
            "epoch 25 training time: 215.63181519508362 sec\n",
            "evaluation of batch 0 took: 0.15567255020141602\n",
            "evaluation of batch 50 took: 0.1557912826538086\n",
            "evaluation of batch 100 took: 0.16103601455688477\n",
            "evaluation of batch 150 took: 0.15837359428405762\n",
            "evaluation of batch 200 took: 0.15418267250061035\n",
            "evaluation of batch 250 took: 0.16059207916259766\n",
            "evaluation of batch 300 took: 0.1567671298980713\n",
            "evaluation of batch 350 took: 0.16184496879577637\n",
            "evaluation of batch 400 took: 0.15420031547546387\n",
            "evaluation of batch 450 took: 0.1564173698425293\n",
            "evaluation of batch 500 took: 0.1581275463104248\n",
            "evaluation of batch 550 took: 0.15775227546691895\n",
            "evaluation of batch 600 took: 0.1538546085357666\n",
            "epoch 25 evaluation on training data time: 97.16856622695923 sec\n",
            "evaluation of batch 0 took: 0.15908265113830566\n",
            "evaluation of batch 50 took: 0.15992069244384766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 25 evaluation on test data time: 22.260239362716675 sec\n",
            "epoch evaluation:  {'epoch': 25, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3362479>, 'test_rouge_1_p': 0.5480498342803031, 'test_rouge_1_r': 0.5049263561958873, 'test_rouge_1_f1': 0.5162686294585642, 'test_rouge_2_p': 0.27472563244047626, 'test_rouge_2_r': 0.26139026988636355, 'test_rouge_2_f1': 0.2646939943117399, 'test_rouge_3_p': 0.11270440171807357, 'test_rouge_3_r': 0.1069473332656926, 'test_rouge_3_f1': 0.10835262904877863, 'test_rouge_L_p': 0.5459213266272418, 'test_rouge_L_r': 0.5032366071428571, 'test_rouge_L_f1': 0.5144368431864627}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 26 - batch 10 - loss 1.0299347639083862\n",
            "epoch 26 - batch 20 - loss 1.0016000270843506\n",
            "epoch 26 - batch 30 - loss 1.0087058544158936\n",
            "epoch 26 - batch 40 - loss 1.0894222259521484\n",
            "epoch 26 - batch 50 - loss 1.035625696182251\n",
            "epoch 26 - batch 60 - loss 1.0336847305297852\n",
            "epoch 26 - batch 70 - loss 1.0400030612945557\n",
            "epoch 26 - batch 80 - loss 1.1074086427688599\n",
            "epoch 26 - batch 90 - loss 0.9865677952766418\n",
            "epoch 26 - batch 100 - loss 1.0922060012817383\n",
            "epoch 26 - batch 110 - loss 1.0890920162200928\n",
            "epoch 26 - batch 120 - loss 1.0830427408218384\n",
            "epoch 26 - batch 130 - loss 1.0926541090011597\n",
            "epoch 26 - batch 140 - loss 1.0879288911819458\n",
            "epoch 26 - batch 150 - loss 1.0283050537109375\n",
            "epoch 26 - batch 160 - loss 1.0182827711105347\n",
            "epoch 26 - batch 170 - loss 1.0471271276474\n",
            "epoch 26 - batch 180 - loss 1.0351027250289917\n",
            "epoch 26 - batch 190 - loss 1.0802230834960938\n",
            "epoch 26 - batch 200 - loss 1.0445181131362915\n",
            "epoch 26 - batch 210 - loss 1.072512149810791\n",
            "epoch 26 - batch 220 - loss 1.0345758199691772\n",
            "epoch 26 - batch 230 - loss 1.0809072256088257\n",
            "epoch 26 - batch 240 - loss 1.0499826669692993\n",
            "epoch 26 - batch 250 - loss 1.0714961290359497\n",
            "epoch 26 - batch 260 - loss 1.0474406480789185\n",
            "epoch 26 - batch 270 - loss 1.0183780193328857\n",
            "epoch 26 - batch 280 - loss 1.0716861486434937\n",
            "epoch 26 - batch 290 - loss 1.059175729751587\n",
            "epoch 26 - batch 300 - loss 1.041696548461914\n",
            "epoch 26 - batch 310 - loss 1.0120314359664917\n",
            "epoch 26 - batch 320 - loss 1.0453897714614868\n",
            "epoch 26 - batch 330 - loss 1.0159204006195068\n",
            "epoch 26 - batch 340 - loss 1.0327248573303223\n",
            "epoch 26 - batch 350 - loss 1.0790350437164307\n",
            "epoch 26 - batch 360 - loss 1.073857069015503\n",
            "epoch 26 - batch 370 - loss 0.9995787739753723\n",
            "epoch 26 - batch 380 - loss 1.0756967067718506\n",
            "epoch 26 - batch 390 - loss 0.9919372200965881\n",
            "epoch 26 - batch 400 - loss 0.9785458445549011\n",
            "epoch 26 - batch 410 - loss 0.99824458360672\n",
            "epoch 26 - batch 420 - loss 1.0457528829574585\n",
            "epoch 26 - batch 430 - loss 1.037090539932251\n",
            "epoch 26 - batch 440 - loss 1.070622444152832\n",
            "epoch 26 - batch 450 - loss 1.0392390489578247\n",
            "epoch 26 - batch 460 - loss 1.0389740467071533\n",
            "epoch 26 - batch 470 - loss 1.0594465732574463\n",
            "epoch 26 - batch 480 - loss 1.0277515649795532\n",
            "epoch 26 - batch 490 - loss 1.0110888481140137\n",
            "epoch 26 - batch 500 - loss 1.030029296875\n",
            "epoch 26 - batch 510 - loss 1.0344364643096924\n",
            "epoch 26 - batch 520 - loss 1.0225452184677124\n",
            "epoch 26 - batch 530 - loss 1.0767476558685303\n",
            "epoch 26 - batch 540 - loss 1.0044792890548706\n",
            "epoch 26 - batch 550 - loss 0.970229983329773\n",
            "epoch 26 - batch 560 - loss 1.0315392017364502\n",
            "epoch 26 - batch 570 - loss 1.083420753479004\n",
            "epoch 26 - batch 580 - loss 1.0513328313827515\n",
            "epoch 26 - batch 590 - loss 1.0569250583648682\n",
            "epoch 26 - batch 600 - loss 1.0252195596694946\n",
            "epoch 26 - batch 610 - loss 1.0339466333389282\n",
            "epoch 26 training time: 215.28856420516968 sec\n",
            "evaluation of batch 0 took: 0.15418624877929688\n",
            "evaluation of batch 50 took: 0.1545088291168213\n",
            "evaluation of batch 100 took: 0.15304064750671387\n",
            "evaluation of batch 150 took: 0.1540982723236084\n",
            "evaluation of batch 200 took: 0.15828299522399902\n",
            "evaluation of batch 250 took: 0.15182805061340332\n",
            "evaluation of batch 300 took: 0.1632533073425293\n",
            "evaluation of batch 350 took: 0.1629657745361328\n",
            "evaluation of batch 400 took: 0.15823864936828613\n",
            "evaluation of batch 450 took: 0.1552426815032959\n",
            "evaluation of batch 500 took: 0.1573331356048584\n",
            "evaluation of batch 550 took: 0.15297651290893555\n",
            "evaluation of batch 600 took: 0.14807891845703125\n",
            "epoch 26 evaluation on training data time: 97.42231369018555 sec\n",
            "evaluation of batch 0 took: 0.15944743156433105\n",
            "evaluation of batch 50 took: 0.15743541717529297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 26 evaluation on test data time: 22.413817167282104 sec\n",
            "epoch evaluation:  {'epoch': 26, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.334581>, 'test_rouge_1_p': 0.5483249265615335, 'test_rouge_1_r': 0.5070867516040506, 'test_rouge_1_f1': 0.5177253454292823, 'test_rouge_2_p': 0.2763473180465367, 'test_rouge_2_r': 0.26353067505411243, 'test_rouge_2_f1': 0.2666908368676329, 'test_rouge_3_p': 0.11368878517316017, 'test_rouge_3_r': 0.10815662202380952, 'test_rouge_3_f1': 0.10951780037299011, 'test_rouge_L_p': 0.5461922215619203, 'test_rouge_L_r': 0.5053615515808596, 'test_rouge_L_f1': 0.5158726281018539}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 - batch 10 - loss 1.0620050430297852\n",
            "epoch 27 - batch 20 - loss 1.0434542894363403\n",
            "epoch 27 - batch 30 - loss 1.0043567419052124\n",
            "epoch 27 - batch 40 - loss 1.1001465320587158\n",
            "epoch 27 - batch 50 - loss 1.0580923557281494\n",
            "epoch 27 - batch 60 - loss 0.9773993492126465\n",
            "epoch 27 - batch 70 - loss 0.9938869476318359\n",
            "epoch 27 - batch 80 - loss 1.1343594789505005\n",
            "epoch 27 - batch 90 - loss 1.1019337177276611\n",
            "epoch 27 - batch 100 - loss 1.0798434019088745\n",
            "epoch 27 - batch 110 - loss 1.0718870162963867\n",
            "epoch 27 - batch 120 - loss 1.0323847532272339\n",
            "epoch 27 - batch 130 - loss 1.0310771465301514\n",
            "epoch 27 - batch 140 - loss 1.0400761365890503\n",
            "epoch 27 - batch 150 - loss 1.0078620910644531\n",
            "epoch 27 - batch 160 - loss 1.0290945768356323\n",
            "epoch 27 - batch 170 - loss 1.0127253532409668\n",
            "epoch 27 - batch 180 - loss 1.0705268383026123\n",
            "epoch 27 - batch 190 - loss 1.04975426197052\n",
            "epoch 27 - batch 200 - loss 1.009033203125\n",
            "epoch 27 - batch 210 - loss 1.0753223896026611\n",
            "epoch 27 - batch 220 - loss 1.0252866744995117\n",
            "epoch 27 - batch 230 - loss 1.041383981704712\n",
            "epoch 27 - batch 240 - loss 1.0559478998184204\n",
            "epoch 27 - batch 250 - loss 1.0208921432495117\n",
            "epoch 27 - batch 260 - loss 1.0530375242233276\n",
            "epoch 27 - batch 270 - loss 1.0825697183609009\n",
            "epoch 27 - batch 280 - loss 1.0056699514389038\n",
            "epoch 27 - batch 290 - loss 1.0411885976791382\n",
            "epoch 27 - batch 300 - loss 1.0780192613601685\n",
            "epoch 27 - batch 310 - loss 1.005618929862976\n",
            "epoch 27 - batch 320 - loss 1.0303865671157837\n",
            "epoch 27 - batch 330 - loss 0.9651240110397339\n",
            "epoch 27 - batch 340 - loss 1.0536974668502808\n",
            "epoch 27 - batch 350 - loss 1.0313735008239746\n",
            "epoch 27 - batch 360 - loss 1.0154731273651123\n",
            "epoch 27 - batch 370 - loss 1.0225402116775513\n",
            "epoch 27 - batch 380 - loss 0.9915589094161987\n",
            "epoch 27 - batch 390 - loss 1.0475584268569946\n",
            "epoch 27 - batch 400 - loss 1.03342604637146\n",
            "epoch 27 - batch 410 - loss 1.1065210103988647\n",
            "epoch 27 - batch 420 - loss 0.9567733407020569\n",
            "epoch 27 - batch 430 - loss 1.0021865367889404\n",
            "epoch 27 - batch 440 - loss 1.0643036365509033\n",
            "epoch 27 - batch 450 - loss 0.996562123298645\n",
            "epoch 27 - batch 460 - loss 0.9808641672134399\n",
            "epoch 27 - batch 470 - loss 1.0570622682571411\n",
            "epoch 27 - batch 480 - loss 1.0487343072891235\n",
            "epoch 27 - batch 490 - loss 1.0131144523620605\n",
            "epoch 27 - batch 500 - loss 1.0153123140335083\n",
            "epoch 27 - batch 510 - loss 1.0597198009490967\n",
            "epoch 27 - batch 520 - loss 1.0391042232513428\n",
            "epoch 27 - batch 530 - loss 1.0123491287231445\n",
            "epoch 27 - batch 540 - loss 1.0507763624191284\n",
            "epoch 27 - batch 550 - loss 1.0286153554916382\n",
            "epoch 27 - batch 560 - loss 0.9765289425849915\n",
            "epoch 27 - batch 570 - loss 0.9767125248908997\n",
            "epoch 27 - batch 580 - loss 1.0000892877578735\n",
            "epoch 27 - batch 590 - loss 1.0131688117980957\n",
            "epoch 27 - batch 600 - loss 0.9848355650901794\n",
            "epoch 27 - batch 610 - loss 1.014088749885559\n",
            "epoch 27 training time: 215.64104509353638 sec\n",
            "evaluation of batch 0 took: 0.15652132034301758\n",
            "evaluation of batch 50 took: 0.1614241600036621\n",
            "evaluation of batch 100 took: 0.15724849700927734\n",
            "evaluation of batch 150 took: 0.15228652954101562\n",
            "evaluation of batch 200 took: 0.15970921516418457\n",
            "evaluation of batch 250 took: 0.15765762329101562\n",
            "evaluation of batch 300 took: 0.15832161903381348\n",
            "evaluation of batch 350 took: 0.15308856964111328\n",
            "evaluation of batch 400 took: 0.15660548210144043\n",
            "evaluation of batch 450 took: 0.15476655960083008\n",
            "evaluation of batch 500 took: 0.15251445770263672\n",
            "evaluation of batch 550 took: 0.16089534759521484\n",
            "evaluation of batch 600 took: 0.16230416297912598\n",
            "epoch 27 evaluation on training data time: 97.33400869369507 sec\n",
            "evaluation of batch 0 took: 0.1641232967376709\n",
            "evaluation of batch 50 took: 0.15601611137390137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 evaluation on test data time: 22.61010718345642 sec\n",
            "epoch evaluation:  {'epoch': 27, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3333462>, 'test_rouge_1_p': 0.5472227456323439, 'test_rouge_1_r': 0.5114767231466448, 'test_rouge_1_f1': 0.519818490942679, 'test_rouge_2_p': 0.27738412303841986, 'test_rouge_2_r': 0.2665406858766233, 'test_rouge_2_f1': 0.2688364349592283, 'test_rouge_3_p': 0.11546562161796532, 'test_rouge_3_r': 0.11083392518939392, 'test_rouge_3_f1': 0.11177867039334675, 'test_rouge_L_p': 0.545048882479128, 'test_rouge_L_r': 0.5096609329100956, 'test_rouge_L_f1': 0.5178943088471545}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 saved checkpoint: models/checkpoints/baseline/ckpt-10\n",
            "epoch 28 - batch 10 - loss 1.027738332748413\n",
            "epoch 28 - batch 20 - loss 1.0363346338272095\n",
            "epoch 28 - batch 30 - loss 0.9611167907714844\n",
            "epoch 28 - batch 40 - loss 0.9958234429359436\n",
            "epoch 28 - batch 50 - loss 0.9769358038902283\n",
            "epoch 28 - batch 60 - loss 0.9851537346839905\n",
            "epoch 28 - batch 70 - loss 1.110581398010254\n",
            "epoch 28 - batch 80 - loss 1.2257194519042969\n",
            "epoch 28 - batch 90 - loss 1.2983357906341553\n",
            "epoch 28 - batch 100 - loss 1.3107534646987915\n",
            "epoch 28 - batch 110 - loss 1.1494085788726807\n",
            "epoch 28 - batch 120 - loss 1.1701024770736694\n",
            "epoch 28 - batch 130 - loss 1.1078242063522339\n",
            "epoch 28 - batch 140 - loss 1.1498233079910278\n",
            "epoch 28 - batch 150 - loss 1.1248881816864014\n",
            "epoch 28 - batch 160 - loss 1.1381375789642334\n",
            "epoch 28 - batch 170 - loss 1.1378718614578247\n",
            "epoch 28 - batch 180 - loss 1.128836750984192\n",
            "epoch 28 - batch 190 - loss 1.1586710214614868\n",
            "epoch 28 - batch 200 - loss 1.1109682321548462\n",
            "epoch 28 - batch 210 - loss 1.1660680770874023\n",
            "epoch 28 - batch 220 - loss 1.1081312894821167\n",
            "epoch 28 - batch 230 - loss 1.0856258869171143\n",
            "epoch 28 - batch 240 - loss 1.0667575597763062\n",
            "epoch 28 - batch 250 - loss 1.0967923402786255\n",
            "epoch 28 - batch 260 - loss 1.0989627838134766\n",
            "epoch 28 - batch 270 - loss 1.0699957609176636\n",
            "epoch 28 - batch 280 - loss 1.1339722871780396\n",
            "epoch 28 - batch 290 - loss 1.0241233110427856\n",
            "epoch 28 - batch 300 - loss 1.1009366512298584\n",
            "epoch 28 - batch 310 - loss 1.0746387243270874\n",
            "epoch 28 - batch 320 - loss 1.0690711736679077\n",
            "epoch 28 - batch 330 - loss 1.061136245727539\n",
            "epoch 28 - batch 340 - loss 1.0844594240188599\n",
            "epoch 28 - batch 350 - loss 1.0293625593185425\n",
            "epoch 28 - batch 360 - loss 1.0873953104019165\n",
            "epoch 28 - batch 370 - loss 1.0633347034454346\n",
            "epoch 28 - batch 380 - loss 0.9605488181114197\n",
            "epoch 28 - batch 390 - loss 1.0188630819320679\n",
            "epoch 28 - batch 400 - loss 1.041463017463684\n",
            "epoch 28 - batch 410 - loss 1.035131573677063\n",
            "epoch 28 - batch 420 - loss 1.0308271646499634\n",
            "epoch 28 - batch 430 - loss 1.0542593002319336\n",
            "epoch 28 - batch 440 - loss 1.056745171546936\n",
            "epoch 28 - batch 450 - loss 1.0175637006759644\n",
            "epoch 28 - batch 460 - loss 1.0540860891342163\n",
            "epoch 28 - batch 470 - loss 0.9317057728767395\n",
            "epoch 28 - batch 480 - loss 1.0222232341766357\n",
            "epoch 28 - batch 490 - loss 1.0357578992843628\n",
            "epoch 28 - batch 500 - loss 1.0442055463790894\n",
            "epoch 28 - batch 510 - loss 1.0131504535675049\n",
            "epoch 28 - batch 520 - loss 1.0283509492874146\n",
            "epoch 28 - batch 530 - loss 1.0417582988739014\n",
            "epoch 28 - batch 540 - loss 1.0096454620361328\n",
            "epoch 28 - batch 550 - loss 1.04769766330719\n",
            "epoch 28 - batch 560 - loss 1.0483086109161377\n",
            "epoch 28 - batch 570 - loss 1.085485816001892\n",
            "epoch 28 - batch 580 - loss 1.0262113809585571\n",
            "epoch 28 - batch 590 - loss 0.9911637902259827\n",
            "epoch 28 - batch 600 - loss 0.9257970452308655\n",
            "epoch 28 - batch 610 - loss 1.063029408454895\n",
            "epoch 28 training time: 217.4408152103424 sec\n",
            "evaluation of batch 0 took: 0.15335917472839355\n",
            "evaluation of batch 50 took: 0.15226197242736816\n",
            "evaluation of batch 100 took: 0.15610623359680176\n",
            "evaluation of batch 150 took: 0.15884828567504883\n",
            "evaluation of batch 200 took: 0.15595531463623047\n",
            "evaluation of batch 250 took: 0.1544022560119629\n",
            "evaluation of batch 300 took: 0.15716290473937988\n",
            "evaluation of batch 350 took: 0.1510486602783203\n",
            "evaluation of batch 400 took: 0.15515780448913574\n",
            "evaluation of batch 450 took: 0.1531822681427002\n",
            "evaluation of batch 500 took: 0.1516122817993164\n",
            "evaluation of batch 550 took: 0.1585981845855713\n",
            "evaluation of batch 600 took: 0.15871858596801758\n",
            "epoch 28 evaluation on training data time: 96.7817280292511 sec\n",
            "evaluation of batch 0 took: 0.16031694412231445\n",
            "evaluation of batch 50 took: 0.15710854530334473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 28 evaluation on test data time: 22.57779097557068 sec\n",
            "epoch evaluation:  {'epoch': 28, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3310825>, 'test_rouge_1_p': 0.5502301595354049, 'test_rouge_1_r': 0.5060344556856835, 'test_rouge_1_f1': 0.5177560818369716, 'test_rouge_2_p': 0.27776037439123374, 'test_rouge_2_r': 0.26341632000811693, 'test_rouge_2_f1': 0.2671250595516272, 'test_rouge_3_p': 0.11539819230248917, 'test_rouge_3_r': 0.10907167376893945, 'test_rouge_3_f1': 0.11072179463641518, 'test_rouge_L_p': 0.5482308335265923, 'test_rouge_L_r': 0.5044405510783858, 'test_rouge_L_f1': 0.5160275635512799}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 29 - batch 10 - loss 1.0476598739624023\n",
            "epoch 29 - batch 20 - loss 1.0201135873794556\n",
            "epoch 29 - batch 30 - loss 1.0032764673233032\n",
            "epoch 29 - batch 40 - loss 0.9509864449501038\n",
            "epoch 29 - batch 50 - loss 1.041244387626648\n",
            "epoch 29 - batch 60 - loss 1.0040132999420166\n",
            "epoch 29 - batch 70 - loss 1.0405548810958862\n",
            "epoch 29 - batch 80 - loss 1.0004349946975708\n",
            "epoch 29 - batch 90 - loss 1.02988600730896\n",
            "epoch 29 - batch 100 - loss 1.0676319599151611\n",
            "epoch 29 - batch 110 - loss 1.0517328977584839\n",
            "epoch 29 - batch 120 - loss 1.0560413599014282\n",
            "epoch 29 - batch 130 - loss 1.0238456726074219\n",
            "epoch 29 - batch 140 - loss 0.9983853101730347\n",
            "epoch 29 - batch 150 - loss 1.0741335153579712\n",
            "epoch 29 - batch 160 - loss 1.033505916595459\n",
            "epoch 29 - batch 170 - loss 0.9998775124549866\n",
            "epoch 29 - batch 180 - loss 1.1513807773590088\n",
            "epoch 29 - batch 190 - loss 1.026057481765747\n",
            "epoch 29 - batch 200 - loss 1.0097180604934692\n",
            "epoch 29 - batch 210 - loss 1.0854374170303345\n",
            "epoch 29 - batch 220 - loss 1.0413280725479126\n",
            "epoch 29 - batch 230 - loss 1.004263162612915\n",
            "epoch 29 - batch 240 - loss 1.0258926153182983\n",
            "epoch 29 - batch 250 - loss 1.036067247390747\n",
            "epoch 29 - batch 260 - loss 0.9779466986656189\n",
            "epoch 29 - batch 270 - loss 1.0305482149124146\n",
            "epoch 29 - batch 280 - loss 0.9699183106422424\n",
            "epoch 29 - batch 290 - loss 1.023510217666626\n",
            "epoch 29 - batch 300 - loss 1.0320631265640259\n",
            "epoch 29 - batch 310 - loss 0.9847617745399475\n",
            "epoch 29 - batch 320 - loss 1.0584161281585693\n",
            "epoch 29 - batch 330 - loss 0.9967642426490784\n",
            "epoch 29 - batch 340 - loss 1.0252584218978882\n",
            "epoch 29 - batch 350 - loss 0.9932978749275208\n",
            "epoch 29 - batch 360 - loss 1.041793704032898\n",
            "epoch 29 - batch 370 - loss 1.0080701112747192\n",
            "epoch 29 - batch 380 - loss 0.9835060834884644\n",
            "epoch 29 - batch 390 - loss 0.9824997782707214\n",
            "epoch 29 - batch 400 - loss 0.9706100821495056\n",
            "epoch 29 - batch 410 - loss 1.0266577005386353\n",
            "epoch 29 - batch 420 - loss 0.9651526212692261\n",
            "epoch 29 - batch 430 - loss 0.9904078841209412\n",
            "epoch 29 - batch 440 - loss 1.056846022605896\n",
            "epoch 29 - batch 450 - loss 1.052099347114563\n",
            "epoch 29 - batch 460 - loss 0.9540256857872009\n",
            "epoch 29 - batch 470 - loss 1.0185986757278442\n",
            "epoch 29 - batch 480 - loss 0.974273145198822\n",
            "epoch 29 - batch 490 - loss 1.0108150243759155\n",
            "epoch 29 - batch 500 - loss 0.9668547511100769\n",
            "epoch 29 - batch 510 - loss 0.9786352515220642\n",
            "epoch 29 - batch 520 - loss 0.9376150369644165\n",
            "epoch 29 - batch 530 - loss 1.0222877264022827\n",
            "epoch 29 - batch 540 - loss 0.9807265400886536\n",
            "epoch 29 - batch 550 - loss 0.901656448841095\n",
            "epoch 29 - batch 560 - loss 0.9729170203208923\n",
            "epoch 29 - batch 570 - loss 1.0161954164505005\n",
            "epoch 29 - batch 580 - loss 0.9677100777626038\n",
            "epoch 29 - batch 590 - loss 0.9941720962524414\n",
            "epoch 29 - batch 600 - loss 0.9923094511032104\n",
            "epoch 29 - batch 610 - loss 1.017450213432312\n",
            "epoch 29 training time: 215.38537740707397 sec\n",
            "evaluation of batch 0 took: 0.15465521812438965\n",
            "evaluation of batch 50 took: 0.15415239334106445\n",
            "evaluation of batch 100 took: 0.15895581245422363\n",
            "evaluation of batch 150 took: 0.16528081893920898\n",
            "evaluation of batch 200 took: 0.15014910697937012\n",
            "evaluation of batch 250 took: 0.15842962265014648\n",
            "evaluation of batch 300 took: 0.1570429801940918\n",
            "evaluation of batch 350 took: 0.1561582088470459\n",
            "evaluation of batch 400 took: 0.1607987880706787\n",
            "evaluation of batch 450 took: 0.1559453010559082\n",
            "evaluation of batch 500 took: 0.15607690811157227\n",
            "evaluation of batch 550 took: 0.15307188034057617\n",
            "evaluation of batch 600 took: 0.16566896438598633\n",
            "epoch 29 evaluation on training data time: 97.23501014709473 sec\n",
            "evaluation of batch 0 took: 0.15920639038085938\n",
            "evaluation of batch 50 took: 0.1593780517578125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 29 evaluation on test data time: 22.411437034606934 sec\n",
            "epoch evaluation:  {'epoch': 29, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3260733>, 'test_rouge_1_p': 0.549900229978355, 'test_rouge_1_r': 0.5113291516890845, 'test_rouge_1_f1': 0.5208767750360435, 'test_rouge_2_p': 0.2791833654626625, 'test_rouge_2_r': 0.2674994926948053, 'test_rouge_2_f1': 0.2701241144486376, 'test_rouge_3_p': 0.11720842633928573, 'test_rouge_3_r': 0.11226769649621213, 'test_rouge_3_f1': 0.11338596222106265, 'test_rouge_L_p': 0.5478049689335962, 'test_rouge_L_r': 0.5096269011866109, 'test_rouge_L_f1': 0.5190471708829816}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 - batch 10 - loss 0.9995235800743103\n",
            "epoch 30 - batch 20 - loss 1.0281486511230469\n",
            "epoch 30 - batch 30 - loss 0.9001432061195374\n",
            "epoch 30 - batch 40 - loss 0.9871088266372681\n",
            "epoch 30 - batch 50 - loss 0.9629983305931091\n",
            "epoch 30 - batch 60 - loss 0.9974890351295471\n",
            "epoch 30 - batch 70 - loss 1.0523083209991455\n",
            "epoch 30 - batch 80 - loss 0.9988294243812561\n",
            "epoch 30 - batch 90 - loss 0.9668635129928589\n",
            "epoch 30 - batch 100 - loss 0.9941205382347107\n",
            "epoch 30 - batch 110 - loss 1.0297077894210815\n",
            "epoch 30 - batch 120 - loss 1.0775827169418335\n",
            "epoch 30 - batch 130 - loss 0.982065737247467\n",
            "epoch 30 - batch 140 - loss 0.9750608801841736\n",
            "epoch 30 - batch 150 - loss 1.026523232460022\n",
            "epoch 30 - batch 160 - loss 1.0108073949813843\n",
            "epoch 30 - batch 170 - loss 1.0082550048828125\n",
            "epoch 30 - batch 180 - loss 1.0367792844772339\n",
            "epoch 30 - batch 190 - loss 0.961060106754303\n",
            "epoch 30 - batch 200 - loss 1.0319106578826904\n",
            "epoch 30 - batch 210 - loss 1.0018244981765747\n",
            "epoch 30 - batch 220 - loss 1.016693353652954\n",
            "epoch 30 - batch 230 - loss 0.9567967057228088\n",
            "epoch 30 - batch 240 - loss 1.0541698932647705\n",
            "epoch 30 - batch 250 - loss 0.9982331991195679\n",
            "epoch 30 - batch 260 - loss 0.9980407953262329\n",
            "epoch 30 - batch 270 - loss 0.9925814867019653\n",
            "epoch 30 - batch 280 - loss 1.0067113637924194\n",
            "epoch 30 - batch 290 - loss 1.0093605518341064\n",
            "epoch 30 - batch 300 - loss 1.0106569528579712\n",
            "epoch 30 - batch 310 - loss 0.9853312373161316\n",
            "epoch 30 - batch 320 - loss 0.9536474943161011\n",
            "epoch 30 - batch 330 - loss 0.9627396464347839\n",
            "epoch 30 - batch 340 - loss 0.948471188545227\n",
            "epoch 30 - batch 350 - loss 0.9868270754814148\n",
            "epoch 30 - batch 360 - loss 0.969629168510437\n",
            "epoch 30 - batch 370 - loss 0.9840812683105469\n",
            "epoch 30 - batch 380 - loss 0.9944772720336914\n",
            "epoch 30 - batch 390 - loss 0.9307817816734314\n",
            "epoch 30 - batch 400 - loss 0.9993857145309448\n",
            "epoch 30 - batch 410 - loss 0.9408779740333557\n",
            "epoch 30 - batch 420 - loss 0.9973187446594238\n",
            "epoch 30 - batch 430 - loss 1.0003366470336914\n",
            "epoch 30 - batch 440 - loss 1.0064085721969604\n",
            "epoch 30 - batch 450 - loss 1.0378644466400146\n",
            "epoch 30 - batch 460 - loss 0.9884375929832458\n",
            "epoch 30 - batch 470 - loss 0.9839965105056763\n",
            "epoch 30 - batch 480 - loss 1.0098704099655151\n",
            "epoch 30 - batch 490 - loss 0.9898849129676819\n",
            "epoch 30 - batch 500 - loss 0.9645878672599792\n",
            "epoch 30 - batch 510 - loss 0.9921635389328003\n",
            "epoch 30 - batch 520 - loss 0.9722325205802917\n",
            "epoch 30 - batch 530 - loss 0.9089328050613403\n",
            "epoch 30 - batch 540 - loss 0.9940012693405151\n",
            "epoch 30 - batch 550 - loss 0.9359887838363647\n",
            "epoch 30 - batch 560 - loss 1.017103910446167\n",
            "epoch 30 - batch 570 - loss 0.9868678450584412\n",
            "epoch 30 - batch 580 - loss 0.9406830072402954\n",
            "epoch 30 - batch 590 - loss 1.055119276046753\n",
            "epoch 30 - batch 600 - loss 0.9030989408493042\n",
            "epoch 30 - batch 610 - loss 0.9852194786071777\n",
            "epoch 30 training time: 215.26551985740662 sec\n",
            "evaluation of batch 0 took: 0.15536975860595703\n",
            "evaluation of batch 50 took: 0.15866994857788086\n",
            "evaluation of batch 100 took: 0.1564643383026123\n",
            "evaluation of batch 150 took: 0.15650534629821777\n",
            "evaluation of batch 200 took: 0.15239357948303223\n",
            "evaluation of batch 250 took: 0.15704703330993652\n",
            "evaluation of batch 300 took: 0.15497565269470215\n",
            "evaluation of batch 350 took: 0.15289783477783203\n",
            "evaluation of batch 400 took: 0.15973210334777832\n",
            "evaluation of batch 450 took: 0.15678644180297852\n",
            "evaluation of batch 500 took: 0.1576673984527588\n",
            "evaluation of batch 550 took: 0.15718960762023926\n",
            "evaluation of batch 600 took: 0.15809869766235352\n",
            "epoch 30 evaluation on training data time: 97.27802586555481 sec\n",
            "evaluation of batch 0 took: 0.15818047523498535\n",
            "evaluation of batch 50 took: 0.1508786678314209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 evaluation on test data time: 22.320628881454468 sec\n",
            "epoch evaluation:  {'epoch': 30, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3286761>, 'test_rouge_1_p': 0.551737792062848, 'test_rouge_1_r': 0.5121212423179498, 'test_rouge_1_f1': 0.5221625496673689, 'test_rouge_2_p': 0.2816184303977274, 'test_rouge_2_r': 0.26867918864989176, 'test_rouge_2_f1': 0.2718130591703795, 'test_rouge_3_p': 0.1182568570752165, 'test_rouge_3_r': 0.11250232514880953, 'test_rouge_3_f1': 0.11391151631106991, 'test_rouge_L_p': 0.5496790388740724, 'test_rouge_L_r': 0.5104484638315553, 'test_rouge_L_f1': 0.5203628419740434}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 saved checkpoint: models/checkpoints/baseline/ckpt-11\n",
            "epoch 31 - batch 10 - loss 1.0042284727096558\n",
            "epoch 31 - batch 20 - loss 1.0375378131866455\n",
            "epoch 31 - batch 30 - loss 0.958811342716217\n",
            "epoch 31 - batch 40 - loss 1.0461244583129883\n",
            "epoch 31 - batch 50 - loss 0.9886273741722107\n",
            "epoch 31 - batch 60 - loss 1.0527212619781494\n",
            "epoch 31 - batch 70 - loss 1.0076932907104492\n",
            "epoch 31 - batch 80 - loss 0.9959083795547485\n",
            "epoch 31 - batch 90 - loss 1.0150134563446045\n",
            "epoch 31 - batch 100 - loss 1.024859070777893\n",
            "epoch 31 - batch 110 - loss 1.0137983560562134\n",
            "epoch 31 - batch 120 - loss 0.9830387234687805\n",
            "epoch 31 - batch 130 - loss 0.9573551416397095\n",
            "epoch 31 - batch 140 - loss 0.9764113426208496\n",
            "epoch 31 - batch 150 - loss 1.0145504474639893\n",
            "epoch 31 - batch 160 - loss 1.0520979166030884\n",
            "epoch 31 - batch 170 - loss 1.0466705560684204\n",
            "epoch 31 - batch 180 - loss 1.0108107328414917\n",
            "epoch 31 - batch 190 - loss 0.9902524948120117\n",
            "epoch 31 - batch 200 - loss 0.9721647500991821\n",
            "epoch 31 - batch 210 - loss 0.9871081113815308\n",
            "epoch 31 - batch 220 - loss 1.0402313470840454\n",
            "epoch 31 - batch 230 - loss 1.013482928276062\n",
            "epoch 31 - batch 240 - loss 1.0103731155395508\n",
            "epoch 31 - batch 250 - loss 1.0630156993865967\n",
            "epoch 31 - batch 260 - loss 0.9887024164199829\n",
            "epoch 31 - batch 270 - loss 0.9540943503379822\n",
            "epoch 31 - batch 280 - loss 1.0036447048187256\n",
            "epoch 31 - batch 290 - loss 1.0320004224777222\n",
            "epoch 31 - batch 300 - loss 0.968760073184967\n",
            "epoch 31 - batch 310 - loss 0.9671334028244019\n",
            "epoch 31 - batch 320 - loss 0.9782719016075134\n",
            "epoch 31 - batch 330 - loss 0.9750736355781555\n",
            "epoch 31 - batch 340 - loss 0.9176139831542969\n",
            "epoch 31 - batch 350 - loss 0.9879773259162903\n",
            "epoch 31 - batch 360 - loss 0.9437838792800903\n",
            "epoch 31 - batch 370 - loss 0.9345859885215759\n",
            "epoch 31 - batch 380 - loss 0.9373370409011841\n",
            "epoch 31 - batch 390 - loss 0.9490612745285034\n",
            "epoch 31 - batch 400 - loss 1.0068439245224\n",
            "epoch 31 - batch 410 - loss 0.9880313277244568\n",
            "epoch 31 - batch 420 - loss 1.0144108533859253\n",
            "epoch 31 - batch 430 - loss 0.9713051915168762\n",
            "epoch 31 - batch 440 - loss 0.9704486131668091\n",
            "epoch 31 - batch 450 - loss 0.9931144118309021\n",
            "epoch 31 - batch 460 - loss 0.9628655314445496\n",
            "epoch 31 - batch 470 - loss 0.9529433250427246\n",
            "epoch 31 - batch 480 - loss 1.0019609928131104\n",
            "epoch 31 - batch 490 - loss 0.9779587984085083\n",
            "epoch 31 - batch 500 - loss 0.9927907586097717\n",
            "epoch 31 - batch 510 - loss 1.020053505897522\n",
            "epoch 31 - batch 520 - loss 0.9427371621131897\n",
            "epoch 31 - batch 530 - loss 0.9195011258125305\n",
            "epoch 31 - batch 540 - loss 0.97661292552948\n",
            "epoch 31 - batch 550 - loss 0.9919903874397278\n",
            "epoch 31 - batch 560 - loss 1.0370676517486572\n",
            "epoch 31 - batch 570 - loss 0.9738886952400208\n",
            "epoch 31 - batch 580 - loss 0.9688157439231873\n",
            "epoch 31 - batch 590 - loss 0.9996098279953003\n",
            "epoch 31 - batch 600 - loss 0.9470587372779846\n",
            "epoch 31 - batch 610 - loss 0.955271303653717\n",
            "epoch 31 training time: 218.0629744529724 sec\n",
            "evaluation of batch 0 took: 0.15570521354675293\n",
            "evaluation of batch 50 took: 0.16434550285339355\n",
            "evaluation of batch 100 took: 0.1565864086151123\n",
            "evaluation of batch 150 took: 0.1535627841949463\n",
            "evaluation of batch 200 took: 0.1566624641418457\n",
            "evaluation of batch 250 took: 0.16035103797912598\n",
            "evaluation of batch 300 took: 0.15523910522460938\n",
            "evaluation of batch 350 took: 0.15523982048034668\n",
            "evaluation of batch 400 took: 0.1571826934814453\n",
            "evaluation of batch 450 took: 0.15137052536010742\n",
            "evaluation of batch 500 took: 0.15591812133789062\n",
            "evaluation of batch 550 took: 0.15794920921325684\n",
            "evaluation of batch 600 took: 0.15588641166687012\n",
            "epoch 31 evaluation on training data time: 98.16279816627502 sec\n",
            "evaluation of batch 0 took: 0.16518115997314453\n",
            "evaluation of batch 50 took: 0.15337014198303223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 31 evaluation on test data time: 22.59321641921997 sec\n",
            "epoch evaluation:  {'epoch': 31, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3286332>, 'test_rouge_1_p': 0.5522843228200371, 'test_rouge_1_r': 0.5134902114254792, 'test_rouge_1_f1': 0.5231557228367547, 'test_rouge_2_p': 0.2814296705898268, 'test_rouge_2_r': 0.26940611471861475, 'test_rouge_2_f1': 0.2721623173847707, 'test_rouge_3_p': 0.11775969798430738, 'test_rouge_3_r': 0.112694044237013, 'test_rouge_3_f1': 0.11387498832392806, 'test_rouge_L_p': 0.5502377993100648, 'test_rouge_L_r': 0.5117794152462121, 'test_rouge_L_f1': 0.5213429434050121}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 32 - batch 10 - loss 0.9262866973876953\n",
            "epoch 32 - batch 20 - loss 0.991284191608429\n",
            "epoch 32 - batch 30 - loss 1.008144736289978\n",
            "epoch 32 - batch 40 - loss 0.9376904368400574\n",
            "epoch 32 - batch 50 - loss 1.0335103273391724\n",
            "epoch 32 - batch 60 - loss 0.9232751131057739\n",
            "epoch 32 - batch 70 - loss 1.0026286840438843\n",
            "epoch 32 - batch 80 - loss 0.9906714558601379\n",
            "epoch 32 - batch 90 - loss 1.0111455917358398\n",
            "epoch 32 - batch 100 - loss 0.9652267694473267\n",
            "epoch 32 - batch 110 - loss 0.9677214026451111\n",
            "epoch 32 - batch 120 - loss 1.003554105758667\n",
            "epoch 32 - batch 130 - loss 1.002992868423462\n",
            "epoch 32 - batch 140 - loss 1.0099551677703857\n",
            "epoch 32 - batch 150 - loss 0.9905780553817749\n",
            "epoch 32 - batch 160 - loss 0.952885627746582\n",
            "epoch 32 - batch 170 - loss 0.9899646043777466\n",
            "epoch 32 - batch 180 - loss 0.9994922280311584\n",
            "epoch 32 - batch 190 - loss 0.9596482515335083\n",
            "epoch 32 - batch 200 - loss 0.9295519590377808\n",
            "epoch 32 - batch 210 - loss 1.0438190698623657\n",
            "epoch 32 - batch 220 - loss 1.0267318487167358\n",
            "epoch 32 - batch 230 - loss 0.9692525863647461\n",
            "epoch 32 - batch 240 - loss 0.9755038022994995\n",
            "epoch 32 - batch 250 - loss 0.9772157669067383\n",
            "epoch 32 - batch 260 - loss 0.9729397892951965\n",
            "epoch 32 - batch 270 - loss 0.9947611689567566\n",
            "epoch 32 - batch 280 - loss 1.0107471942901611\n",
            "epoch 32 - batch 290 - loss 0.9700339436531067\n",
            "epoch 32 - batch 300 - loss 1.0196154117584229\n",
            "epoch 32 - batch 310 - loss 0.9906191825866699\n",
            "epoch 32 - batch 320 - loss 0.9565353393554688\n",
            "epoch 32 - batch 330 - loss 0.9896319508552551\n",
            "epoch 32 - batch 340 - loss 0.9437651634216309\n",
            "epoch 32 - batch 350 - loss 0.9824447631835938\n",
            "epoch 32 - batch 360 - loss 0.9855111837387085\n",
            "epoch 32 - batch 370 - loss 0.9702933430671692\n",
            "epoch 32 - batch 380 - loss 0.9403371214866638\n",
            "epoch 32 - batch 390 - loss 0.9466294646263123\n",
            "epoch 32 - batch 400 - loss 0.9736876487731934\n",
            "epoch 32 - batch 410 - loss 0.9584385752677917\n",
            "epoch 32 - batch 420 - loss 0.9076303839683533\n",
            "epoch 32 - batch 430 - loss 1.003438115119934\n",
            "epoch 32 - batch 440 - loss 1.0059773921966553\n",
            "epoch 32 - batch 450 - loss 0.9851173758506775\n",
            "epoch 32 - batch 460 - loss 0.935420036315918\n",
            "epoch 32 - batch 470 - loss 0.9349905848503113\n",
            "epoch 32 - batch 480 - loss 0.9676201939582825\n",
            "epoch 32 - batch 490 - loss 0.9787240624427795\n",
            "epoch 32 - batch 500 - loss 0.9763160347938538\n",
            "epoch 32 - batch 510 - loss 0.9849756956100464\n",
            "epoch 32 - batch 520 - loss 0.9635433554649353\n",
            "epoch 32 - batch 530 - loss 0.9957414269447327\n",
            "epoch 32 - batch 540 - loss 0.9295223951339722\n",
            "epoch 32 - batch 550 - loss 0.9953646659851074\n",
            "epoch 32 - batch 560 - loss 0.9736538529396057\n",
            "epoch 32 - batch 570 - loss 0.9384785890579224\n",
            "epoch 32 - batch 580 - loss 1.0151089429855347\n",
            "epoch 32 - batch 590 - loss 0.9329313635826111\n",
            "epoch 32 - batch 600 - loss 0.899605393409729\n",
            "epoch 32 - batch 610 - loss 1.0079975128173828\n",
            "epoch 32 training time: 215.25139832496643 sec\n",
            "evaluation of batch 0 took: 0.1552433967590332\n",
            "evaluation of batch 50 took: 0.16149544715881348\n",
            "evaluation of batch 100 took: 0.15254831314086914\n",
            "evaluation of batch 150 took: 0.16423344612121582\n",
            "evaluation of batch 200 took: 0.149749755859375\n",
            "evaluation of batch 250 took: 0.1589193344116211\n",
            "evaluation of batch 300 took: 0.15599751472473145\n",
            "evaluation of batch 350 took: 0.15465831756591797\n",
            "evaluation of batch 400 took: 0.16686248779296875\n",
            "evaluation of batch 450 took: 0.15861845016479492\n",
            "evaluation of batch 500 took: 0.15254902839660645\n",
            "evaluation of batch 550 took: 0.15731167793273926\n",
            "evaluation of batch 600 took: 0.15707731246948242\n",
            "epoch 32 evaluation on training data time: 97.61741495132446 sec\n",
            "evaluation of batch 0 took: 0.1716015338897705\n",
            "evaluation of batch 50 took: 0.16021060943603516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 32 evaluation on test data time: 22.531904458999634 sec\n",
            "epoch evaluation:  {'epoch': 32, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3299521>, 'test_rouge_1_p': 0.5516085198283859, 'test_rouge_1_r': 0.5144124801909402, 'test_rouge_1_f1': 0.5236054053354428, 'test_rouge_2_p': 0.2826357886904763, 'test_rouge_2_r': 0.27020596590909085, 'test_rouge_2_f1': 0.27316332643164004, 'test_rouge_3_p': 0.11853904558982682, 'test_rouge_3_r': 0.11326032366071427, 'test_rouge_3_f1': 0.11448480137793758, 'test_rouge_L_p': 0.5495377785347094, 'test_rouge_L_r': 0.5127172051348946, 'test_rouge_L_f1': 0.5217898476630468}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 - batch 10 - loss 0.9861221313476562\n",
            "epoch 33 - batch 20 - loss 0.9972711205482483\n",
            "epoch 33 - batch 30 - loss 0.973107635974884\n",
            "epoch 33 - batch 40 - loss 0.9814419150352478\n",
            "epoch 33 - batch 50 - loss 0.9758769273757935\n",
            "epoch 33 - batch 60 - loss 0.9489212036132812\n",
            "epoch 33 - batch 70 - loss 1.0078083276748657\n",
            "epoch 33 - batch 80 - loss 0.9678019881248474\n",
            "epoch 33 - batch 90 - loss 0.9744516015052795\n",
            "epoch 33 - batch 100 - loss 1.0155125856399536\n",
            "epoch 33 - batch 110 - loss 0.9621242880821228\n",
            "epoch 33 - batch 120 - loss 1.0051957368850708\n",
            "epoch 33 - batch 130 - loss 0.9769400954246521\n",
            "epoch 33 - batch 140 - loss 0.9670135378837585\n",
            "epoch 33 - batch 150 - loss 0.9615674614906311\n",
            "epoch 33 - batch 160 - loss 0.9991642832756042\n",
            "epoch 33 - batch 170 - loss 1.0117143392562866\n",
            "epoch 33 - batch 180 - loss 0.9803548455238342\n",
            "epoch 33 - batch 190 - loss 0.9869303107261658\n",
            "epoch 33 - batch 200 - loss 0.9329001307487488\n",
            "epoch 33 - batch 210 - loss 0.9803145527839661\n",
            "epoch 33 - batch 220 - loss 1.0295003652572632\n",
            "epoch 33 - batch 230 - loss 1.0268417596817017\n",
            "epoch 33 - batch 240 - loss 1.0025043487548828\n",
            "epoch 33 - batch 250 - loss 1.006125807762146\n",
            "epoch 33 - batch 260 - loss 0.9604693651199341\n",
            "epoch 33 - batch 270 - loss 0.9510883688926697\n",
            "epoch 33 - batch 280 - loss 0.9700775742530823\n",
            "epoch 33 - batch 290 - loss 0.9850009083747864\n",
            "epoch 33 - batch 300 - loss 0.9376530647277832\n",
            "epoch 33 - batch 310 - loss 0.9655555486679077\n",
            "epoch 33 - batch 320 - loss 0.9947277903556824\n",
            "epoch 33 - batch 330 - loss 1.006688117980957\n",
            "epoch 33 - batch 340 - loss 1.0042740106582642\n",
            "epoch 33 - batch 350 - loss 0.9356260299682617\n",
            "epoch 33 - batch 360 - loss 0.9085583090782166\n",
            "epoch 33 - batch 370 - loss 0.9070363640785217\n",
            "epoch 33 - batch 380 - loss 0.9459626078605652\n",
            "epoch 33 - batch 390 - loss 0.9381301999092102\n",
            "epoch 33 - batch 400 - loss 0.9620246887207031\n",
            "epoch 33 - batch 410 - loss 0.9760102033615112\n",
            "epoch 33 - batch 420 - loss 0.9806793928146362\n",
            "epoch 33 - batch 430 - loss 0.9501172304153442\n",
            "epoch 33 - batch 440 - loss 0.9643402695655823\n",
            "epoch 33 - batch 450 - loss 0.9735687971115112\n",
            "epoch 33 - batch 460 - loss 0.9167591333389282\n",
            "epoch 33 - batch 470 - loss 0.9236064553260803\n",
            "epoch 33 - batch 480 - loss 0.9957805871963501\n",
            "epoch 33 - batch 490 - loss 0.9285617470741272\n",
            "epoch 33 - batch 500 - loss 0.9478066563606262\n",
            "epoch 33 - batch 510 - loss 0.9577576518058777\n",
            "epoch 33 - batch 520 - loss 0.9468474984169006\n",
            "epoch 33 - batch 530 - loss 0.8980607390403748\n",
            "epoch 33 - batch 540 - loss 0.9570429921150208\n",
            "epoch 33 - batch 550 - loss 0.9507578611373901\n",
            "epoch 33 - batch 560 - loss 0.9561654329299927\n",
            "epoch 33 - batch 570 - loss 0.9453924894332886\n",
            "epoch 33 - batch 580 - loss 0.9466449022293091\n",
            "epoch 33 - batch 590 - loss 0.9791098833084106\n",
            "epoch 33 - batch 600 - loss 0.9540976881980896\n",
            "epoch 33 - batch 610 - loss 0.9673812985420227\n",
            "epoch 33 training time: 217.30632305145264 sec\n",
            "evaluation of batch 0 took: 0.15767240524291992\n",
            "evaluation of batch 50 took: 0.15856456756591797\n",
            "evaluation of batch 100 took: 0.15479135513305664\n",
            "evaluation of batch 150 took: 0.15589547157287598\n",
            "evaluation of batch 200 took: 0.16186285018920898\n",
            "evaluation of batch 250 took: 0.1486365795135498\n",
            "evaluation of batch 300 took: 0.15219449996948242\n",
            "evaluation of batch 350 took: 0.16014313697814941\n",
            "evaluation of batch 400 took: 0.1567842960357666\n",
            "evaluation of batch 450 took: 0.15312862396240234\n",
            "evaluation of batch 500 took: 0.15567803382873535\n",
            "evaluation of batch 550 took: 0.16193175315856934\n",
            "evaluation of batch 600 took: 0.1509408950805664\n",
            "epoch 33 evaluation on training data time: 97.65452671051025 sec\n",
            "evaluation of batch 0 took: 0.16095805168151855\n",
            "evaluation of batch 50 took: 0.1515028476715088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 evaluation on test data time: 22.538851976394653 sec\n",
            "epoch evaluation:  {'epoch': 33, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3340279>, 'test_rouge_1_p': 0.5488053566597092, 'test_rouge_1_r': 0.514234108060838, 'test_rouge_1_f1': 0.5222630935420098, 'test_rouge_2_p': 0.2822506594967534, 'test_rouge_2_r': 0.2710470356466451, 'test_rouge_2_f1': 0.2734705690880865, 'test_rouge_3_p': 0.11827461275703466, 'test_rouge_3_r': 0.11371267079274894, 'test_rouge_3_f1': 0.11465642956993402, 'test_rouge_L_p': 0.5467189130623837, 'test_rouge_L_r': 0.5124512020717379, 'test_rouge_L_f1': 0.5203909601345174}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 saved checkpoint: models/checkpoints/baseline/ckpt-12\n",
            "epoch 34 - batch 10 - loss 0.969371497631073\n",
            "epoch 34 - batch 20 - loss 0.9169282913208008\n",
            "epoch 34 - batch 30 - loss 0.8984485268592834\n",
            "epoch 34 - batch 40 - loss 0.9192309379577637\n",
            "epoch 34 - batch 50 - loss 0.9512456059455872\n",
            "epoch 34 - batch 60 - loss 0.9032115936279297\n",
            "epoch 34 - batch 70 - loss 0.9439383149147034\n",
            "epoch 34 - batch 80 - loss 0.9653149247169495\n",
            "epoch 34 - batch 90 - loss 0.9629049897193909\n",
            "epoch 34 - batch 100 - loss 0.9514995217323303\n",
            "epoch 34 - batch 110 - loss 1.0088626146316528\n",
            "epoch 34 - batch 120 - loss 0.9237362742424011\n",
            "epoch 34 - batch 130 - loss 0.9197489619255066\n",
            "epoch 34 - batch 140 - loss 0.9680189490318298\n",
            "epoch 34 - batch 150 - loss 0.9900792241096497\n",
            "epoch 34 - batch 160 - loss 0.8746322989463806\n",
            "epoch 34 - batch 170 - loss 0.9868981242179871\n",
            "epoch 34 - batch 180 - loss 0.9803687334060669\n",
            "epoch 34 - batch 190 - loss 0.9888508915901184\n",
            "epoch 34 - batch 200 - loss 0.9786379933357239\n",
            "epoch 34 - batch 210 - loss 1.0029908418655396\n",
            "epoch 34 - batch 220 - loss 0.9195589423179626\n",
            "epoch 34 - batch 230 - loss 0.9364302754402161\n",
            "epoch 34 - batch 240 - loss 0.9364370703697205\n",
            "epoch 34 - batch 250 - loss 0.9238638281822205\n",
            "epoch 34 - batch 260 - loss 0.9187111258506775\n",
            "epoch 34 - batch 270 - loss 0.9395344853401184\n",
            "epoch 34 - batch 280 - loss 0.9535247087478638\n",
            "epoch 34 - batch 290 - loss 0.9917100071907043\n",
            "epoch 34 - batch 300 - loss 0.9422645568847656\n",
            "epoch 34 - batch 310 - loss 0.949295699596405\n",
            "epoch 34 - batch 320 - loss 0.9833882451057434\n",
            "epoch 34 - batch 330 - loss 0.966852605342865\n",
            "epoch 34 - batch 340 - loss 0.9693365097045898\n",
            "epoch 34 - batch 350 - loss 0.9472326636314392\n",
            "epoch 34 - batch 360 - loss 0.9320845007896423\n",
            "epoch 34 - batch 370 - loss 0.9402480125427246\n",
            "epoch 34 - batch 380 - loss 0.9157125353813171\n",
            "epoch 34 - batch 390 - loss 0.9270843863487244\n",
            "epoch 34 - batch 400 - loss 0.9776764512062073\n",
            "epoch 34 - batch 410 - loss 0.9639632105827332\n",
            "epoch 34 - batch 420 - loss 0.9360915422439575\n",
            "epoch 34 - batch 430 - loss 0.9101141691207886\n",
            "epoch 34 - batch 440 - loss 1.0161103010177612\n",
            "epoch 34 - batch 450 - loss 0.985304057598114\n",
            "epoch 34 - batch 460 - loss 0.9258273243904114\n",
            "epoch 34 - batch 470 - loss 0.9786542057991028\n",
            "epoch 34 - batch 480 - loss 0.9351209998130798\n",
            "epoch 34 - batch 490 - loss 0.9147641062736511\n",
            "epoch 34 - batch 500 - loss 0.8933643102645874\n",
            "epoch 34 - batch 510 - loss 0.940369188785553\n",
            "epoch 34 - batch 520 - loss 0.980958104133606\n",
            "epoch 34 - batch 530 - loss 0.9092494249343872\n",
            "epoch 34 - batch 540 - loss 0.9662737250328064\n",
            "epoch 34 - batch 550 - loss 0.9256998896598816\n",
            "epoch 34 - batch 560 - loss 0.8833455443382263\n",
            "epoch 34 - batch 570 - loss 1.017359733581543\n",
            "epoch 34 - batch 580 - loss 1.0016241073608398\n",
            "epoch 34 - batch 590 - loss 0.9571463465690613\n",
            "epoch 34 - batch 600 - loss 0.9234320521354675\n",
            "epoch 34 - batch 610 - loss 1.0207688808441162\n",
            "epoch 34 training time: 216.31242752075195 sec\n",
            "evaluation of batch 0 took: 0.15431785583496094\n",
            "evaluation of batch 50 took: 0.15823769569396973\n",
            "evaluation of batch 100 took: 0.16458535194396973\n",
            "evaluation of batch 150 took: 0.16529297828674316\n",
            "evaluation of batch 200 took: 0.15272283554077148\n",
            "evaluation of batch 250 took: 0.1535320281982422\n",
            "evaluation of batch 300 took: 0.16135025024414062\n",
            "evaluation of batch 350 took: 0.1662449836730957\n",
            "evaluation of batch 400 took: 0.1578657627105713\n",
            "evaluation of batch 450 took: 0.1583404541015625\n",
            "evaluation of batch 500 took: 0.16187429428100586\n",
            "evaluation of batch 550 took: 0.15913128852844238\n",
            "evaluation of batch 600 took: 0.15847182273864746\n",
            "epoch 34 evaluation on training data time: 98.067875623703 sec\n",
            "evaluation of batch 0 took: 0.16440606117248535\n",
            "evaluation of batch 50 took: 0.15295648574829102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 34 evaluation on test data time: 22.62519907951355 sec\n",
            "epoch evaluation:  {'epoch': 34, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3351574>, 'test_rouge_1_p': 0.5507977374188311, 'test_rouge_1_r': 0.5145958347827767, 'test_rouge_1_f1': 0.5232671932848221, 'test_rouge_2_p': 0.2812163910308441, 'test_rouge_2_r': 0.2700104420319263, 'test_rouge_2_f1': 0.272542175875171, 'test_rouge_3_p': 0.11861598687770562, 'test_rouge_3_r': 0.11363847740800867, 'test_rouge_3_f1': 0.1147831149505257, 'test_rouge_L_p': 0.5486157513431508, 'test_rouge_L_r': 0.5127627720122139, 'test_rouge_L_f1': 0.5213251095213857}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 35 - batch 10 - loss 0.9426695108413696\n",
            "epoch 35 - batch 20 - loss 0.9536885619163513\n",
            "epoch 35 - batch 30 - loss 0.8823578953742981\n",
            "epoch 35 - batch 40 - loss 0.9389711022377014\n",
            "epoch 35 - batch 50 - loss 0.9517661929130554\n",
            "epoch 35 - batch 60 - loss 0.9263624548912048\n",
            "epoch 35 - batch 70 - loss 0.9899842143058777\n",
            "epoch 35 - batch 80 - loss 0.9958987236022949\n",
            "epoch 35 - batch 90 - loss 0.9868836998939514\n",
            "epoch 35 - batch 100 - loss 1.0305825471878052\n",
            "epoch 35 - batch 110 - loss 0.9846347570419312\n",
            "epoch 35 - batch 120 - loss 0.9623000025749207\n",
            "epoch 35 - batch 130 - loss 0.9546961188316345\n",
            "epoch 35 - batch 140 - loss 0.9433017373085022\n",
            "epoch 35 - batch 150 - loss 1.014817476272583\n",
            "epoch 35 - batch 160 - loss 0.9442617297172546\n",
            "epoch 35 - batch 170 - loss 0.909429669380188\n",
            "epoch 35 - batch 180 - loss 0.9937363862991333\n",
            "epoch 35 - batch 190 - loss 0.9291725754737854\n",
            "epoch 35 - batch 200 - loss 0.9158007502555847\n",
            "epoch 35 - batch 210 - loss 0.9756039977073669\n",
            "epoch 35 - batch 220 - loss 0.9456688761711121\n",
            "epoch 35 - batch 230 - loss 0.9700265526771545\n",
            "epoch 35 - batch 240 - loss 0.9330189824104309\n",
            "epoch 35 - batch 250 - loss 0.9985405802726746\n",
            "epoch 35 - batch 260 - loss 0.9290189743041992\n",
            "epoch 35 - batch 270 - loss 0.9717298746109009\n",
            "epoch 35 - batch 280 - loss 0.9497621655464172\n",
            "epoch 35 - batch 290 - loss 0.9000248312950134\n",
            "epoch 35 - batch 300 - loss 0.9710924029350281\n",
            "epoch 35 - batch 310 - loss 0.9359657168388367\n",
            "epoch 35 - batch 320 - loss 0.9710474014282227\n",
            "epoch 35 - batch 330 - loss 0.9247534871101379\n",
            "epoch 35 - batch 340 - loss 0.9434047341346741\n",
            "epoch 35 - batch 350 - loss 0.9309595823287964\n",
            "epoch 35 - batch 360 - loss 0.9886171221733093\n",
            "epoch 35 - batch 370 - loss 0.9627764821052551\n",
            "epoch 35 - batch 380 - loss 0.9803581237792969\n",
            "epoch 35 - batch 390 - loss 0.9097296595573425\n",
            "epoch 35 - batch 400 - loss 0.8772347569465637\n",
            "epoch 35 - batch 410 - loss 0.9600182771682739\n",
            "epoch 35 - batch 420 - loss 0.9601547122001648\n",
            "epoch 35 - batch 430 - loss 0.944033145904541\n",
            "epoch 35 - batch 440 - loss 0.9626538157463074\n",
            "epoch 35 - batch 450 - loss 0.9413551092147827\n",
            "epoch 35 - batch 460 - loss 0.941764771938324\n",
            "epoch 35 - batch 470 - loss 0.9996453523635864\n",
            "epoch 35 - batch 480 - loss 0.9553342461585999\n",
            "epoch 35 - batch 490 - loss 0.9263314008712769\n",
            "epoch 35 - batch 500 - loss 0.8750681281089783\n",
            "epoch 35 - batch 510 - loss 0.9241622686386108\n",
            "epoch 35 - batch 520 - loss 0.9965586066246033\n",
            "epoch 35 - batch 530 - loss 0.9840531349182129\n",
            "epoch 35 - batch 540 - loss 0.915685772895813\n",
            "epoch 35 - batch 550 - loss 0.9028517603874207\n",
            "epoch 35 - batch 560 - loss 0.956985354423523\n",
            "epoch 35 - batch 570 - loss 0.9423443675041199\n",
            "epoch 35 - batch 580 - loss 0.9580503106117249\n",
            "epoch 35 - batch 590 - loss 0.8686453700065613\n",
            "epoch 35 - batch 600 - loss 0.9164656400680542\n",
            "epoch 35 - batch 610 - loss 0.9372144937515259\n",
            "epoch 35 training time: 216.76007294654846 sec\n",
            "evaluation of batch 0 took: 0.15469002723693848\n",
            "evaluation of batch 50 took: 0.16112375259399414\n",
            "evaluation of batch 100 took: 0.15699338912963867\n",
            "evaluation of batch 150 took: 0.16559815406799316\n",
            "evaluation of batch 200 took: 0.15321922302246094\n",
            "evaluation of batch 250 took: 0.15584564208984375\n",
            "evaluation of batch 300 took: 0.16240715980529785\n",
            "evaluation of batch 350 took: 0.15783095359802246\n",
            "evaluation of batch 400 took: 0.15677475929260254\n",
            "evaluation of batch 450 took: 0.15351629257202148\n",
            "evaluation of batch 500 took: 0.15501880645751953\n",
            "evaluation of batch 550 took: 0.15833830833435059\n",
            "evaluation of batch 600 took: 0.15566682815551758\n",
            "epoch 35 evaluation on training data time: 98.01625037193298 sec\n",
            "evaluation of batch 0 took: 0.16367721557617188\n",
            "evaluation of batch 50 took: 0.15062499046325684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 35 evaluation on test data time: 22.47994065284729 sec\n",
            "epoch evaluation:  {'epoch': 35, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.335818>, 'test_rouge_1_p': 0.5496959792439703, 'test_rouge_1_r': 0.5159817442602038, 'test_rouge_1_f1': 0.5236873984178921, 'test_rouge_2_p': 0.28304712865259746, 'test_rouge_2_r': 0.2722778848755411, 'test_rouge_2_f1': 0.27448465240736464, 'test_rouge_3_p': 0.11938222909902592, 'test_rouge_3_r': 0.11468458299512989, 'test_rouge_3_f1': 0.11567159356640389, 'test_rouge_L_p': 0.5475579898152442, 'test_rouge_L_r': 0.5141770060296843, 'test_rouge_L_f1': 0.5217785474665801}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 - batch 10 - loss 0.950867772102356\n",
            "epoch 36 - batch 20 - loss 0.9998145699501038\n",
            "epoch 36 - batch 30 - loss 0.9145002365112305\n",
            "epoch 36 - batch 40 - loss 0.9216561913490295\n",
            "epoch 36 - batch 50 - loss 0.9307565093040466\n",
            "epoch 36 - batch 60 - loss 0.9126750826835632\n",
            "epoch 36 - batch 70 - loss 0.8793719410896301\n",
            "epoch 36 - batch 80 - loss 0.9363570809364319\n",
            "epoch 36 - batch 90 - loss 0.9731222987174988\n",
            "epoch 36 - batch 100 - loss 0.9203985333442688\n",
            "epoch 36 - batch 110 - loss 0.8946952223777771\n",
            "epoch 36 - batch 120 - loss 0.9634936451911926\n",
            "epoch 36 - batch 130 - loss 0.953277051448822\n",
            "epoch 36 - batch 140 - loss 0.8841635584831238\n",
            "epoch 36 - batch 150 - loss 0.9823728799819946\n",
            "epoch 36 - batch 160 - loss 1.029861330986023\n",
            "epoch 36 - batch 170 - loss 0.9340896606445312\n",
            "epoch 36 - batch 180 - loss 0.9781965017318726\n",
            "epoch 36 - batch 190 - loss 0.8928752541542053\n",
            "epoch 36 - batch 200 - loss 0.923815906047821\n",
            "epoch 36 - batch 210 - loss 0.9576113820075989\n",
            "epoch 36 - batch 220 - loss 0.9726937413215637\n",
            "epoch 36 - batch 230 - loss 0.9038088917732239\n",
            "epoch 36 - batch 240 - loss 0.9608245491981506\n",
            "epoch 36 - batch 250 - loss 0.9540799260139465\n",
            "epoch 36 - batch 260 - loss 0.9850196838378906\n",
            "epoch 36 - batch 270 - loss 0.9467436075210571\n",
            "epoch 36 - batch 280 - loss 0.9638530015945435\n",
            "epoch 36 - batch 290 - loss 0.9964154958724976\n",
            "epoch 36 - batch 300 - loss 0.9266687035560608\n",
            "epoch 36 - batch 310 - loss 0.9351476430892944\n",
            "epoch 36 - batch 320 - loss 0.9222623705863953\n",
            "epoch 36 - batch 330 - loss 0.9438174366950989\n",
            "epoch 36 - batch 340 - loss 0.9455847144126892\n",
            "epoch 36 - batch 350 - loss 0.9035003781318665\n",
            "epoch 36 - batch 360 - loss 0.9172868132591248\n",
            "epoch 36 - batch 370 - loss 0.9306861758232117\n",
            "epoch 36 - batch 380 - loss 0.8846249580383301\n",
            "epoch 36 - batch 390 - loss 0.9268249273300171\n",
            "epoch 36 - batch 400 - loss 0.9084081649780273\n",
            "epoch 36 - batch 410 - loss 0.8477944731712341\n",
            "epoch 36 - batch 420 - loss 0.9023303389549255\n",
            "epoch 36 - batch 430 - loss 0.907198429107666\n",
            "epoch 36 - batch 440 - loss 0.9818878173828125\n",
            "epoch 36 - batch 450 - loss 0.8943251371383667\n",
            "epoch 36 - batch 460 - loss 0.9362858533859253\n",
            "epoch 36 - batch 470 - loss 0.8668455481529236\n",
            "epoch 36 - batch 480 - loss 0.9159426093101501\n",
            "epoch 36 - batch 490 - loss 0.9635990858078003\n",
            "epoch 36 - batch 500 - loss 0.9804754257202148\n",
            "epoch 36 - batch 510 - loss 0.9919779896736145\n",
            "epoch 36 - batch 520 - loss 1.0083332061767578\n",
            "epoch 36 - batch 530 - loss 0.891906201839447\n",
            "epoch 36 - batch 540 - loss 0.8917484283447266\n",
            "epoch 36 - batch 550 - loss 0.8974795937538147\n",
            "epoch 36 - batch 560 - loss 0.9441689252853394\n",
            "epoch 36 - batch 570 - loss 0.9401648640632629\n",
            "epoch 36 - batch 580 - loss 1.0102477073669434\n",
            "epoch 36 - batch 590 - loss 0.9734885096549988\n",
            "epoch 36 - batch 600 - loss 0.8963015675544739\n",
            "epoch 36 - batch 610 - loss 0.905247688293457\n",
            "epoch 36 training time: 217.48106575012207 sec\n",
            "evaluation of batch 0 took: 0.15497946739196777\n",
            "evaluation of batch 50 took: 0.16068816184997559\n",
            "evaluation of batch 100 took: 0.15421628952026367\n",
            "evaluation of batch 150 took: 0.1606295108795166\n",
            "evaluation of batch 200 took: 0.16189146041870117\n",
            "evaluation of batch 250 took: 0.1549971103668213\n",
            "evaluation of batch 300 took: 0.15341424942016602\n",
            "evaluation of batch 350 took: 0.16115713119506836\n",
            "evaluation of batch 400 took: 0.1565101146697998\n",
            "evaluation of batch 450 took: 0.1567225456237793\n",
            "evaluation of batch 500 took: 0.15304112434387207\n",
            "evaluation of batch 550 took: 0.1539781093597412\n",
            "evaluation of batch 600 took: 0.16189813613891602\n",
            "epoch 36 evaluation on training data time: 98.31910753250122 sec\n",
            "evaluation of batch 0 took: 0.16378188133239746\n",
            "evaluation of batch 50 took: 0.15990304946899414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 evaluation on test data time: 22.62277388572693 sec\n",
            "epoch evaluation:  {'epoch': 36, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3376857>, 'test_rouge_1_p': 0.5513737401920995, 'test_rouge_1_r': 0.5148333925189391, 'test_rouge_1_f1': 0.5237219441194175, 'test_rouge_2_p': 0.2823136498917748, 'test_rouge_2_r': 0.27057418492965357, 'test_rouge_2_f1': 0.27321888476895584, 'test_rouge_3_p': 0.11895440171807363, 'test_rouge_3_r': 0.11378284801136362, 'test_rouge_3_f1': 0.11494123513515259, 'test_rouge_L_p': 0.5492049802875695, 'test_rouge_L_r': 0.5130345728490258, 'test_rouge_L_f1': 0.5218083722706541}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 saved checkpoint: models/checkpoints/baseline/ckpt-13\n",
            "epoch 37 - batch 10 - loss 0.9278155565261841\n",
            "epoch 37 - batch 20 - loss 0.9262251853942871\n",
            "epoch 37 - batch 30 - loss 0.9137248992919922\n",
            "epoch 37 - batch 40 - loss 0.9481645226478577\n",
            "epoch 37 - batch 50 - loss 0.9028584361076355\n",
            "epoch 37 - batch 60 - loss 0.9311215281486511\n",
            "epoch 37 - batch 70 - loss 0.935195803642273\n",
            "epoch 37 - batch 80 - loss 0.9219282269477844\n",
            "epoch 37 - batch 90 - loss 0.9724935293197632\n",
            "epoch 37 - batch 100 - loss 0.9096473455429077\n",
            "epoch 37 - batch 110 - loss 0.956237256526947\n",
            "epoch 37 - batch 120 - loss 0.9570625424385071\n",
            "epoch 37 - batch 130 - loss 0.9311131834983826\n",
            "epoch 37 - batch 140 - loss 0.9691645503044128\n",
            "epoch 37 - batch 150 - loss 0.9999211430549622\n",
            "epoch 37 - batch 160 - loss 0.9544965028762817\n",
            "epoch 37 - batch 170 - loss 0.9462887048721313\n",
            "epoch 37 - batch 180 - loss 0.8809475302696228\n",
            "epoch 37 - batch 190 - loss 0.9065262675285339\n",
            "epoch 37 - batch 200 - loss 0.925287127494812\n",
            "epoch 37 - batch 210 - loss 0.9549552798271179\n",
            "epoch 37 - batch 220 - loss 0.9255574345588684\n",
            "epoch 37 - batch 230 - loss 0.9565399885177612\n",
            "epoch 37 - batch 240 - loss 0.925879955291748\n",
            "epoch 37 - batch 250 - loss 0.9585710167884827\n",
            "epoch 37 - batch 260 - loss 0.9170106053352356\n",
            "epoch 37 - batch 270 - loss 0.9475476145744324\n",
            "epoch 37 - batch 280 - loss 0.9189834594726562\n",
            "epoch 37 - batch 290 - loss 0.9234521985054016\n",
            "epoch 37 - batch 300 - loss 0.9386335611343384\n",
            "epoch 37 - batch 310 - loss 0.9182590842247009\n",
            "epoch 37 - batch 320 - loss 0.896145224571228\n",
            "epoch 37 - batch 330 - loss 0.9281378984451294\n",
            "epoch 37 - batch 340 - loss 0.9945104718208313\n",
            "epoch 37 - batch 350 - loss 0.9567016959190369\n",
            "epoch 37 - batch 360 - loss 0.8532263040542603\n",
            "epoch 37 - batch 370 - loss 0.9399605989456177\n",
            "epoch 37 - batch 380 - loss 0.8638226389884949\n",
            "epoch 37 - batch 390 - loss 0.954310953617096\n",
            "epoch 37 - batch 400 - loss 0.9291888475418091\n",
            "epoch 37 - batch 410 - loss 0.9510097503662109\n",
            "epoch 37 - batch 420 - loss 0.9190272092819214\n",
            "epoch 37 - batch 430 - loss 0.8329476714134216\n",
            "epoch 37 - batch 440 - loss 0.9690341949462891\n",
            "epoch 37 - batch 450 - loss 0.9385671019554138\n",
            "epoch 37 - batch 460 - loss 0.8866769671440125\n",
            "epoch 37 - batch 470 - loss 0.9092996716499329\n",
            "epoch 37 - batch 480 - loss 0.9269863367080688\n",
            "epoch 37 - batch 490 - loss 0.875388503074646\n",
            "epoch 37 - batch 500 - loss 0.9981592297554016\n",
            "epoch 37 - batch 510 - loss 0.9544184803962708\n",
            "epoch 37 - batch 520 - loss 0.9338164329528809\n",
            "epoch 37 - batch 530 - loss 0.9147013425827026\n",
            "epoch 37 - batch 540 - loss 0.9557002186775208\n",
            "epoch 37 - batch 550 - loss 0.9034758806228638\n",
            "epoch 37 - batch 560 - loss 0.9222249984741211\n",
            "epoch 37 - batch 570 - loss 0.918278694152832\n",
            "epoch 37 - batch 580 - loss 0.9140154123306274\n",
            "epoch 37 - batch 590 - loss 0.9401578903198242\n",
            "epoch 37 - batch 600 - loss 0.8717846274375916\n",
            "epoch 37 - batch 610 - loss 0.9522475004196167\n",
            "epoch 37 training time: 216.98472356796265 sec\n",
            "evaluation of batch 0 took: 0.15801405906677246\n",
            "evaluation of batch 50 took: 0.1747281551361084\n",
            "evaluation of batch 100 took: 0.15543055534362793\n",
            "evaluation of batch 150 took: 0.15573430061340332\n",
            "evaluation of batch 200 took: 0.155242919921875\n",
            "evaluation of batch 250 took: 0.15561652183532715\n",
            "evaluation of batch 300 took: 0.15719366073608398\n",
            "evaluation of batch 350 took: 0.15780162811279297\n",
            "evaluation of batch 400 took: 0.16045427322387695\n",
            "evaluation of batch 450 took: 0.15552806854248047\n",
            "evaluation of batch 500 took: 0.16043853759765625\n",
            "evaluation of batch 550 took: 0.16432785987854004\n",
            "evaluation of batch 600 took: 0.16607403755187988\n",
            "epoch 37 evaluation on training data time: 98.14700722694397 sec\n",
            "evaluation of batch 0 took: 0.15996432304382324\n",
            "evaluation of batch 50 took: 0.1546177864074707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 37 evaluation on test data time: 22.528284788131714 sec\n",
            "epoch evaluation:  {'epoch': 37, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3410964>, 'test_rouge_1_p': 0.5497859051290972, 'test_rouge_1_r': 0.5156550759508346, 'test_rouge_1_f1': 0.5234199573142854, 'test_rouge_2_p': 0.2825711072781385, 'test_rouge_2_r': 0.27163783482142856, 'test_rouge_2_f1': 0.27391306842339197, 'test_rouge_3_p': 0.11962489008387447, 'test_rouge_3_r': 0.1147767434388528, 'test_rouge_3_f1': 0.11579465533846113, 'test_rouge_L_p': 0.5476759986665122, 'test_rouge_L_r': 0.5138740723562152, 'test_rouge_L_f1': 0.5215404881407822}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 38 - batch 10 - loss 0.9463557004928589\n",
            "epoch 38 - batch 20 - loss 0.9224153757095337\n",
            "epoch 38 - batch 30 - loss 0.8920778632164001\n",
            "epoch 38 - batch 40 - loss 0.9045095443725586\n",
            "epoch 38 - batch 50 - loss 0.9776818156242371\n",
            "epoch 38 - batch 60 - loss 0.9520973563194275\n",
            "epoch 38 - batch 70 - loss 0.8996483683586121\n",
            "epoch 38 - batch 80 - loss 0.9788719415664673\n",
            "epoch 38 - batch 90 - loss 0.939147412776947\n",
            "epoch 38 - batch 100 - loss 0.9364731907844543\n",
            "epoch 38 - batch 110 - loss 0.8922691345214844\n",
            "epoch 38 - batch 120 - loss 0.9744755625724792\n",
            "epoch 38 - batch 130 - loss 0.8871201276779175\n",
            "epoch 38 - batch 140 - loss 0.9331889748573303\n",
            "epoch 38 - batch 150 - loss 0.9481272101402283\n",
            "epoch 38 - batch 160 - loss 0.9400578737258911\n",
            "epoch 38 - batch 170 - loss 0.9359325170516968\n",
            "epoch 38 - batch 180 - loss 0.9554747343063354\n",
            "epoch 38 - batch 190 - loss 0.9452092051506042\n",
            "epoch 38 - batch 200 - loss 0.8747585415840149\n",
            "epoch 38 - batch 210 - loss 0.923463761806488\n",
            "epoch 38 - batch 220 - loss 0.9719746708869934\n",
            "epoch 38 - batch 230 - loss 0.8858820796012878\n",
            "epoch 38 - batch 240 - loss 0.9705218076705933\n",
            "epoch 38 - batch 250 - loss 0.9271199107170105\n",
            "epoch 38 - batch 260 - loss 0.9306418299674988\n",
            "epoch 38 - batch 270 - loss 0.9055718779563904\n",
            "epoch 38 - batch 280 - loss 0.9338448643684387\n",
            "epoch 38 - batch 290 - loss 0.961961567401886\n",
            "epoch 38 - batch 300 - loss 0.9107354283332825\n",
            "epoch 38 - batch 310 - loss 0.9050546288490295\n",
            "epoch 38 - batch 320 - loss 0.890309751033783\n",
            "epoch 38 - batch 330 - loss 0.8962894082069397\n",
            "epoch 38 - batch 340 - loss 0.9170299172401428\n",
            "epoch 38 - batch 350 - loss 0.9588972926139832\n",
            "epoch 38 - batch 360 - loss 0.9457491636276245\n",
            "epoch 38 - batch 370 - loss 0.9149872660636902\n",
            "epoch 38 - batch 380 - loss 0.8927194476127625\n",
            "epoch 38 - batch 390 - loss 0.9085472226142883\n",
            "epoch 38 - batch 400 - loss 0.9572613835334778\n",
            "epoch 38 - batch 410 - loss 0.9586783647537231\n",
            "epoch 38 - batch 420 - loss 0.9314932823181152\n",
            "epoch 38 - batch 430 - loss 0.9283276200294495\n",
            "epoch 38 - batch 440 - loss 0.9427060484886169\n",
            "epoch 38 - batch 450 - loss 0.9214312434196472\n",
            "epoch 38 - batch 460 - loss 0.951370894908905\n",
            "epoch 38 - batch 470 - loss 0.9197635054588318\n",
            "epoch 38 - batch 480 - loss 0.9146838784217834\n",
            "epoch 38 - batch 490 - loss 0.8941181302070618\n",
            "epoch 38 - batch 500 - loss 0.9211826920509338\n",
            "epoch 38 - batch 510 - loss 0.9554829597473145\n",
            "epoch 38 - batch 520 - loss 0.963388979434967\n",
            "epoch 38 - batch 530 - loss 0.9015201926231384\n",
            "epoch 38 - batch 540 - loss 0.9277377724647522\n",
            "epoch 38 - batch 550 - loss 0.8690237402915955\n",
            "epoch 38 - batch 560 - loss 1.0013879537582397\n",
            "epoch 38 - batch 570 - loss 0.8802741169929504\n",
            "epoch 38 - batch 580 - loss 0.9335459470748901\n",
            "epoch 38 - batch 590 - loss 0.8782767057418823\n",
            "epoch 38 - batch 600 - loss 0.8851740956306458\n",
            "epoch 38 - batch 610 - loss 0.9328327178955078\n",
            "epoch 38 training time: 216.36834335327148 sec\n",
            "evaluation of batch 0 took: 0.15619564056396484\n",
            "evaluation of batch 50 took: 0.15354037284851074\n",
            "evaluation of batch 100 took: 0.1623249053955078\n",
            "evaluation of batch 150 took: 0.15500473976135254\n",
            "evaluation of batch 200 took: 0.15736627578735352\n",
            "evaluation of batch 250 took: 0.15935826301574707\n",
            "evaluation of batch 300 took: 0.15637803077697754\n",
            "evaluation of batch 350 took: 0.15835905075073242\n",
            "evaluation of batch 400 took: 0.1585063934326172\n",
            "evaluation of batch 450 took: 0.1583089828491211\n",
            "evaluation of batch 500 took: 0.16061639785766602\n",
            "evaluation of batch 550 took: 0.16007590293884277\n",
            "evaluation of batch 600 took: 0.15509724617004395\n",
            "epoch 38 evaluation on training data time: 98.1454107761383 sec\n",
            "evaluation of batch 0 took: 0.16284751892089844\n",
            "evaluation of batch 50 took: 0.1507408618927002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 38 evaluation on test data time: 22.383339405059814 sec\n",
            "epoch evaluation:  {'epoch': 38, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3420229>, 'test_rouge_1_p': 0.5517229956613328, 'test_rouge_1_r': 0.5145018323380488, 'test_rouge_1_f1': 0.523501695730541, 'test_rouge_2_p': 0.28259309050324666, 'test_rouge_2_r': 0.2708436908143939, 'test_rouge_2_f1': 0.27343404293105, 'test_rouge_3_p': 0.11988530675054115, 'test_rouge_3_r': 0.1145118878517316, 'test_rouge_3_f1': 0.11574299878568854, 'test_rouge_L_p': 0.5496257114351423, 'test_rouge_L_r': 0.5127439292478354, 'test_rouge_L_f1': 0.5216348099388886}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 - batch 10 - loss 0.9153793454170227\n",
            "epoch 39 - batch 20 - loss 0.9000392556190491\n",
            "epoch 39 - batch 30 - loss 0.8859308958053589\n",
            "epoch 39 - batch 40 - loss 0.8862319588661194\n",
            "epoch 39 - batch 50 - loss 0.958983838558197\n",
            "epoch 39 - batch 60 - loss 0.931208074092865\n",
            "epoch 39 - batch 70 - loss 0.8940388560295105\n",
            "epoch 39 - batch 80 - loss 0.922619640827179\n",
            "epoch 39 - batch 90 - loss 0.9159820675849915\n",
            "epoch 39 - batch 100 - loss 0.8996241688728333\n",
            "epoch 39 - batch 110 - loss 0.97618168592453\n",
            "epoch 39 - batch 120 - loss 0.950149655342102\n",
            "epoch 39 - batch 130 - loss 0.9253126382827759\n",
            "epoch 39 - batch 140 - loss 0.950134813785553\n",
            "epoch 39 - batch 150 - loss 0.9884868264198303\n",
            "epoch 39 - batch 160 - loss 0.9188352227210999\n",
            "epoch 39 - batch 170 - loss 0.9432265162467957\n",
            "epoch 39 - batch 180 - loss 0.919435441493988\n",
            "epoch 39 - batch 190 - loss 0.9262880682945251\n",
            "epoch 39 - batch 200 - loss 0.9817761182785034\n",
            "epoch 39 - batch 210 - loss 0.8985373377799988\n",
            "epoch 39 - batch 220 - loss 0.9140172600746155\n",
            "epoch 39 - batch 230 - loss 0.9736253023147583\n",
            "epoch 39 - batch 240 - loss 0.8778576850891113\n",
            "epoch 39 - batch 250 - loss 0.9330801367759705\n",
            "epoch 39 - batch 260 - loss 0.922816276550293\n",
            "epoch 39 - batch 270 - loss 0.9321039319038391\n",
            "epoch 39 - batch 280 - loss 0.9017791748046875\n",
            "epoch 39 - batch 290 - loss 0.8970469236373901\n",
            "epoch 39 - batch 300 - loss 0.8700998425483704\n",
            "epoch 39 - batch 310 - loss 0.9218319058418274\n",
            "epoch 39 - batch 320 - loss 0.8890740275382996\n",
            "epoch 39 - batch 330 - loss 0.8605543971061707\n",
            "epoch 39 - batch 340 - loss 0.9361473917961121\n",
            "epoch 39 - batch 350 - loss 0.9108138680458069\n",
            "epoch 39 - batch 360 - loss 0.9301726222038269\n",
            "epoch 39 - batch 370 - loss 0.9361084699630737\n",
            "epoch 39 - batch 380 - loss 0.8656768798828125\n",
            "epoch 39 - batch 390 - loss 0.9072070717811584\n",
            "epoch 39 - batch 400 - loss 0.9206517934799194\n",
            "epoch 39 - batch 410 - loss 0.8744719624519348\n",
            "epoch 39 - batch 420 - loss 0.864355742931366\n",
            "epoch 39 - batch 430 - loss 0.8439426422119141\n",
            "epoch 39 - batch 440 - loss 0.9661674499511719\n",
            "epoch 39 - batch 450 - loss 0.9569397568702698\n",
            "epoch 39 - batch 460 - loss 0.8978375196456909\n",
            "epoch 39 - batch 470 - loss 0.9530941247940063\n",
            "epoch 39 - batch 480 - loss 0.939298152923584\n",
            "epoch 39 - batch 490 - loss 0.884020209312439\n",
            "epoch 39 - batch 500 - loss 0.9127343893051147\n",
            "epoch 39 - batch 510 - loss 0.9477066397666931\n",
            "epoch 39 - batch 520 - loss 0.8886112570762634\n",
            "epoch 39 - batch 530 - loss 0.9321303963661194\n",
            "epoch 39 - batch 540 - loss 0.8726417422294617\n",
            "epoch 39 - batch 550 - loss 0.8695208430290222\n",
            "epoch 39 - batch 560 - loss 0.9268297553062439\n",
            "epoch 39 - batch 570 - loss 0.9110633134841919\n",
            "epoch 39 - batch 580 - loss 0.9017078280448914\n",
            "epoch 39 - batch 590 - loss 0.8647124171257019\n",
            "epoch 39 - batch 600 - loss 0.8768167495727539\n",
            "epoch 39 - batch 610 - loss 0.9466111063957214\n",
            "epoch 39 training time: 215.55518078804016 sec\n",
            "evaluation of batch 0 took: 0.15396380424499512\n",
            "evaluation of batch 50 took: 0.1550428867340088\n",
            "evaluation of batch 100 took: 0.15623974800109863\n",
            "evaluation of batch 150 took: 0.1572246551513672\n",
            "evaluation of batch 200 took: 0.15734291076660156\n",
            "evaluation of batch 250 took: 0.1587228775024414\n",
            "evaluation of batch 300 took: 0.16200637817382812\n",
            "evaluation of batch 350 took: 0.15350556373596191\n",
            "evaluation of batch 400 took: 0.15698003768920898\n",
            "evaluation of batch 450 took: 0.1576528549194336\n",
            "evaluation of batch 500 took: 0.1647505760192871\n",
            "evaluation of batch 550 took: 0.15807175636291504\n",
            "evaluation of batch 600 took: 0.155242919921875\n",
            "epoch 39 evaluation on training data time: 98.20552134513855 sec\n",
            "evaluation of batch 0 took: 0.16230177879333496\n",
            "evaluation of batch 50 took: 0.15505313873291016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 evaluation on test data time: 22.51490545272827 sec\n",
            "epoch evaluation:  {'epoch': 39, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3429925>, 'test_rouge_1_p': 0.549894885155767, 'test_rouge_1_r': 0.5179031928822664, 'test_rouge_1_f1': 0.5245987908674344, 'test_rouge_2_p': 0.2828000287472945, 'test_rouge_2_r': 0.27313332826028125, 'test_rouge_2_f1': 0.27485423028112754, 'test_rouge_3_p': 0.1194236590232684, 'test_rouge_3_r': 0.11525318756764072, 'test_rouge_3_f1': 0.11600576073232327, 'test_rouge_L_p': 0.5477837104301947, 'test_rouge_L_r': 0.5161015649157387, 'test_rouge_L_f1': 0.5227027800054385}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 saved checkpoint: models/checkpoints/baseline/ckpt-14\n",
            "epoch 40 - batch 10 - loss 0.860752522945404\n",
            "epoch 40 - batch 20 - loss 0.9028181433677673\n",
            "epoch 40 - batch 30 - loss 0.9008086323738098\n",
            "epoch 40 - batch 40 - loss 0.8818139433860779\n",
            "epoch 40 - batch 50 - loss 0.8705617189407349\n",
            "epoch 40 - batch 60 - loss 0.911698043346405\n",
            "epoch 40 - batch 70 - loss 0.8524380922317505\n",
            "epoch 40 - batch 80 - loss 0.9208451509475708\n",
            "epoch 40 - batch 90 - loss 0.9161695837974548\n",
            "epoch 40 - batch 100 - loss 0.9089127779006958\n",
            "epoch 40 - batch 110 - loss 0.9136254191398621\n",
            "epoch 40 - batch 120 - loss 0.908955991268158\n",
            "epoch 40 - batch 130 - loss 0.9180325269699097\n",
            "epoch 40 - batch 140 - loss 0.8955654501914978\n",
            "epoch 40 - batch 150 - loss 0.8984349370002747\n",
            "epoch 40 - batch 160 - loss 0.870549738407135\n",
            "epoch 40 - batch 170 - loss 0.9335442781448364\n",
            "epoch 40 - batch 180 - loss 0.9103235602378845\n",
            "epoch 40 - batch 190 - loss 0.8950767517089844\n",
            "epoch 40 - batch 200 - loss 0.945606529712677\n",
            "epoch 40 - batch 210 - loss 0.8802903294563293\n",
            "epoch 40 - batch 220 - loss 0.9277275204658508\n",
            "epoch 40 - batch 230 - loss 0.8673741221427917\n",
            "epoch 40 - batch 240 - loss 0.9008258581161499\n",
            "epoch 40 - batch 250 - loss 0.9387860894203186\n",
            "epoch 40 - batch 260 - loss 0.8949168920516968\n",
            "epoch 40 - batch 270 - loss 0.9128259420394897\n",
            "epoch 40 - batch 280 - loss 0.8713301420211792\n",
            "epoch 40 - batch 290 - loss 0.9613103270530701\n",
            "epoch 40 - batch 300 - loss 0.9267686009407043\n",
            "epoch 40 - batch 310 - loss 0.9304967522621155\n",
            "epoch 40 - batch 320 - loss 0.9416531324386597\n",
            "epoch 40 - batch 330 - loss 0.9696833491325378\n",
            "epoch 40 - batch 340 - loss 0.8935251235961914\n",
            "epoch 40 - batch 350 - loss 0.9488562941551208\n",
            "epoch 40 - batch 360 - loss 0.8926762342453003\n",
            "epoch 40 - batch 370 - loss 0.9438062906265259\n",
            "epoch 40 - batch 380 - loss 0.8876041769981384\n",
            "epoch 40 - batch 390 - loss 0.8673467636108398\n",
            "epoch 40 - batch 400 - loss 0.8460286855697632\n",
            "epoch 40 - batch 410 - loss 0.9309381246566772\n",
            "epoch 40 - batch 420 - loss 0.960469663143158\n",
            "epoch 40 - batch 430 - loss 0.8848825097084045\n",
            "epoch 40 - batch 440 - loss 0.893595814704895\n",
            "epoch 40 - batch 450 - loss 0.9009391069412231\n",
            "epoch 40 - batch 460 - loss 0.8731566667556763\n",
            "epoch 40 - batch 470 - loss 0.9210711717605591\n",
            "epoch 40 - batch 480 - loss 0.8941518068313599\n",
            "epoch 40 - batch 490 - loss 0.8906053304672241\n",
            "epoch 40 - batch 500 - loss 0.9016684293746948\n",
            "epoch 40 - batch 510 - loss 0.8835605978965759\n",
            "epoch 40 - batch 520 - loss 0.8719247579574585\n",
            "epoch 40 - batch 530 - loss 0.9220033288002014\n",
            "epoch 40 - batch 540 - loss 0.8674626350402832\n",
            "epoch 40 - batch 550 - loss 0.8643397688865662\n",
            "epoch 40 - batch 560 - loss 0.9042068123817444\n",
            "epoch 40 - batch 570 - loss 0.9314862489700317\n",
            "epoch 40 - batch 580 - loss 0.9371646046638489\n",
            "epoch 40 - batch 590 - loss 0.9454995393753052\n",
            "epoch 40 - batch 600 - loss 0.8656331896781921\n",
            "epoch 40 - batch 610 - loss 0.9026152491569519\n",
            "epoch 40 training time: 215.44068455696106 sec\n",
            "evaluation of batch 0 took: 0.1527702808380127\n",
            "evaluation of batch 50 took: 0.15378236770629883\n",
            "evaluation of batch 100 took: 0.1579267978668213\n",
            "evaluation of batch 150 took: 0.1639246940612793\n",
            "evaluation of batch 200 took: 0.1655139923095703\n",
            "evaluation of batch 250 took: 0.15954208374023438\n",
            "evaluation of batch 300 took: 0.15575122833251953\n",
            "evaluation of batch 350 took: 0.16148924827575684\n",
            "evaluation of batch 400 took: 0.15835881233215332\n",
            "evaluation of batch 450 took: 0.15883445739746094\n",
            "evaluation of batch 500 took: 0.1570591926574707\n",
            "evaluation of batch 550 took: 0.15698599815368652\n",
            "evaluation of batch 600 took: 0.15102386474609375\n",
            "epoch 40 evaluation on training data time: 98.1889317035675 sec\n",
            "evaluation of batch 0 took: 0.1620924472808838\n",
            "evaluation of batch 50 took: 0.15198349952697754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 40 evaluation on test data time: 22.54095482826233 sec\n",
            "epoch evaluation:  {'epoch': 40, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3463268>, 'test_rouge_1_p': 0.5519820232780612, 'test_rouge_1_r': 0.5168748429769636, 'test_rouge_1_f1': 0.5249626975263643, 'test_rouge_2_p': 0.28330141538149345, 'test_rouge_2_r': 0.2721115310470779, 'test_rouge_2_f1': 0.27447375321512907, 'test_rouge_3_p': 0.1193336123511905, 'test_rouge_3_r': 0.1145875608766234, 'test_rouge_3_f1': 0.11556634786964032, 'test_rouge_L_p': 0.5499174723156308, 'test_rouge_L_r': 0.5151222243158626, 'test_rouge_L_f1': 0.523115882358983}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 41 - batch 10 - loss 0.8793960213661194\n",
            "epoch 41 - batch 20 - loss 0.879271924495697\n",
            "epoch 41 - batch 30 - loss 0.8899837136268616\n",
            "epoch 41 - batch 40 - loss 0.9078505635261536\n",
            "epoch 41 - batch 50 - loss 0.9543460011482239\n",
            "epoch 41 - batch 60 - loss 0.8218417167663574\n",
            "epoch 41 - batch 70 - loss 0.8888053894042969\n",
            "epoch 41 - batch 80 - loss 0.9634484648704529\n",
            "epoch 41 - batch 90 - loss 0.902478039264679\n",
            "epoch 41 - batch 100 - loss 0.9109824895858765\n",
            "epoch 41 - batch 110 - loss 0.9351622462272644\n",
            "epoch 41 - batch 120 - loss 0.9359731674194336\n",
            "epoch 41 - batch 130 - loss 0.8812279105186462\n",
            "epoch 41 - batch 140 - loss 0.8383966088294983\n",
            "epoch 41 - batch 150 - loss 0.9504681825637817\n",
            "epoch 41 - batch 160 - loss 0.9220653176307678\n",
            "epoch 41 - batch 170 - loss 0.9077342748641968\n",
            "epoch 41 - batch 180 - loss 0.8990212678909302\n",
            "epoch 41 - batch 190 - loss 0.9206423163414001\n",
            "epoch 41 - batch 200 - loss 0.8975414633750916\n",
            "epoch 41 - batch 210 - loss 0.9090871810913086\n",
            "epoch 41 - batch 220 - loss 0.9076582789421082\n",
            "epoch 41 - batch 230 - loss 0.9418271780014038\n",
            "epoch 41 - batch 240 - loss 0.8869672417640686\n",
            "epoch 41 - batch 250 - loss 0.9394705891609192\n",
            "epoch 41 - batch 260 - loss 0.908209502696991\n",
            "epoch 41 - batch 270 - loss 0.8916158080101013\n",
            "epoch 41 - batch 280 - loss 0.8874699473381042\n",
            "epoch 41 - batch 290 - loss 0.8614932298660278\n",
            "epoch 41 - batch 300 - loss 0.8726487755775452\n",
            "epoch 41 - batch 310 - loss 0.9087041616439819\n",
            "epoch 41 - batch 320 - loss 0.9440196752548218\n",
            "epoch 41 - batch 330 - loss 0.9276825189590454\n",
            "epoch 41 - batch 340 - loss 0.9128908514976501\n",
            "epoch 41 - batch 350 - loss 0.8540571928024292\n",
            "epoch 41 - batch 360 - loss 0.9458084106445312\n",
            "epoch 41 - batch 370 - loss 0.9489926695823669\n",
            "epoch 41 - batch 380 - loss 0.9060614705085754\n",
            "epoch 41 - batch 390 - loss 0.8830075860023499\n",
            "epoch 41 - batch 400 - loss 0.8866105675697327\n",
            "epoch 41 - batch 410 - loss 0.9069347977638245\n",
            "epoch 41 - batch 420 - loss 0.9096381068229675\n",
            "epoch 41 - batch 430 - loss 0.9134759902954102\n",
            "epoch 41 - batch 440 - loss 0.9287852048873901\n",
            "epoch 41 - batch 450 - loss 0.9084941148757935\n",
            "epoch 41 - batch 460 - loss 0.924123227596283\n",
            "epoch 41 - batch 470 - loss 0.8464532494544983\n",
            "epoch 41 - batch 480 - loss 0.8763661980628967\n",
            "epoch 41 - batch 490 - loss 0.8459383249282837\n",
            "epoch 41 - batch 500 - loss 0.913756251335144\n",
            "epoch 41 - batch 510 - loss 0.9077164530754089\n",
            "epoch 41 - batch 520 - loss 0.8950398564338684\n",
            "epoch 41 - batch 530 - loss 0.84308922290802\n",
            "epoch 41 - batch 540 - loss 0.8695061802864075\n",
            "epoch 41 - batch 550 - loss 0.8959512710571289\n",
            "epoch 41 - batch 560 - loss 0.9075638651847839\n",
            "epoch 41 - batch 570 - loss 0.921410858631134\n",
            "epoch 41 - batch 580 - loss 0.9304783940315247\n",
            "epoch 41 - batch 590 - loss 0.8689127564430237\n",
            "epoch 41 - batch 600 - loss 0.8267563581466675\n",
            "epoch 41 - batch 610 - loss 0.9744814038276672\n",
            "epoch 41 training time: 218.38858914375305 sec\n",
            "evaluation of batch 0 took: 0.15967845916748047\n",
            "evaluation of batch 50 took: 0.16126179695129395\n",
            "evaluation of batch 100 took: 0.1589665412902832\n",
            "evaluation of batch 150 took: 0.15798616409301758\n",
            "evaluation of batch 200 took: 0.15768909454345703\n",
            "evaluation of batch 250 took: 0.1538536548614502\n",
            "evaluation of batch 300 took: 0.16221022605895996\n",
            "evaluation of batch 350 took: 0.1553044319152832\n",
            "evaluation of batch 400 took: 0.15897226333618164\n",
            "evaluation of batch 450 took: 0.15187358856201172\n",
            "evaluation of batch 500 took: 0.15514779090881348\n",
            "evaluation of batch 550 took: 0.15961146354675293\n",
            "evaluation of batch 600 took: 0.15869355201721191\n",
            "epoch 41 evaluation on training data time: 98.45006966590881 sec\n",
            "evaluation of batch 0 took: 0.1624467372894287\n",
            "evaluation of batch 50 took: 0.1557457447052002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 41 evaluation on test data time: 22.51959228515625 sec\n",
            "epoch evaluation:  {'epoch': 41, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3543483>, 'test_rouge_1_p': 0.5480075286506647, 'test_rouge_1_r': 0.5179664852446662, 'test_rouge_1_f1': 0.5239020830050405, 'test_rouge_2_p': 0.2813567454680736, 'test_rouge_2_r': 0.2724734087527057, 'test_rouge_2_f1': 0.27390495190629627, 'test_rouge_3_p': 0.11877599939123379, 'test_rouge_3_r': 0.11515257203733763, 'test_rouge_3_f1': 0.11567278130475676, 'test_rouge_L_p': 0.5458083002377087, 'test_rouge_L_r': 0.5160721834898732, 'test_rouge_L_f1': 0.5219218188101354}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 - batch 10 - loss 0.8953719139099121\n",
            "epoch 42 - batch 20 - loss 0.9744122624397278\n",
            "epoch 42 - batch 30 - loss 0.943173348903656\n",
            "epoch 42 - batch 40 - loss 0.9815115332603455\n",
            "epoch 42 - batch 50 - loss 0.8630452752113342\n",
            "epoch 42 - batch 60 - loss 0.9167845845222473\n",
            "epoch 42 - batch 70 - loss 0.916079044342041\n",
            "epoch 42 - batch 80 - loss 0.8956264853477478\n",
            "epoch 42 - batch 90 - loss 0.9469294548034668\n",
            "epoch 42 - batch 100 - loss 0.9020681381225586\n",
            "epoch 42 - batch 110 - loss 0.9100311994552612\n",
            "epoch 42 - batch 120 - loss 0.9648359417915344\n",
            "epoch 42 - batch 130 - loss 0.8870059251785278\n",
            "epoch 42 - batch 140 - loss 0.8630529046058655\n",
            "epoch 42 - batch 150 - loss 0.922739565372467\n",
            "epoch 42 - batch 160 - loss 0.9106372594833374\n",
            "epoch 42 - batch 170 - loss 0.87468022108078\n",
            "epoch 42 - batch 180 - loss 0.9314773678779602\n",
            "epoch 42 - batch 190 - loss 0.8993796110153198\n",
            "epoch 42 - batch 200 - loss 0.8446900248527527\n",
            "epoch 42 - batch 210 - loss 0.9384471774101257\n",
            "epoch 42 - batch 220 - loss 0.9428988099098206\n",
            "epoch 42 - batch 230 - loss 0.8744370341300964\n",
            "epoch 42 - batch 240 - loss 0.897662341594696\n",
            "epoch 42 - batch 250 - loss 0.9173393845558167\n",
            "epoch 42 - batch 260 - loss 0.9014238715171814\n",
            "epoch 42 - batch 270 - loss 0.8802958130836487\n",
            "epoch 42 - batch 280 - loss 0.9263243079185486\n",
            "epoch 42 - batch 290 - loss 0.8898587822914124\n",
            "epoch 42 - batch 300 - loss 0.8830093741416931\n",
            "epoch 42 - batch 310 - loss 0.8505356907844543\n",
            "epoch 42 - batch 320 - loss 0.8688865303993225\n",
            "epoch 42 - batch 330 - loss 0.9036006331443787\n",
            "epoch 42 - batch 340 - loss 0.8826002478599548\n",
            "epoch 42 - batch 350 - loss 0.900044858455658\n",
            "epoch 42 - batch 360 - loss 0.8369277715682983\n",
            "epoch 42 - batch 370 - loss 0.8928377032279968\n",
            "epoch 42 - batch 380 - loss 0.9115799069404602\n",
            "epoch 42 - batch 390 - loss 0.87553471326828\n",
            "epoch 42 - batch 400 - loss 0.8878048062324524\n",
            "epoch 42 - batch 410 - loss 0.8587110638618469\n",
            "epoch 42 - batch 420 - loss 0.857789933681488\n",
            "epoch 42 - batch 430 - loss 0.8952420353889465\n",
            "epoch 42 - batch 440 - loss 0.9398884177207947\n",
            "epoch 42 - batch 450 - loss 0.9125374555587769\n",
            "epoch 42 - batch 460 - loss 0.9123794436454773\n",
            "epoch 42 - batch 470 - loss 0.8816620707511902\n",
            "epoch 42 - batch 480 - loss 0.8931118845939636\n",
            "epoch 42 - batch 490 - loss 0.848673403263092\n",
            "epoch 42 - batch 500 - loss 0.8607093691825867\n",
            "epoch 42 - batch 510 - loss 0.8519709706306458\n",
            "epoch 42 - batch 520 - loss 0.8296893239021301\n",
            "epoch 42 - batch 530 - loss 0.9303154349327087\n",
            "epoch 42 - batch 540 - loss 0.9009732007980347\n",
            "epoch 42 - batch 550 - loss 0.860239565372467\n",
            "epoch 42 - batch 560 - loss 0.8799678087234497\n",
            "epoch 42 - batch 570 - loss 0.8976677060127258\n",
            "epoch 42 - batch 580 - loss 0.8440186381340027\n",
            "epoch 42 - batch 590 - loss 0.9029986262321472\n",
            "epoch 42 - batch 600 - loss 0.9084920883178711\n",
            "epoch 42 - batch 610 - loss 0.8918117880821228\n",
            "epoch 42 training time: 217.63388514518738 sec\n",
            "evaluation of batch 0 took: 0.1575028896331787\n",
            "evaluation of batch 50 took: 0.16114306449890137\n",
            "evaluation of batch 100 took: 0.1534130573272705\n",
            "evaluation of batch 150 took: 0.1561422348022461\n",
            "evaluation of batch 200 took: 0.16100573539733887\n",
            "evaluation of batch 250 took: 0.15944433212280273\n",
            "evaluation of batch 300 took: 0.16256451606750488\n",
            "evaluation of batch 350 took: 0.157515287399292\n",
            "evaluation of batch 400 took: 0.16262578964233398\n",
            "evaluation of batch 450 took: 0.15663552284240723\n",
            "evaluation of batch 500 took: 0.14923429489135742\n",
            "evaluation of batch 550 took: 0.1541910171508789\n",
            "evaluation of batch 600 took: 0.1568145751953125\n",
            "epoch 42 evaluation on training data time: 98.28224968910217 sec\n",
            "evaluation of batch 0 took: 0.16637229919433594\n",
            "evaluation of batch 50 took: 0.1583251953125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 evaluation on test data time: 22.497770309448242 sec\n",
            "epoch evaluation:  {'epoch': 42, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.353762>, 'test_rouge_1_p': 0.5498210843286178, 'test_rouge_1_r': 0.516888763673083, 'test_rouge_1_f1': 0.5241359423095169, 'test_rouge_2_p': 0.2824398420589825, 'test_rouge_2_r': 0.2724856686282468, 'test_rouge_2_f1': 0.27442388376020177, 'test_rouge_3_p': 0.11913407230790046, 'test_rouge_3_r': 0.11490420386904762, 'test_rouge_3_f1': 0.11567477428945058, 'test_rouge_L_p': 0.5477261554479746, 'test_rouge_L_r': 0.5151062804383116, 'test_rouge_L_f1': 0.5222626670570817}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 saved checkpoint: models/checkpoints/baseline/ckpt-15\n",
            "epoch 43 - batch 10 - loss 0.9364198446273804\n",
            "epoch 43 - batch 20 - loss 0.895431637763977\n",
            "epoch 43 - batch 30 - loss 0.8653329014778137\n",
            "epoch 43 - batch 40 - loss 0.8620688319206238\n",
            "epoch 43 - batch 50 - loss 0.9245979189872742\n",
            "epoch 43 - batch 60 - loss 0.866113543510437\n",
            "epoch 43 - batch 70 - loss 0.8863915801048279\n",
            "epoch 43 - batch 80 - loss 0.8544260859489441\n",
            "epoch 43 - batch 90 - loss 0.8466479182243347\n",
            "epoch 43 - batch 100 - loss 0.9304633736610413\n",
            "epoch 43 - batch 110 - loss 0.894327700138092\n",
            "epoch 43 - batch 120 - loss 0.9690684676170349\n",
            "epoch 43 - batch 130 - loss 0.8603872656822205\n",
            "epoch 43 - batch 140 - loss 0.8920860290527344\n",
            "epoch 43 - batch 150 - loss 0.9412277936935425\n",
            "epoch 43 - batch 160 - loss 0.8499622941017151\n",
            "epoch 43 - batch 170 - loss 0.9220905900001526\n",
            "epoch 43 - batch 180 - loss 0.8767195343971252\n",
            "epoch 43 - batch 190 - loss 0.8635035753250122\n",
            "epoch 43 - batch 200 - loss 0.8658896684646606\n",
            "epoch 43 - batch 210 - loss 0.9210809469223022\n",
            "epoch 43 - batch 220 - loss 0.8843168616294861\n",
            "epoch 43 - batch 230 - loss 0.8349164128303528\n",
            "epoch 43 - batch 240 - loss 0.9196612238883972\n",
            "epoch 43 - batch 250 - loss 0.8759948015213013\n",
            "epoch 43 - batch 260 - loss 0.815201461315155\n",
            "epoch 43 - batch 270 - loss 0.8543069958686829\n",
            "epoch 43 - batch 280 - loss 0.9083477258682251\n",
            "epoch 43 - batch 290 - loss 0.8661212921142578\n",
            "epoch 43 - batch 300 - loss 0.9363076090812683\n",
            "epoch 43 - batch 310 - loss 0.836455225944519\n",
            "epoch 43 - batch 320 - loss 0.8980860114097595\n",
            "epoch 43 - batch 330 - loss 0.9212678074836731\n",
            "epoch 43 - batch 340 - loss 0.8636918663978577\n",
            "epoch 43 - batch 350 - loss 0.8795394897460938\n",
            "epoch 43 - batch 360 - loss 0.915608823299408\n",
            "epoch 43 - batch 370 - loss 0.8707576394081116\n",
            "epoch 43 - batch 380 - loss 0.8693355917930603\n",
            "epoch 43 - batch 390 - loss 0.8265126943588257\n",
            "epoch 43 - batch 400 - loss 0.8674924969673157\n",
            "epoch 43 - batch 410 - loss 0.8935471177101135\n",
            "epoch 43 - batch 420 - loss 0.8760570287704468\n",
            "epoch 43 - batch 430 - loss 0.865097165107727\n",
            "epoch 43 - batch 440 - loss 0.9265500903129578\n",
            "epoch 43 - batch 450 - loss 0.9311687350273132\n",
            "epoch 43 - batch 460 - loss 0.8193987607955933\n",
            "epoch 43 - batch 470 - loss 0.9395967125892639\n",
            "epoch 43 - batch 480 - loss 0.8964676856994629\n",
            "epoch 43 - batch 490 - loss 0.8434268832206726\n",
            "epoch 43 - batch 500 - loss 0.8322198987007141\n",
            "epoch 43 - batch 510 - loss 0.8996180295944214\n",
            "epoch 43 - batch 520 - loss 0.8614752888679504\n",
            "epoch 43 - batch 530 - loss 0.9028164744377136\n",
            "epoch 43 - batch 540 - loss 0.8446601033210754\n",
            "epoch 43 - batch 550 - loss 0.8705053329467773\n",
            "epoch 43 - batch 560 - loss 0.8833639025688171\n",
            "epoch 43 - batch 570 - loss 0.9739993214607239\n",
            "epoch 43 - batch 580 - loss 0.8844946622848511\n",
            "epoch 43 - batch 590 - loss 0.887349545955658\n",
            "epoch 43 - batch 600 - loss 0.8520947694778442\n",
            "epoch 43 - batch 610 - loss 0.9793583154678345\n",
            "epoch 43 training time: 217.06473636627197 sec\n",
            "evaluation of batch 0 took: 0.15303707122802734\n",
            "evaluation of batch 50 took: 0.15480256080627441\n",
            "evaluation of batch 100 took: 0.15573763847351074\n",
            "evaluation of batch 150 took: 0.15966343879699707\n",
            "evaluation of batch 200 took: 0.15378141403198242\n",
            "evaluation of batch 250 took: 0.16173553466796875\n",
            "evaluation of batch 300 took: 0.1552140712738037\n",
            "evaluation of batch 350 took: 0.15436840057373047\n",
            "evaluation of batch 400 took: 0.15681219100952148\n",
            "evaluation of batch 450 took: 0.1703476905822754\n",
            "evaluation of batch 500 took: 0.1539297103881836\n",
            "evaluation of batch 550 took: 0.16210198402404785\n",
            "evaluation of batch 600 took: 0.16016530990600586\n",
            "epoch 43 evaluation on training data time: 98.47249937057495 sec\n",
            "evaluation of batch 0 took: 0.15992188453674316\n",
            "evaluation of batch 50 took: 0.15153288841247559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 43 evaluation on test data time: 22.45212483406067 sec\n",
            "epoch evaluation:  {'epoch': 43, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.354669>, 'test_rouge_1_p': 0.5495176675073439, 'test_rouge_1_r': 0.5166830332985465, 'test_rouge_1_f1': 0.5237875186231518, 'test_rouge_2_p': 0.282032095508658, 'test_rouge_2_r': 0.2724417021780302, 'test_rouge_2_f1': 0.2741573554568177, 'test_rouge_3_p': 0.11998380850919917, 'test_rouge_3_r': 0.11582411728896107, 'test_rouge_3_f1': 0.11655118427579365, 'test_rouge_L_p': 0.5474694831767936, 'test_rouge_L_r': 0.5149547532081014, 'test_rouge_L_f1': 0.5219621986058673}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "Exception in thread Thread-8:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/watchdog/observers/api.py\", line 203, in run\n",
            "    self.dispatch_events(self.event_queue, self.timeout)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/watchdog/observers/api.py\", line 376, in dispatch_events\n",
            "    handler.dispatch(event)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/watchdog/events.py\", line 452, in dispatch\n",
            "    super(PatternMatchingEventHandler, self).dispatch(event)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/watchdog/events.py\", line 336, in dispatch\n",
            "    }[event.event_type](event)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/wandb/run_manager.py\", line 696, in _on_file_modified\n",
            "    self._get_file_event_handler(event.src_path, save_name).on_modified()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/wandb/run_manager.py\", line 244, in on_modified\n",
            "    self._eventually_update()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/wandb/run_manager.py\", line 252, in _eventually_update\n",
            "    self._update()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/wandb/run_manager.py\", line 266, in _update\n",
            "    with open(self.file_path) as f:\n",
            "PermissionError: [Errno 1] Operation not permitted: './reports/wandb/run-20200629_112600-2h37hin6/config.yaml'\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 44 - batch 10 - loss 0.911722719669342\n",
            "epoch 44 - batch 20 - loss 0.909255862236023\n",
            "epoch 44 - batch 30 - loss 0.8730103373527527\n",
            "epoch 44 - batch 40 - loss 0.8696104884147644\n",
            "epoch 44 - batch 50 - loss 0.8556820750236511\n",
            "epoch 44 - batch 60 - loss 0.8860600590705872\n",
            "epoch 44 - batch 70 - loss 0.9342566728591919\n",
            "epoch 44 - batch 80 - loss 0.8420740365982056\n",
            "epoch 44 - batch 90 - loss 0.8770466446876526\n",
            "epoch 44 - batch 100 - loss 0.9170803427696228\n",
            "epoch 44 - batch 110 - loss 0.8774796724319458\n",
            "epoch 44 - batch 120 - loss 0.9430631995201111\n",
            "epoch 44 - batch 130 - loss 0.8396212458610535\n",
            "epoch 44 - batch 140 - loss 0.8757569193840027\n",
            "epoch 44 - batch 150 - loss 0.9257749915122986\n",
            "epoch 44 - batch 160 - loss 0.8568646311759949\n",
            "epoch 44 - batch 170 - loss 0.8327410817146301\n",
            "epoch 44 - batch 180 - loss 0.8593820929527283\n",
            "epoch 44 - batch 190 - loss 0.8295110464096069\n",
            "epoch 44 - batch 200 - loss 0.9058787226676941\n",
            "epoch 44 - batch 210 - loss 0.9052818417549133\n",
            "epoch 44 - batch 220 - loss 0.9473303556442261\n",
            "epoch 44 - batch 230 - loss 0.879249095916748\n",
            "epoch 44 - batch 240 - loss 0.906925618648529\n",
            "epoch 44 - batch 250 - loss 0.902088463306427\n",
            "epoch 44 - batch 260 - loss 0.8537036180496216\n",
            "epoch 44 - batch 270 - loss 0.9044346213340759\n",
            "epoch 44 - batch 280 - loss 0.8834053874015808\n",
            "epoch 44 - batch 290 - loss 0.8211740255355835\n",
            "epoch 44 - batch 300 - loss 0.8545951247215271\n",
            "epoch 44 - batch 310 - loss 0.9138280749320984\n",
            "epoch 44 - batch 320 - loss 0.9331381916999817\n",
            "epoch 44 - batch 330 - loss 0.8594499826431274\n",
            "epoch 44 - batch 340 - loss 0.9261811375617981\n",
            "epoch 44 - batch 350 - loss 0.8607198596000671\n",
            "epoch 44 - batch 360 - loss 0.8611246943473816\n",
            "epoch 44 - batch 370 - loss 0.8337637782096863\n",
            "epoch 44 - batch 380 - loss 0.8482264280319214\n",
            "epoch 44 - batch 390 - loss 0.9206857681274414\n",
            "epoch 44 - batch 400 - loss 0.8907758593559265\n",
            "epoch 44 - batch 410 - loss 0.8628884553909302\n",
            "epoch 44 - batch 420 - loss 0.9267848134040833\n",
            "epoch 44 - batch 430 - loss 0.8945989608764648\n",
            "epoch 44 - batch 440 - loss 0.8798349499702454\n",
            "epoch 44 - batch 450 - loss 0.8950093984603882\n",
            "epoch 44 - batch 460 - loss 0.8516203761100769\n",
            "epoch 44 - batch 470 - loss 0.8712844848632812\n",
            "epoch 44 - batch 480 - loss 0.8652399778366089\n",
            "epoch 44 - batch 490 - loss 0.8242220878601074\n",
            "epoch 44 - batch 500 - loss 0.8920726776123047\n",
            "epoch 44 - batch 510 - loss 0.8605025410652161\n",
            "epoch 44 - batch 520 - loss 0.8625305891036987\n",
            "epoch 44 - batch 530 - loss 0.8763855695724487\n",
            "epoch 44 - batch 540 - loss 0.8711414337158203\n",
            "epoch 44 - batch 550 - loss 0.85016930103302\n",
            "epoch 44 - batch 560 - loss 0.8681578636169434\n",
            "epoch 44 - batch 570 - loss 0.8786408305168152\n",
            "epoch 44 - batch 580 - loss 0.8223919868469238\n",
            "epoch 44 - batch 590 - loss 0.8590697050094604\n",
            "epoch 44 - batch 600 - loss 0.8888638615608215\n",
            "epoch 44 - batch 610 - loss 0.926019012928009\n",
            "epoch 44 training time: 217.72242784500122 sec\n",
            "evaluation of batch 0 took: 0.15661883354187012\n",
            "evaluation of batch 50 took: 0.15710854530334473\n",
            "evaluation of batch 100 took: 0.15868830680847168\n",
            "evaluation of batch 150 took: 0.1643965244293213\n",
            "evaluation of batch 200 took: 0.16625237464904785\n",
            "evaluation of batch 250 took: 0.15827560424804688\n",
            "evaluation of batch 300 took: 0.16106033325195312\n",
            "evaluation of batch 350 took: 0.154252290725708\n",
            "evaluation of batch 400 took: 0.15416693687438965\n",
            "evaluation of batch 450 took: 0.15513157844543457\n",
            "evaluation of batch 500 took: 0.15791559219360352\n",
            "evaluation of batch 550 took: 0.15251755714416504\n",
            "evaluation of batch 600 took: 0.15664267539978027\n",
            "epoch 44 evaluation on training data time: 98.86238598823547 sec\n",
            "evaluation of batch 0 took: 0.16272521018981934\n",
            "evaluation of batch 50 took: 0.1669154167175293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 44 evaluation on test data time: 22.52012014389038 sec\n",
            "epoch evaluation:  {'epoch': 44, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3591775>, 'test_rouge_1_p': 0.5501575967745054, 'test_rouge_1_r': 0.5168226932108069, 'test_rouge_1_f1': 0.5240977209118441, 'test_rouge_2_p': 0.2832356770833334, 'test_rouge_2_r': 0.27285050561417745, 'test_rouge_2_f1': 0.2748486283287421, 'test_rouge_3_p': 0.11985296604437233, 'test_rouge_3_r': 0.11521408279220781, 'test_rouge_3_f1': 0.1161021487193363, 'test_rouge_L_p': 0.548155069911487, 'test_rouge_L_r': 0.5151167889030611, 'test_rouge_L_f1': 0.5223040486243106}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 - batch 10 - loss 0.8190523982048035\n",
            "epoch 45 - batch 20 - loss 0.9115849733352661\n",
            "epoch 45 - batch 30 - loss 0.8979548215866089\n",
            "epoch 45 - batch 40 - loss 0.849737823009491\n",
            "epoch 45 - batch 50 - loss 0.7740138173103333\n",
            "epoch 45 - batch 60 - loss 0.93258136510849\n",
            "epoch 45 - batch 70 - loss 0.8924292922019958\n",
            "epoch 45 - batch 80 - loss 0.8879450559616089\n",
            "epoch 45 - batch 90 - loss 0.8925223350524902\n",
            "epoch 45 - batch 100 - loss 0.9505382776260376\n",
            "epoch 45 - batch 110 - loss 0.8027161359786987\n",
            "epoch 45 - batch 120 - loss 0.916630208492279\n",
            "epoch 45 - batch 130 - loss 0.8166675567626953\n",
            "epoch 45 - batch 140 - loss 0.8219001889228821\n",
            "epoch 45 - batch 150 - loss 0.8680812120437622\n",
            "epoch 45 - batch 160 - loss 0.8482871055603027\n",
            "epoch 45 - batch 170 - loss 0.8567659258842468\n",
            "epoch 45 - batch 180 - loss 0.9505252838134766\n",
            "epoch 45 - batch 190 - loss 0.8609927296638489\n",
            "epoch 45 - batch 200 - loss 0.9429145455360413\n",
            "epoch 45 - batch 210 - loss 0.8956395387649536\n",
            "epoch 45 - batch 220 - loss 0.9230971932411194\n",
            "epoch 45 - batch 230 - loss 0.8340864181518555\n",
            "epoch 45 - batch 240 - loss 0.8869892954826355\n",
            "epoch 45 - batch 250 - loss 0.8652768731117249\n",
            "epoch 45 - batch 260 - loss 0.8303250074386597\n",
            "epoch 45 - batch 270 - loss 0.9634241461753845\n",
            "epoch 45 - batch 280 - loss 0.8743955492973328\n",
            "epoch 45 - batch 290 - loss 0.852820098400116\n",
            "epoch 45 - batch 300 - loss 0.8467212319374084\n",
            "epoch 45 - batch 310 - loss 0.8118101358413696\n",
            "epoch 45 - batch 320 - loss 0.8074372410774231\n",
            "epoch 45 - batch 330 - loss 0.8583288192749023\n",
            "epoch 45 - batch 340 - loss 0.8661457896232605\n",
            "epoch 45 - batch 350 - loss 0.892078697681427\n",
            "epoch 45 - batch 360 - loss 0.8436753153800964\n",
            "epoch 45 - batch 370 - loss 0.8482838869094849\n",
            "epoch 45 - batch 380 - loss 0.8358508944511414\n",
            "epoch 45 - batch 390 - loss 0.8545383214950562\n",
            "epoch 45 - batch 400 - loss 0.8621817827224731\n",
            "epoch 45 - batch 410 - loss 0.8518065214157104\n",
            "epoch 45 - batch 420 - loss 0.879494845867157\n",
            "epoch 45 - batch 430 - loss 0.7789903283119202\n",
            "epoch 45 - batch 440 - loss 0.914854884147644\n",
            "epoch 45 - batch 450 - loss 0.9054945707321167\n",
            "epoch 45 - batch 460 - loss 0.8542030453681946\n",
            "epoch 45 - batch 470 - loss 0.9068983197212219\n",
            "epoch 45 - batch 480 - loss 0.8799347281455994\n",
            "epoch 45 - batch 490 - loss 0.8930519223213196\n",
            "epoch 45 - batch 500 - loss 0.8909222483634949\n",
            "epoch 45 - batch 510 - loss 0.8831626176834106\n",
            "epoch 45 - batch 520 - loss 0.8555558323860168\n",
            "epoch 45 - batch 530 - loss 0.8917089104652405\n",
            "epoch 45 - batch 540 - loss 0.8658894300460815\n",
            "epoch 45 - batch 550 - loss 0.8297437429428101\n",
            "epoch 45 - batch 560 - loss 0.8815995454788208\n",
            "epoch 45 - batch 570 - loss 0.9175742268562317\n",
            "epoch 45 - batch 580 - loss 0.8341353535652161\n",
            "epoch 45 - batch 590 - loss 0.8825178742408752\n",
            "epoch 45 - batch 600 - loss 0.8674312233924866\n",
            "epoch 45 - batch 610 - loss 0.9235949516296387\n",
            "epoch 45 training time: 218.2178978919983 sec\n",
            "evaluation of batch 0 took: 0.15868806838989258\n",
            "evaluation of batch 50 took: 0.15781068801879883\n",
            "evaluation of batch 100 took: 0.15560460090637207\n",
            "evaluation of batch 150 took: 0.15712857246398926\n",
            "evaluation of batch 200 took: 0.16432905197143555\n",
            "evaluation of batch 250 took: 0.15976428985595703\n",
            "evaluation of batch 300 took: 0.15412044525146484\n",
            "evaluation of batch 350 took: 0.15631532669067383\n",
            "evaluation of batch 400 took: 0.15726947784423828\n",
            "evaluation of batch 450 took: 0.15166711807250977\n",
            "evaluation of batch 500 took: 0.1589794158935547\n",
            "evaluation of batch 550 took: 0.16193890571594238\n",
            "evaluation of batch 600 took: 0.16193747520446777\n",
            "epoch 45 evaluation on training data time: 99.04130673408508 sec\n",
            "evaluation of batch 0 took: 0.16361618041992188\n",
            "evaluation of batch 50 took: 0.15391111373901367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 evaluation on test data time: 22.70685839653015 sec\n",
            "epoch evaluation:  {'epoch': 45, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3615649>, 'test_rouge_1_p': 0.5484016564722481, 'test_rouge_1_r': 0.5190876297251854, 'test_rouge_1_f1': 0.5247607130254754, 'test_rouge_2_p': 0.28300041429924244, 'test_rouge_2_r': 0.2738813920454546, 'test_rouge_2_f1': 0.27537137686729307, 'test_rouge_3_p': 0.12039747362012986, 'test_rouge_3_r': 0.11616422314664498, 'test_rouge_3_f1': 0.11688511993339004, 'test_rouge_L_p': 0.5464605497738871, 'test_rouge_L_r': 0.5174134621888525, 'test_rouge_L_f1': 0.5230143326303026}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 saved checkpoint: models/checkpoints/baseline/ckpt-16\n",
            "epoch 46 - batch 10 - loss 0.8189831972122192\n",
            "epoch 46 - batch 20 - loss 0.8680458664894104\n",
            "epoch 46 - batch 30 - loss 0.8398104310035706\n",
            "epoch 46 - batch 40 - loss 0.8643900752067566\n",
            "epoch 46 - batch 50 - loss 0.824733316898346\n",
            "epoch 46 - batch 60 - loss 0.8398040533065796\n",
            "epoch 46 - batch 70 - loss 0.8567781448364258\n",
            "epoch 46 - batch 80 - loss 0.8536003232002258\n",
            "epoch 46 - batch 90 - loss 0.8401373624801636\n",
            "epoch 46 - batch 100 - loss 0.8981078267097473\n",
            "epoch 46 - batch 110 - loss 0.8316399455070496\n",
            "epoch 46 - batch 120 - loss 0.8202029466629028\n",
            "epoch 46 - batch 130 - loss 0.8394431471824646\n",
            "epoch 46 - batch 140 - loss 0.9721497893333435\n",
            "epoch 46 - batch 150 - loss 0.8807345628738403\n",
            "epoch 46 - batch 160 - loss 0.9463587999343872\n",
            "epoch 46 - batch 170 - loss 0.8728323578834534\n",
            "epoch 46 - batch 180 - loss 0.8491072058677673\n",
            "epoch 46 - batch 190 - loss 0.8692857623100281\n",
            "epoch 46 - batch 200 - loss 0.9019160866737366\n",
            "epoch 46 - batch 210 - loss 0.8553761839866638\n",
            "epoch 46 - batch 220 - loss 0.901752769947052\n",
            "epoch 46 - batch 230 - loss 0.9020698666572571\n",
            "epoch 46 - batch 240 - loss 0.8682507276535034\n",
            "epoch 46 - batch 250 - loss 0.8684238791465759\n",
            "epoch 46 - batch 260 - loss 0.8175871968269348\n",
            "epoch 46 - batch 270 - loss 0.9265093803405762\n",
            "epoch 46 - batch 280 - loss 0.8350315093994141\n",
            "epoch 46 - batch 290 - loss 0.8290484547615051\n",
            "epoch 46 - batch 300 - loss 0.8452946543693542\n",
            "epoch 46 - batch 310 - loss 0.8792524337768555\n",
            "epoch 46 - batch 320 - loss 0.8762325048446655\n",
            "epoch 46 - batch 330 - loss 0.8900077939033508\n",
            "epoch 46 - batch 340 - loss 0.8434926271438599\n",
            "epoch 46 - batch 350 - loss 0.8100899457931519\n",
            "epoch 46 - batch 360 - loss 0.836879312992096\n",
            "epoch 46 - batch 370 - loss 0.8734772801399231\n",
            "epoch 46 - batch 380 - loss 0.8805758357048035\n",
            "epoch 46 - batch 390 - loss 0.8247886896133423\n",
            "epoch 46 - batch 400 - loss 0.8472188115119934\n",
            "epoch 46 - batch 410 - loss 0.8862366080284119\n",
            "epoch 46 - batch 420 - loss 0.827936589717865\n",
            "epoch 46 - batch 430 - loss 0.8202347159385681\n",
            "epoch 46 - batch 440 - loss 0.8148651719093323\n",
            "epoch 46 - batch 450 - loss 0.8780912756919861\n",
            "epoch 46 - batch 460 - loss 0.8560553789138794\n",
            "epoch 46 - batch 470 - loss 0.8378438353538513\n",
            "epoch 46 - batch 480 - loss 0.8603261709213257\n",
            "epoch 46 - batch 490 - loss 0.8559126257896423\n",
            "epoch 46 - batch 500 - loss 0.828109860420227\n",
            "epoch 46 - batch 510 - loss 0.8518460392951965\n",
            "epoch 46 - batch 520 - loss 0.8870201706886292\n",
            "epoch 46 - batch 530 - loss 0.8255797624588013\n",
            "epoch 46 - batch 540 - loss 0.8602280020713806\n",
            "epoch 46 - batch 550 - loss 0.858767569065094\n",
            "epoch 46 - batch 560 - loss 0.8947750926017761\n",
            "epoch 46 - batch 570 - loss 0.8996199369430542\n",
            "epoch 46 - batch 580 - loss 0.8507503867149353\n",
            "epoch 46 - batch 590 - loss 0.8787989020347595\n",
            "epoch 46 - batch 600 - loss 0.8399258852005005\n",
            "epoch 46 - batch 610 - loss 0.9074457883834839\n",
            "epoch 46 training time: 218.0300042629242 sec\n",
            "evaluation of batch 0 took: 0.15714788436889648\n",
            "evaluation of batch 50 took: 0.15128803253173828\n",
            "evaluation of batch 100 took: 0.15558838844299316\n",
            "evaluation of batch 150 took: 0.16273760795593262\n",
            "evaluation of batch 200 took: 0.17877411842346191\n",
            "evaluation of batch 250 took: 0.15639758110046387\n",
            "evaluation of batch 300 took: 0.16140961647033691\n",
            "evaluation of batch 350 took: 0.15778565406799316\n",
            "evaluation of batch 400 took: 0.16270875930786133\n",
            "evaluation of batch 450 took: 0.15565872192382812\n",
            "evaluation of batch 500 took: 0.15516996383666992\n",
            "evaluation of batch 550 took: 0.15372228622436523\n",
            "evaluation of batch 600 took: 0.15426993370056152\n",
            "epoch 46 evaluation on training data time: 99.18928861618042 sec\n",
            "evaluation of batch 0 took: 0.16096186637878418\n",
            "evaluation of batch 50 took: 0.1516857147216797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 46 evaluation on test data time: 22.56265139579773 sec\n",
            "epoch evaluation:  {'epoch': 46, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3664721>, 'test_rouge_1_p': 0.5488711855480829, 'test_rouge_1_r': 0.5160282472363943, 'test_rouge_1_f1': 0.5232526659791777, 'test_rouge_2_p': 0.2823609983766233, 'test_rouge_2_r': 0.2723288267721861, 'test_rouge_2_f1': 0.2742807146209897, 'test_rouge_3_p': 0.12000536897997832, 'test_rouge_3_r': 0.11549711681547617, 'test_rouge_3_f1': 0.11639829819238301, 'test_rouge_L_p': 0.5468708026534479, 'test_rouge_L_r': 0.5143255135861161, 'test_rouge_L_f1': 0.5214616040553283}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 47 - batch 10 - loss 0.8598054051399231\n",
            "epoch 47 - batch 20 - loss 0.856621265411377\n",
            "epoch 47 - batch 30 - loss 0.8592329621315002\n",
            "epoch 47 - batch 40 - loss 0.8813133239746094\n",
            "epoch 47 - batch 50 - loss 0.8128722906112671\n",
            "epoch 47 - batch 60 - loss 0.8483420610427856\n",
            "epoch 47 - batch 70 - loss 0.8744403123855591\n",
            "epoch 47 - batch 80 - loss 0.8968858122825623\n",
            "epoch 47 - batch 90 - loss 0.8515585660934448\n",
            "epoch 47 - batch 100 - loss 0.8737348914146423\n",
            "epoch 47 - batch 110 - loss 0.8594849705696106\n",
            "epoch 47 - batch 120 - loss 0.8750858902931213\n",
            "epoch 47 - batch 130 - loss 0.8786103129386902\n",
            "epoch 47 - batch 140 - loss 0.8566961288452148\n",
            "epoch 47 - batch 150 - loss 0.8720112442970276\n",
            "epoch 47 - batch 160 - loss 0.9180504083633423\n",
            "epoch 47 - batch 170 - loss 0.8456732630729675\n",
            "epoch 47 - batch 180 - loss 0.8487083315849304\n",
            "epoch 47 - batch 190 - loss 0.8476114273071289\n",
            "epoch 47 - batch 200 - loss 0.8777268528938293\n",
            "epoch 47 - batch 210 - loss 0.8290198445320129\n",
            "epoch 47 - batch 220 - loss 0.9238854646682739\n",
            "epoch 47 - batch 230 - loss 0.8790407180786133\n",
            "epoch 47 - batch 240 - loss 0.8818272352218628\n",
            "epoch 47 - batch 250 - loss 0.8738773465156555\n",
            "epoch 47 - batch 260 - loss 0.8445035219192505\n",
            "epoch 47 - batch 270 - loss 0.8533608317375183\n",
            "epoch 47 - batch 280 - loss 0.8463226556777954\n",
            "epoch 47 - batch 290 - loss 0.832858681678772\n",
            "epoch 47 - batch 300 - loss 0.8632363080978394\n",
            "epoch 47 - batch 310 - loss 0.8744291663169861\n",
            "epoch 47 - batch 320 - loss 0.8584817051887512\n",
            "epoch 47 - batch 330 - loss 0.8157868981361389\n",
            "epoch 47 - batch 340 - loss 0.8793312907218933\n",
            "epoch 47 - batch 350 - loss 0.8408204317092896\n",
            "epoch 47 - batch 360 - loss 0.8140520453453064\n",
            "epoch 47 - batch 370 - loss 0.8307779431343079\n",
            "epoch 47 - batch 380 - loss 0.8703694939613342\n",
            "epoch 47 - batch 390 - loss 0.8585771918296814\n",
            "epoch 47 - batch 400 - loss 0.8679198026657104\n",
            "epoch 47 - batch 410 - loss 0.8799460530281067\n",
            "epoch 47 - batch 420 - loss 0.8405264616012573\n",
            "epoch 47 - batch 430 - loss 0.8785239458084106\n",
            "epoch 47 - batch 440 - loss 0.877982497215271\n",
            "epoch 47 - batch 450 - loss 0.9226741790771484\n",
            "epoch 47 - batch 460 - loss 0.830920398235321\n",
            "epoch 47 - batch 470 - loss 0.8066415786743164\n",
            "epoch 47 - batch 480 - loss 0.8781037926673889\n",
            "epoch 47 - batch 490 - loss 0.9044622778892517\n",
            "epoch 47 - batch 500 - loss 0.8379930853843689\n",
            "epoch 47 - batch 510 - loss 0.8519682288169861\n",
            "epoch 47 - batch 520 - loss 0.890974223613739\n",
            "epoch 47 - batch 530 - loss 0.8530396223068237\n",
            "epoch 47 - batch 540 - loss 0.8481388092041016\n",
            "epoch 47 - batch 550 - loss 0.8895702362060547\n",
            "epoch 47 - batch 560 - loss 0.886897087097168\n",
            "epoch 47 - batch 570 - loss 0.9048106074333191\n",
            "epoch 47 - batch 580 - loss 0.8156663179397583\n",
            "epoch 47 - batch 590 - loss 0.9040282368659973\n",
            "epoch 47 - batch 600 - loss 0.8668412566184998\n",
            "epoch 47 - batch 610 - loss 0.8699160814285278\n",
            "epoch 47 training time: 216.10243225097656 sec\n",
            "evaluation of batch 0 took: 0.15364360809326172\n",
            "evaluation of batch 50 took: 0.15456533432006836\n",
            "evaluation of batch 100 took: 0.1622602939605713\n",
            "evaluation of batch 150 took: 0.15985679626464844\n",
            "evaluation of batch 200 took: 0.15512919425964355\n",
            "evaluation of batch 250 took: 0.1573929786682129\n",
            "evaluation of batch 300 took: 0.1565706729888916\n",
            "evaluation of batch 350 took: 0.16294026374816895\n",
            "evaluation of batch 400 took: 0.16096735000610352\n",
            "evaluation of batch 450 took: 0.15810942649841309\n",
            "evaluation of batch 500 took: 0.16627812385559082\n",
            "evaluation of batch 550 took: 0.18019723892211914\n",
            "evaluation of batch 600 took: 0.15832972526550293\n",
            "epoch 47 evaluation on training data time: 98.78486943244934 sec\n",
            "evaluation of batch 0 took: 0.16538453102111816\n",
            "evaluation of batch 50 took: 0.15816235542297363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 47 evaluation on test data time: 22.658571004867554 sec\n",
            "epoch evaluation:  {'epoch': 47, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3704839>, 'test_rouge_1_p': 0.5478194029742579, 'test_rouge_1_r': 0.5177044983476343, 'test_rouge_1_f1': 0.5238653261139563, 'test_rouge_2_p': 0.2829820244859309, 'test_rouge_2_r': 0.2733664772727272, 'test_rouge_2_f1': 0.2750762632338576, 'test_rouge_3_p': 0.11952068114177485, 'test_rouge_3_r': 0.11510184151785711, 'test_rouge_3_f1': 0.11595898598549267, 'test_rouge_L_p': 0.5457852299300405, 'test_rouge_L_r': 0.5159892330511748, 'test_rouge_L_f1': 0.5220535725266343}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 - batch 10 - loss 0.8352867364883423\n",
            "epoch 48 - batch 20 - loss 0.8428264260292053\n",
            "epoch 48 - batch 30 - loss 0.8419957160949707\n",
            "epoch 48 - batch 40 - loss 0.8340639472007751\n",
            "epoch 48 - batch 50 - loss 0.8634848594665527\n",
            "epoch 48 - batch 60 - loss 0.8731482625007629\n",
            "epoch 48 - batch 70 - loss 0.8578653931617737\n",
            "epoch 48 - batch 80 - loss 0.8510090112686157\n",
            "epoch 48 - batch 90 - loss 0.8331470489501953\n",
            "epoch 48 - batch 100 - loss 0.8985200524330139\n",
            "epoch 48 - batch 110 - loss 0.8738885521888733\n",
            "epoch 48 - batch 120 - loss 0.9225040674209595\n",
            "epoch 48 - batch 130 - loss 0.8514904975891113\n",
            "epoch 48 - batch 140 - loss 0.8934311270713806\n",
            "epoch 48 - batch 150 - loss 0.8818612098693848\n",
            "epoch 48 - batch 160 - loss 0.8648490309715271\n",
            "epoch 48 - batch 170 - loss 0.8361042737960815\n",
            "epoch 48 - batch 180 - loss 0.8498435616493225\n",
            "epoch 48 - batch 190 - loss 0.8050047159194946\n",
            "epoch 48 - batch 200 - loss 0.8104320168495178\n",
            "epoch 48 - batch 210 - loss 0.8775859475135803\n",
            "epoch 48 - batch 220 - loss 0.9340993165969849\n",
            "epoch 48 - batch 230 - loss 0.8464487791061401\n",
            "epoch 48 - batch 240 - loss 0.8557046055793762\n",
            "epoch 48 - batch 250 - loss 0.9007757902145386\n",
            "epoch 48 - batch 260 - loss 0.8515840172767639\n",
            "epoch 48 - batch 270 - loss 0.846405029296875\n",
            "epoch 48 - batch 280 - loss 0.8499380946159363\n",
            "epoch 48 - batch 290 - loss 0.849330723285675\n",
            "epoch 48 - batch 300 - loss 0.8067246079444885\n",
            "epoch 48 - batch 310 - loss 0.875132143497467\n",
            "epoch 48 - batch 320 - loss 0.8857265114784241\n",
            "epoch 48 - batch 330 - loss 0.8376383185386658\n",
            "epoch 48 - batch 340 - loss 0.9202585220336914\n",
            "epoch 48 - batch 350 - loss 0.8316481709480286\n",
            "epoch 48 - batch 360 - loss 0.8268729448318481\n",
            "epoch 48 - batch 370 - loss 0.8438535332679749\n",
            "epoch 48 - batch 380 - loss 0.7838892340660095\n",
            "epoch 48 - batch 390 - loss 0.8446989059448242\n",
            "epoch 48 - batch 400 - loss 0.8706479668617249\n",
            "epoch 48 - batch 410 - loss 0.8340622186660767\n",
            "epoch 48 - batch 420 - loss 0.8827165961265564\n",
            "epoch 48 - batch 430 - loss 0.9048365354537964\n",
            "epoch 48 - batch 440 - loss 0.8426434397697449\n",
            "epoch 48 - batch 450 - loss 0.8324045538902283\n",
            "epoch 48 - batch 460 - loss 0.8312708735466003\n",
            "epoch 48 - batch 470 - loss 0.8327263593673706\n",
            "epoch 48 - batch 480 - loss 0.8691731691360474\n",
            "epoch 48 - batch 490 - loss 0.8452247977256775\n",
            "epoch 48 - batch 500 - loss 0.7968157529830933\n",
            "epoch 48 - batch 510 - loss 0.812764048576355\n",
            "epoch 48 - batch 520 - loss 0.9055250883102417\n",
            "epoch 48 - batch 530 - loss 0.8537977933883667\n",
            "epoch 48 - batch 540 - loss 0.8325767517089844\n",
            "epoch 48 - batch 550 - loss 0.8978949785232544\n",
            "epoch 48 - batch 560 - loss 0.888508677482605\n",
            "epoch 48 - batch 570 - loss 0.8270655870437622\n",
            "epoch 48 - batch 580 - loss 0.9072556495666504\n",
            "epoch 48 - batch 590 - loss 0.8360433578491211\n",
            "epoch 48 - batch 600 - loss 0.813727080821991\n",
            "epoch 48 - batch 610 - loss 0.8764818906784058\n",
            "epoch 48 training time: 217.32196617126465 sec\n",
            "evaluation of batch 0 took: 0.15593481063842773\n",
            "evaluation of batch 50 took: 0.15688276290893555\n",
            "evaluation of batch 100 took: 0.15527844429016113\n",
            "evaluation of batch 150 took: 0.15758919715881348\n",
            "evaluation of batch 200 took: 0.16495847702026367\n",
            "evaluation of batch 250 took: 0.1677844524383545\n",
            "evaluation of batch 300 took: 0.16111469268798828\n",
            "evaluation of batch 350 took: 0.16303324699401855\n",
            "evaluation of batch 400 took: 0.16141366958618164\n",
            "evaluation of batch 450 took: 0.17202448844909668\n",
            "evaluation of batch 500 took: 0.16577363014221191\n",
            "evaluation of batch 550 took: 0.15934205055236816\n",
            "evaluation of batch 600 took: 0.16597533226013184\n",
            "epoch 48 evaluation on training data time: 99.14574813842773 sec\n",
            "evaluation of batch 0 took: 0.16460633277893066\n",
            "evaluation of batch 50 took: 0.15321040153503418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 evaluation on test data time: 22.58186388015747 sec\n",
            "epoch evaluation:  {'epoch': 48, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3725419>, 'test_rouge_1_p': 0.5466434212179191, 'test_rouge_1_r': 0.5173777092513143, 'test_rouge_1_f1': 0.5230239064743426, 'test_rouge_2_p': 0.282078387107684, 'test_rouge_2_r': 0.27286804991883107, 'test_rouge_2_f1': 0.27434501445636794, 'test_rouge_3_p': 0.11975023674242426, 'test_rouge_3_r': 0.11546097132034633, 'test_rouge_3_f1': 0.11617086642895794, 'test_rouge_L_p': 0.5446363346474955, 'test_rouge_L_r': 0.5156613568722943, 'test_rouge_L_f1': 0.5212198270817388}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 saved checkpoint: models/checkpoints/baseline/ckpt-17\n",
            "epoch 49 - batch 10 - loss 0.8464433550834656\n",
            "epoch 49 - batch 20 - loss 0.8776922225952148\n",
            "epoch 49 - batch 30 - loss 0.845463216304779\n",
            "epoch 49 - batch 40 - loss 0.8734613656997681\n",
            "epoch 49 - batch 50 - loss 0.840926468372345\n",
            "epoch 49 - batch 60 - loss 0.8332617878913879\n",
            "epoch 49 - batch 70 - loss 0.8329983353614807\n",
            "epoch 49 - batch 80 - loss 0.8394113183021545\n",
            "epoch 49 - batch 90 - loss 0.8485854864120483\n",
            "epoch 49 - batch 100 - loss 0.8959065675735474\n",
            "epoch 49 - batch 110 - loss 0.8656982779502869\n",
            "epoch 49 - batch 120 - loss 0.8528912663459778\n",
            "epoch 49 - batch 130 - loss 0.7934668660163879\n",
            "epoch 49 - batch 140 - loss 0.8362916707992554\n",
            "epoch 49 - batch 150 - loss 0.9157176613807678\n",
            "epoch 49 - batch 160 - loss 0.8367365598678589\n",
            "epoch 49 - batch 170 - loss 0.86546790599823\n",
            "epoch 49 - batch 180 - loss 0.8418970108032227\n",
            "epoch 49 - batch 190 - loss 0.8753058314323425\n",
            "epoch 49 - batch 200 - loss 0.8914213180541992\n",
            "epoch 49 - batch 210 - loss 0.8628320693969727\n",
            "epoch 49 - batch 220 - loss 0.8418322801589966\n",
            "epoch 49 - batch 230 - loss 0.805034339427948\n",
            "epoch 49 - batch 240 - loss 0.8089414238929749\n",
            "epoch 49 - batch 250 - loss 0.8798280358314514\n",
            "epoch 49 - batch 260 - loss 0.8858351111412048\n",
            "epoch 49 - batch 270 - loss 0.8048951029777527\n",
            "epoch 49 - batch 280 - loss 0.8261857628822327\n",
            "epoch 49 - batch 290 - loss 0.8488677144050598\n",
            "epoch 49 - batch 300 - loss 0.8139514923095703\n",
            "epoch 49 - batch 310 - loss 0.8809366226196289\n",
            "epoch 49 - batch 320 - loss 0.8847465515136719\n",
            "epoch 49 - batch 330 - loss 0.8117138743400574\n",
            "epoch 49 - batch 340 - loss 0.8481189608573914\n",
            "epoch 49 - batch 350 - loss 0.8056896924972534\n",
            "epoch 49 - batch 360 - loss 0.8176798820495605\n",
            "epoch 49 - batch 370 - loss 0.8551872372627258\n",
            "epoch 49 - batch 380 - loss 0.8483731150627136\n",
            "epoch 49 - batch 390 - loss 0.8319639563560486\n",
            "epoch 49 - batch 400 - loss 0.855593204498291\n",
            "epoch 49 - batch 410 - loss 0.9050263166427612\n",
            "epoch 49 - batch 420 - loss 0.9289043545722961\n",
            "epoch 49 - batch 430 - loss 0.8726990818977356\n",
            "epoch 49 - batch 440 - loss 0.8101657032966614\n",
            "epoch 49 - batch 450 - loss 0.8925045728683472\n",
            "epoch 49 - batch 460 - loss 0.7939792275428772\n",
            "epoch 49 - batch 470 - loss 0.8971166014671326\n",
            "epoch 49 - batch 480 - loss 0.8773588538169861\n",
            "epoch 49 - batch 490 - loss 0.848382294178009\n",
            "epoch 49 - batch 500 - loss 0.7923054099082947\n",
            "epoch 49 - batch 510 - loss 0.8109560012817383\n",
            "epoch 49 - batch 520 - loss 0.8115014433860779\n",
            "epoch 49 - batch 530 - loss 0.8575677275657654\n",
            "epoch 49 - batch 540 - loss 0.8197814226150513\n",
            "epoch 49 - batch 550 - loss 0.8412903547286987\n",
            "epoch 49 - batch 560 - loss 0.8515589833259583\n",
            "epoch 49 - batch 570 - loss 0.8384696841239929\n",
            "epoch 49 - batch 580 - loss 0.8663560152053833\n",
            "epoch 49 - batch 590 - loss 0.9088061451911926\n",
            "epoch 49 - batch 600 - loss 0.7805152535438538\n",
            "epoch 49 - batch 610 - loss 0.9174319505691528\n",
            "epoch 49 training time: 218.17665147781372 sec\n",
            "evaluation of batch 0 took: 0.16153478622436523\n",
            "evaluation of batch 50 took: 0.15396332740783691\n",
            "evaluation of batch 100 took: 0.1651155948638916\n",
            "evaluation of batch 150 took: 0.16251063346862793\n",
            "evaluation of batch 200 took: 0.1687486171722412\n",
            "evaluation of batch 250 took: 0.15942168235778809\n",
            "evaluation of batch 300 took: 0.1628427505493164\n",
            "evaluation of batch 350 took: 0.15535640716552734\n",
            "evaluation of batch 400 took: 0.15956926345825195\n",
            "evaluation of batch 450 took: 0.16145849227905273\n",
            "evaluation of batch 500 took: 0.16772031784057617\n",
            "evaluation of batch 550 took: 0.16835331916809082\n",
            "evaluation of batch 600 took: 0.16601085662841797\n",
            "epoch 49 evaluation on training data time: 98.75941920280457 sec\n",
            "evaluation of batch 0 took: 0.16118216514587402\n",
            "evaluation of batch 50 took: 0.16705560684204102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 49 evaluation on test data time: 22.619874715805054 sec\n",
            "epoch evaluation:  {'epoch': 49, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3741112>, 'test_rouge_1_p': 0.5465446778853589, 'test_rouge_1_r': 0.5185590660269789, 'test_rouge_1_f1': 0.5235429875522204, 'test_rouge_2_p': 0.2814021915584415, 'test_rouge_2_r': 0.27343876826298696, 'test_rouge_2_f1': 0.27440252917627106, 'test_rouge_3_p': 0.12005927015692643, 'test_rouge_3_r': 0.11644641166125544, 'test_rouge_3_f1': 0.11690977053700269, 'test_rouge_L_p': 0.5444845658433828, 'test_rouge_L_r': 0.5167570756995978, 'test_rouge_L_f1': 0.5216730072636374}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 50 - batch 10 - loss 0.8413720726966858\n",
            "epoch 50 - batch 20 - loss 0.8412604928016663\n",
            "epoch 50 - batch 30 - loss 0.8603997826576233\n",
            "epoch 50 - batch 40 - loss 0.8726649880409241\n",
            "epoch 50 - batch 50 - loss 0.8261845707893372\n",
            "epoch 50 - batch 60 - loss 0.8132057189941406\n",
            "epoch 50 - batch 70 - loss 0.9069117307662964\n",
            "epoch 50 - batch 80 - loss 0.8558676838874817\n",
            "epoch 50 - batch 90 - loss 0.826498806476593\n",
            "epoch 50 - batch 100 - loss 0.8402538299560547\n",
            "epoch 50 - batch 110 - loss 0.8495314717292786\n",
            "epoch 50 - batch 120 - loss 0.8620043992996216\n",
            "epoch 50 - batch 130 - loss 0.8186782002449036\n",
            "epoch 50 - batch 140 - loss 0.7844666242599487\n",
            "epoch 50 - batch 150 - loss 0.8352819085121155\n",
            "epoch 50 - batch 160 - loss 0.8527398705482483\n",
            "epoch 50 - batch 170 - loss 0.8278715014457703\n",
            "epoch 50 - batch 180 - loss 0.8106671571731567\n",
            "epoch 50 - batch 190 - loss 0.8620635271072388\n",
            "epoch 50 - batch 200 - loss 0.8371495008468628\n",
            "epoch 50 - batch 210 - loss 0.8470035791397095\n",
            "epoch 50 - batch 220 - loss 0.8475213646888733\n",
            "epoch 50 - batch 230 - loss 0.8039916157722473\n",
            "epoch 50 - batch 240 - loss 0.832794189453125\n",
            "epoch 50 - batch 250 - loss 0.9046317338943481\n",
            "epoch 50 - batch 260 - loss 0.8003111481666565\n",
            "epoch 50 - batch 270 - loss 0.8287203907966614\n",
            "epoch 50 - batch 280 - loss 0.8578072786331177\n",
            "epoch 50 - batch 290 - loss 0.8773459196090698\n",
            "epoch 50 - batch 300 - loss 0.8772056698799133\n",
            "epoch 50 - batch 310 - loss 0.7827115654945374\n",
            "epoch 50 - batch 320 - loss 0.8446151614189148\n",
            "epoch 50 - batch 330 - loss 0.8081492185592651\n",
            "epoch 50 - batch 340 - loss 0.8347705006599426\n",
            "epoch 50 - batch 350 - loss 0.845982551574707\n",
            "epoch 50 - batch 360 - loss 0.8505740761756897\n",
            "epoch 50 - batch 370 - loss 0.8701832890510559\n",
            "epoch 50 - batch 380 - loss 0.7977763414382935\n",
            "epoch 50 - batch 390 - loss 0.8235560655593872\n",
            "epoch 50 - batch 400 - loss 0.8472962975502014\n",
            "epoch 50 - batch 410 - loss 0.8264045715332031\n",
            "epoch 50 - batch 420 - loss 0.7842768430709839\n",
            "epoch 50 - batch 430 - loss 0.8605254292488098\n",
            "epoch 50 - batch 440 - loss 0.9046501517295837\n",
            "epoch 50 - batch 450 - loss 0.8644570708274841\n",
            "epoch 50 - batch 460 - loss 0.821739137172699\n",
            "epoch 50 - batch 470 - loss 0.8072057962417603\n",
            "epoch 50 - batch 480 - loss 0.8494811058044434\n",
            "epoch 50 - batch 490 - loss 0.8020883202552795\n",
            "epoch 50 - batch 500 - loss 0.7761934995651245\n",
            "epoch 50 - batch 510 - loss 0.9054839015007019\n",
            "epoch 50 - batch 520 - loss 0.8130252957344055\n",
            "epoch 50 - batch 530 - loss 0.8493685126304626\n",
            "epoch 50 - batch 540 - loss 0.850734531879425\n",
            "epoch 50 - batch 550 - loss 0.7995308041572571\n",
            "epoch 50 - batch 560 - loss 0.8824809193611145\n",
            "epoch 50 - batch 570 - loss 0.8810925483703613\n",
            "epoch 50 - batch 580 - loss 0.8243464827537537\n",
            "epoch 50 - batch 590 - loss 0.8051711320877075\n",
            "epoch 50 - batch 600 - loss 0.8304030299186707\n",
            "epoch 50 - batch 610 - loss 0.8148669600486755\n",
            "epoch 50 training time: 218.2728488445282 sec\n",
            "evaluation of batch 0 took: 0.1602339744567871\n",
            "evaluation of batch 50 took: 0.16260218620300293\n",
            "evaluation of batch 100 took: 0.16408729553222656\n",
            "evaluation of batch 150 took: 0.16400146484375\n",
            "evaluation of batch 200 took: 0.15717124938964844\n",
            "evaluation of batch 250 took: 0.15437912940979004\n",
            "evaluation of batch 300 took: 0.15999746322631836\n",
            "evaluation of batch 350 took: 0.15851449966430664\n",
            "evaluation of batch 400 took: 0.15504145622253418\n",
            "evaluation of batch 450 took: 0.15383577346801758\n",
            "evaluation of batch 500 took: 0.1581249237060547\n",
            "evaluation of batch 550 took: 0.15882325172424316\n",
            "evaluation of batch 600 took: 0.1567213535308838\n",
            "epoch 50 evaluation on training data time: 98.84019875526428 sec\n",
            "evaluation of batch 0 took: 0.1605379581451416\n",
            "evaluation of batch 50 took: 0.15236425399780273\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 50 evaluation on test data time: 22.596935033798218 sec\n",
            "epoch evaluation:  {'epoch': 50, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3755338>, 'test_rouge_1_p': 0.5468542548411408, 'test_rouge_1_r': 0.519023340870439, 'test_rouge_1_f1': 0.5242579163655157, 'test_rouge_2_p': 0.28306044541396114, 'test_rouge_2_r': 0.27439926609848486, 'test_rouge_2_f1': 0.2757308049769494, 'test_rouge_3_p': 0.12065683340097404, 'test_rouge_3_r': 0.11664616308171, 'test_rouge_3_f1': 0.11728427047902494, 'test_rouge_L_p': 0.5449458814065401, 'test_rouge_L_r': 0.5174068189065401, 'test_rouge_L_f1': 0.5225517559838023}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-18\n",
            "Done saving model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "00-colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpetkoff/identifier-suggestion/blob/master/notebooks/00-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeJd9knlVGS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utilities\n",
        "import os\n",
        "import shutil\n",
        "from subprocess import check_output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntLDgQhocMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2325737b-cae1-4d1c-f109-4834d8543220"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 30 23:32:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_PqX9NDdEJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "1a047e9e-52c3-4290-b723-8b90d4b027a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvcl4iohdWEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "041371b3-0c8c-489c-afa3-6126a248a7e6"
      },
      "source": [
        "%env WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
        "\n",
        "# TODO: how can one read an environment variable?!?\n",
        "%cd '/content/gdrive/My Drive/src'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
            "/content/gdrive/My Drive/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNR4ZqljgeyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = check_output(['date', '-u', \"+%Y-%m-%dT%H-%M-%S\"]).decode('utf-8').strip()\n",
        "\n",
        "os.environ['PROJECT_DIR'] = os.path.join(\n",
        "    os.environ['WORKSPACE_DIR'],\n",
        "    f'identifier-suggestion-{timestamp}',\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoI39h1dEJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "17655976-8751-4d8d-c38e-63a795b85d90"
      },
      "source": [
        "!git clone https://github.com/antonpetkoff/identifier-suggestion.git --depth 1 \"${PROJECT_DIR}\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/src/identifier-suggestion-2020-06-30T23-33-13'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 113 (delta 1), reused 65 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (113/113), 985.08 KiB | 1.30 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mki1yqMgixt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.environ['PROJECT_DIR'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjKIGx4i2dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "2b60d1d0-8148-4ceb-a50c-a33117885a67"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/src/identifier-suggestion-2020-06-30T23-33-13\n",
            "total 69\n",
            "drwx------  4 root root  4096 Jun 30 23:33 data\n",
            "-rw-------  1 root root 35149 Jun 30 23:33 LICENSE\n",
            "drwx------  2 root root  4096 Jun 30 23:33 notebooks\n",
            "-rw-------  1 root root 10499 Jun 30 23:33 README.md\n",
            "drwx------  3 root root  4096 Jun 30 23:33 reports\n",
            "drwx------  2 root root  4096 Jun 30 23:33 requirements\n",
            "drwx------ 13 root root  4096 Jun 30 23:33 src\n",
            "drwx------  3 root root  4096 Jun 30 23:33 vscode-extension\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCr-lrn4SRhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8880932f-0645-40b4-ce8a-2d36504c8f93"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7BkvE4dEJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "984e0679-6fd9-4b10-ebf6-d76f209c5b4e"
      },
      "source": [
        "# Google Colab has standard libraries like numpy, pandas, matplotlib and TF (of course) pre-installed\n",
        "!pip install -r requirements/colab.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting javalang==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/e0/12344443d66b9a84844171be90112892a371da6db09866741774b8bc0a2f/javalang-0.13.0-py3-none-any.whl\n",
            "Collecting pydash==4.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/fc/19b89da8a38a89f4403451a4ed2a05e16f804e6c71e4d5eaedb9d56366c3/pydash-4.8.0-py2.py3-none-any.whl (84kB)\n",
            "\r\u001b[K     |███▉                            | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.9MB/s \n",
            "\u001b[?25hCollecting python-dotenv==0.13.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cb/2a/07f87440444fdf2c5870a710b6770d766a1c7df9c827b0c90e807f1fb4c5/python_dotenv-0.13.0-py2.py3-none-any.whl\n",
            "Collecting wandb==0.8.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/c9/ebbcefa6ef2ba14a7c62a4ee4415a5fecef8fac5e4d1b4e22af26fd9fe22/wandb-0.8.35-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.4MB/s \n",
            "\u001b[?25hCollecting tables==3.6.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/c3/8fd9e3bb21872f9d69eb93b3014c86479864cca94e625fd03713ccacec80/tables-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 16.1MB/s \n",
            "\u001b[?25hCollecting rouge-score==0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting pathos==0.2.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/ea/b2cf3a6561fc5deb64de8ae0af5e3e4e2db03ca588cb7415efce4a8de26e/pathos-0.2.6.zip (219kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from javalang==0.13.0->-r requirements/colab.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (7.352.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.2MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/f9/c315aa88e51fabdc08e91b333cfefb255aff04a2ee96d632c32cb19180c9/GitPython-3.1.3-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 44.7MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.23.0)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.8.1)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.1MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/6b/939519d77c95a9b2c85b771e9dccbf9e69cb90016c7cd63887c26400dd7a/sentry_sdk-0.15.1-py2.py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (3.2.5)\n",
            "Collecting ppft>=1.6.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/fb/fa21f6e9aedc4823448473ed96e8eab64af1cb248c18165f045a90e1c6b4/ppft-1.6.6.2.zip (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from pathos==0.2.6->-r requirements/colab.txt (line 7)) (0.3.2)\n",
            "Collecting pox>=0.2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/0c/ec447fb0ed88bc1c09bf0dadf00e40ea05fda17e841d15bb351a52d9e192/pox-0.2.8.zip (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess>=0.70.10 in /usr/local/lib/python3.6/dist-packages (from pathos==0.2.6->-r requirements/colab.txt (line 7)) (0.70.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (1.24.3)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.8.35->-r requirements/colab.txt (line 4)) (2.3)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathos, subprocess32, gql, watchdog, ppft, pox, graphql-core, pathtools\n",
            "  Building wheel for pathos (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathos: filename=pathos-0.2.6-cp36-none-any.whl size=77673 sha256=6876236bebcf715e9ea7db2851ff267cc6bff62f7b126bb703cf013c23a24ba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/e8/c8/04cdd0c4bc6fbce35f642fc004244228916daae74bb0f482da\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=2c5ab60f44e52818605a1e1616baf8bf9fdb0964730511c88308c466fe16e246\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=544539f1aeba9941ef0311ca2bae297abeaa8ce547fa92f2a2c95d2c17783e2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=b79e545207735ad0730ef8f8bd8c98086bd5b015347ad2f11ec6ce4de9f8fdad\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for ppft (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ppft: filename=ppft-1.6.6.2-cp36-none-any.whl size=64743 sha256=0076465ec3d6e750bfb352fb1d1baec995113a1c4b7426c741dda33b5df43ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/d2/2d/0ee21ede61786bb13247dbc69079373fd500c2bb0481913084\n",
            "  Building wheel for pox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pox: filename=pox-0.2.8-cp36-none-any.whl size=28290 sha256=0815850e284db452d111483ca7cb783031943fbd517bc3d5b388d6e1f5daad74\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/ed/ce/a93103746b327e18bffaeb99ba0d57a88b392f31d719cea700\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=cbdff776824a4409e04abf7075b8db234aaaf0745fff70e8763f229691b634c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=d43d5dc7aa0322c94964b1a5d678e5faacc506a0b43c4a92d985ffb54f990a2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built pathos subprocess32 gql watchdog ppft pox graphql-core pathtools\n",
            "\u001b[31mERROR: rouge-score 0.0.4 has requirement six>=1.14.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: javalang, pydash, python-dotenv, docker-pycreds, subprocess32, smmap, gitdb, GitPython, shortuuid, graphql-core, gql, configparser, pathtools, watchdog, sentry-sdk, wandb, tables, rouge-score, ppft, pox, pathos\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "Successfully installed GitPython-3.1.3 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 javalang-0.13.0 pathos-0.2.6 pathtools-0.1.2 pox-0.2.8 ppft-1.6.6.2 pydash-4.8.0 python-dotenv-0.13.0 rouge-score-0.0.4 sentry-sdk-0.15.1 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 tables-3.6.1 wandb-0.8.35 watchdog-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrqt3o4TP5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9263ef2c-506d-4eb6-a377-443dc975a569"
      },
      "source": [
        "# provide secrets to the project, e.g. access to wandb\n",
        "shutil.copy(\n",
        "    os.path.join(os.environ['WORKSPACE_DIR'], 'secrets/.env'),\n",
        "    os.environ['PROJECT_DIR']\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/src/identifier-suggestion-2020-06-30T23-33-13/.env'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhbZ4muuveC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# monkey-patch (mock) os.symlink to be a noop, because wandb.save() uses it, but it is not supported by Google Colab Notebooks\n",
        "os.symlink = lambda *x: print('Executing mocked noop symlink with arguments', x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHkwwGMPts7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "68f5bde3-5fc4-42f7-ad97-491c74253265"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY3fkLJIm7ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87ee4e78-f2b2-42cb-ed6f-54ae74e37757"
      },
      "source": [
        "from argparse import Namespace\n",
        "from src.pipelines.baseline import run\n",
        "\n",
        "params = {\n",
        "  'dir_data': '../data/processed/subtoken/',\n",
        "  'file_checkpoint_dir': 'models/checkpoints/baseline/',\n",
        "  'dir_preprocessed_data': '../data/processed/seq2seq/',\n",
        "  'max_input_length': 128,\n",
        "  'max_output_length': 8,\n",
        "  'input_vocab_size': 5000,\n",
        "  'input_embedding_dim': 50,\n",
        "  'output_vocab_size': 5000,\n",
        "  'output_embedding_dim': 50,\n",
        "  'latent_dim': 320,\n",
        "  'learning_rate': 0.001,\n",
        "  'dropout_rate': 0.05,\n",
        "  'epochs': 50,\n",
        "  'early_stopping_patience': 3,\n",
        "  'early_stopping_min_delta': 0.001,\n",
        "  'evaluation_dataset': 'validation',\n",
        "  'batch_size': 1024,\n",
        "  'random_seed': 1,\n",
        "}\n",
        "\n",
        "run(Namespace(**params))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing logger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/336uv99f\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/336uv99f</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Experiment parameters:  Namespace(batch_size=1024, dir_data='../data/processed/subtoken/', dir_preprocessed_data='../data/processed/seq2seq/', dropout_rate=0.05, early_stopping_min_delta=0.001, early_stopping_patience=3, epochs=50, evaluation_dataset='validation', file_checkpoint_dir='models/checkpoints/baseline/', input_embedding_dim=50, input_vocab_size=5000, latent_dim=320, learning_rate=0.001, max_input_length=128, max_output_length=8, output_embedding_dim=50, output_vocab_size=5000, random_seed=1)\n",
            "Loading preprocessed files...\n",
            "Loaded input vocabulary.\n",
            "Loaded output vocabulary.\n",
            "Loaded preprocessed files.\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "EncoderLSTM (LSTM)           multiple                  474880    \n",
            "=================================================================\n",
            "Total params: 724,880\n",
            "Trainable params: 724,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "DecoderEmbedding (Embedding) multiple                  250000    \n",
            "_________________________________________________________________\n",
            "DecoderLSTM (LSTM)           multiple                  884480    \n",
            "_________________________________________________________________\n",
            "DenseOutput (Dense)          multiple                  1605000   \n",
            "_________________________________________________________________\n",
            "bahdanau_attention (Bahdanau multiple                  205761    \n",
            "=================================================================\n",
            "Total params: 2,945,241\n",
            "Trainable params: 2,945,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-1\n",
            "Executing mocked noop symlink with arguments ('/content/gdrive/My Drive/src/identifier-suggestion-2020-06-30T23-33-13/models/checkpoints/baseline/config.json', './reports/wandb/run-20200630_233355-336uv99f/config.json')\n",
            "Done saving model\n",
            "Restored from models/checkpoints/baseline/ckpt-1\n",
            "epoch 1 - batch 10 - loss 4.408756732940674\n",
            "epoch 1 - batch 20 - loss 3.08723783493042\n",
            "epoch 1 - batch 30 - loss 3.1153604984283447\n",
            "epoch 1 - batch 40 - loss 3.1828439235687256\n",
            "epoch 1 - batch 50 - loss 3.0684542655944824\n",
            "epoch 1 - batch 60 - loss 3.129190444946289\n",
            "epoch 1 - batch 70 - loss 3.215830087661743\n",
            "epoch 1 - batch 80 - loss 3.1878738403320312\n",
            "epoch 1 - batch 90 - loss 3.1342785358428955\n",
            "epoch 1 - batch 100 - loss 3.0857670307159424\n",
            "epoch 1 - batch 110 - loss 3.153585195541382\n",
            "epoch 1 - batch 120 - loss 3.0321671962738037\n",
            "epoch 1 - batch 130 - loss 3.171478748321533\n",
            "epoch 1 - batch 140 - loss 3.091330051422119\n",
            "epoch 1 - batch 150 - loss 3.07035231590271\n",
            "epoch 1 - batch 160 - loss 2.955005168914795\n",
            "epoch 1 - batch 170 - loss 3.0259265899658203\n",
            "epoch 1 - batch 180 - loss 3.0205094814300537\n",
            "epoch 1 - batch 190 - loss 3.015817403793335\n",
            "epoch 1 - batch 200 - loss 2.8715548515319824\n",
            "epoch 1 - batch 210 - loss 3.036540985107422\n",
            "epoch 1 - batch 220 - loss 2.987924575805664\n",
            "epoch 1 - batch 230 - loss 2.899728298187256\n",
            "epoch 1 - batch 240 - loss 2.922208070755005\n",
            "epoch 1 - batch 250 - loss 2.9066736698150635\n",
            "epoch 1 - batch 260 - loss 2.8901429176330566\n",
            "epoch 1 - batch 270 - loss 2.793677806854248\n",
            "epoch 1 - batch 280 - loss 2.8648135662078857\n",
            "epoch 1 - batch 290 - loss 2.9003548622131348\n",
            "epoch 1 - batch 300 - loss 2.9491591453552246\n",
            "epoch 1 - batch 310 - loss 2.8610241413116455\n",
            "epoch 1 - batch 320 - loss 2.9023568630218506\n",
            "epoch 1 - batch 330 - loss 2.9033737182617188\n",
            "epoch 1 - batch 340 - loss 2.777205228805542\n",
            "epoch 1 - batch 350 - loss 2.869511842727661\n",
            "epoch 1 - batch 360 - loss 2.891423463821411\n",
            "epoch 1 - batch 370 - loss 2.7679896354675293\n",
            "epoch 1 - batch 380 - loss 2.7468225955963135\n",
            "epoch 1 - batch 390 - loss 2.8586578369140625\n",
            "epoch 1 - batch 400 - loss 2.7625787258148193\n",
            "epoch 1 - batch 410 - loss 2.7683069705963135\n",
            "epoch 1 - batch 420 - loss 2.791799545288086\n",
            "epoch 1 - batch 430 - loss 2.6827547550201416\n",
            "epoch 1 - batch 440 - loss 2.8427374362945557\n",
            "epoch 1 - batch 450 - loss 2.6668264865875244\n",
            "epoch 1 - batch 460 - loss 2.802610158920288\n",
            "epoch 1 - batch 470 - loss 2.84684681892395\n",
            "epoch 1 - batch 480 - loss 2.8351380825042725\n",
            "epoch 1 - batch 490 - loss 2.756720781326294\n",
            "epoch 1 - batch 500 - loss 2.7930796146392822\n",
            "epoch 1 - batch 510 - loss 2.8025095462799072\n",
            "epoch 1 - batch 520 - loss 2.7332489490509033\n",
            "epoch 1 - batch 530 - loss 2.8277134895324707\n",
            "epoch 1 - batch 540 - loss 2.7241859436035156\n",
            "epoch 1 - batch 550 - loss 2.712717294692993\n",
            "epoch 1 - batch 560 - loss 2.794786214828491\n",
            "epoch 1 - batch 570 - loss 2.6835615634918213\n",
            "epoch 1 - batch 580 - loss 2.790191650390625\n",
            "epoch 1 - batch 590 - loss 2.750633716583252\n",
            "epoch 1 - batch 600 - loss 2.7594621181488037\n",
            "epoch 1 - batch 610 - loss 2.699265718460083\n",
            "epoch 1 training time: 225.96883964538574 sec\n",
            "evaluation of batch 0 took: 0.28659868240356445\n",
            "evaluation of batch 50 took: 0.1016700267791748\n",
            "evaluation of batch 100 took: 0.11197996139526367\n",
            "evaluation of batch 150 took: 0.10611891746520996\n",
            "evaluation of batch 200 took: 0.1143488883972168\n",
            "evaluation of batch 250 took: 0.11664628982543945\n",
            "evaluation of batch 300 took: 0.11452102661132812\n",
            "evaluation of batch 350 took: 0.11813211441040039\n",
            "evaluation of batch 400 took: 0.14704394340515137\n",
            "evaluation of batch 450 took: 0.13172316551208496\n",
            "evaluation of batch 500 took: 0.11945748329162598\n",
            "evaluation of batch 550 took: 0.12876629829406738\n",
            "evaluation of batch 600 took: 0.12534809112548828\n",
            "epoch 1 evaluation on training data time: 73.59410119056702 sec\n",
            "evaluation of batch 0 took: 0.13929438591003418\n",
            "evaluation of batch 50 took: 0.12878870964050293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1 evaluation on test data time: 20.025397777557373 sec\n",
            "epoch evaluation:  {'epoch': 1, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.753346>, 'test_rouge_1_p': 0.239067573051948, 'test_rouge_1_r': 0.10253368748067401, 'test_rouge_1_f1': 0.1381500672763612, 'test_rouge_2_p': 0.005516943993506494, 'test_rouge_2_r': 0.0017303334686147183, 'test_rouge_2_f1': 0.0024998067408781694, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.23904854910714282, 'test_rouge_L_r': 0.10252544377125844, 'test_rouge_L_f1': 0.13813859251600252}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 - batch 10 - loss 2.740074634552002\n",
            "epoch 2 - batch 20 - loss 2.8388512134552\n",
            "epoch 2 - batch 30 - loss 2.7030441761016846\n",
            "epoch 2 - batch 40 - loss 2.8343117237091064\n",
            "epoch 2 - batch 50 - loss 2.8263630867004395\n",
            "epoch 2 - batch 60 - loss 2.751079559326172\n",
            "epoch 2 - batch 70 - loss 2.706350326538086\n",
            "epoch 2 - batch 80 - loss 2.7418601512908936\n",
            "epoch 2 - batch 90 - loss 2.811220169067383\n",
            "epoch 2 - batch 100 - loss 2.732536554336548\n",
            "epoch 2 - batch 110 - loss 2.7823450565338135\n",
            "epoch 2 - batch 120 - loss 2.8103713989257812\n",
            "epoch 2 - batch 130 - loss 2.659917116165161\n",
            "epoch 2 - batch 140 - loss 2.679319381713867\n",
            "epoch 2 - batch 150 - loss 2.776893138885498\n",
            "epoch 2 - batch 160 - loss 2.651320219039917\n",
            "epoch 2 - batch 170 - loss 2.6882975101470947\n",
            "epoch 2 - batch 180 - loss 2.768831491470337\n",
            "epoch 2 - batch 190 - loss 2.700641632080078\n",
            "epoch 2 - batch 200 - loss 2.738805055618286\n",
            "epoch 2 - batch 210 - loss 2.7737433910369873\n",
            "epoch 2 - batch 220 - loss 2.7442867755889893\n",
            "epoch 2 - batch 230 - loss 2.6921675205230713\n",
            "epoch 2 - batch 240 - loss 2.6636695861816406\n",
            "epoch 2 - batch 250 - loss 2.6901049613952637\n",
            "epoch 2 - batch 260 - loss 2.6412692070007324\n",
            "epoch 2 - batch 270 - loss 2.7019832134246826\n",
            "epoch 2 - batch 280 - loss 2.6549251079559326\n",
            "epoch 2 - batch 290 - loss 2.6906330585479736\n",
            "epoch 2 - batch 300 - loss 2.8206381797790527\n",
            "epoch 2 - batch 310 - loss 2.6742591857910156\n",
            "epoch 2 - batch 320 - loss 2.672762632369995\n",
            "epoch 2 - batch 330 - loss 2.7281651496887207\n",
            "epoch 2 - batch 340 - loss 2.745272397994995\n",
            "epoch 2 - batch 350 - loss 2.7361133098602295\n",
            "epoch 2 - batch 360 - loss 2.679734706878662\n",
            "epoch 2 - batch 370 - loss 2.675407886505127\n",
            "epoch 2 - batch 380 - loss 2.587219476699829\n",
            "epoch 2 - batch 390 - loss 2.6654505729675293\n",
            "epoch 2 - batch 400 - loss 2.5553479194641113\n",
            "epoch 2 - batch 410 - loss 2.649834156036377\n",
            "epoch 2 - batch 420 - loss 2.726426362991333\n",
            "epoch 2 - batch 430 - loss 2.6956067085266113\n",
            "epoch 2 - batch 440 - loss 2.6035940647125244\n",
            "epoch 2 - batch 450 - loss 2.6127536296844482\n",
            "epoch 2 - batch 460 - loss 2.7014381885528564\n",
            "epoch 2 - batch 470 - loss 2.7663347721099854\n",
            "epoch 2 - batch 480 - loss 2.6549036502838135\n",
            "epoch 2 - batch 490 - loss 2.6199581623077393\n",
            "epoch 2 - batch 500 - loss 2.6379873752593994\n",
            "epoch 2 - batch 510 - loss 2.7379536628723145\n",
            "epoch 2 - batch 520 - loss 2.6122212409973145\n",
            "epoch 2 - batch 530 - loss 2.708474636077881\n",
            "epoch 2 - batch 540 - loss 2.623897075653076\n",
            "epoch 2 - batch 550 - loss 2.6258795261383057\n",
            "epoch 2 - batch 560 - loss 2.624689817428589\n",
            "epoch 2 - batch 570 - loss 2.679574728012085\n",
            "epoch 2 - batch 580 - loss 2.5939276218414307\n",
            "epoch 2 - batch 590 - loss 2.6924960613250732\n",
            "epoch 2 - batch 600 - loss 2.5855350494384766\n",
            "epoch 2 - batch 610 - loss 2.61012864112854\n",
            "epoch 2 training time: 222.0795557498932 sec\n",
            "evaluation of batch 0 took: 0.12394189834594727\n",
            "evaluation of batch 50 took: 0.12273287773132324\n",
            "evaluation of batch 100 took: 0.1216115951538086\n",
            "evaluation of batch 150 took: 0.12933802604675293\n",
            "evaluation of batch 200 took: 0.12084841728210449\n",
            "evaluation of batch 250 took: 0.1344912052154541\n",
            "evaluation of batch 300 took: 0.1219322681427002\n",
            "evaluation of batch 350 took: 0.12176179885864258\n",
            "evaluation of batch 400 took: 0.1259593963623047\n",
            "evaluation of batch 450 took: 0.1227424144744873\n",
            "evaluation of batch 500 took: 0.12084579467773438\n",
            "evaluation of batch 550 took: 0.12389492988586426\n",
            "evaluation of batch 600 took: 0.11675095558166504\n",
            "epoch 2 evaluation on training data time: 77.16631245613098 sec\n",
            "evaluation of batch 0 took: 0.1320185661315918\n",
            "evaluation of batch 50 took: 0.11987996101379395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 evaluation on test data time: 19.65424156188965 sec\n",
            "epoch evaluation:  {'epoch': 2, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.6220782>, 'test_rouge_1_p': 0.26453975944070823, 'test_rouge_1_r': 0.1315359329100958, 'test_rouge_1_f1': 0.16681850959004496, 'test_rouge_2_p': 0.004310825892857141, 'test_rouge_2_r': 0.002836258793290042, 'test_rouge_2_f1': 0.0032425110626206396, 'test_rouge_3_p': 6.341314935064934e-05, 'test_rouge_3_r': 1.7755681818181817e-05, 'test_rouge_3_f1': 2.7056277056277056e-05, 'test_rouge_L_p': 0.26443331594001257, 'test_rouge_L_r': 0.13146931890653982, 'test_rouge_L_f1': 0.16673919254390568}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 - batch 10 - loss 2.656694173812866\n",
            "epoch 3 - batch 20 - loss 2.5961644649505615\n",
            "epoch 3 - batch 30 - loss 2.5638442039489746\n",
            "epoch 3 - batch 40 - loss 2.579758405685425\n",
            "epoch 3 - batch 50 - loss 2.6256861686706543\n",
            "epoch 3 - batch 60 - loss 2.6878135204315186\n",
            "epoch 3 - batch 70 - loss 2.6034297943115234\n",
            "epoch 3 - batch 80 - loss 2.5561416149139404\n",
            "epoch 3 - batch 90 - loss 2.577531099319458\n",
            "epoch 3 - batch 100 - loss 2.5387370586395264\n",
            "epoch 3 - batch 110 - loss 2.621436834335327\n",
            "epoch 3 - batch 120 - loss 2.5972445011138916\n",
            "epoch 3 - batch 130 - loss 2.5599400997161865\n",
            "epoch 3 - batch 140 - loss 2.5711445808410645\n",
            "epoch 3 - batch 150 - loss 2.5836942195892334\n",
            "epoch 3 - batch 160 - loss 2.6407554149627686\n",
            "epoch 3 - batch 170 - loss 2.5841715335845947\n",
            "epoch 3 - batch 180 - loss 2.5866541862487793\n",
            "epoch 3 - batch 190 - loss 2.5754635334014893\n",
            "epoch 3 - batch 200 - loss 2.5504238605499268\n",
            "epoch 3 - batch 210 - loss 2.521181344985962\n",
            "epoch 3 - batch 220 - loss 2.558292865753174\n",
            "epoch 3 - batch 230 - loss 2.5597965717315674\n",
            "epoch 3 - batch 240 - loss 2.426612615585327\n",
            "epoch 3 - batch 250 - loss 2.6504054069519043\n",
            "epoch 3 - batch 260 - loss 2.54256010055542\n",
            "epoch 3 - batch 270 - loss 2.55077862739563\n",
            "epoch 3 - batch 280 - loss 2.4945695400238037\n",
            "epoch 3 - batch 290 - loss 2.6131253242492676\n",
            "epoch 3 - batch 300 - loss 2.498661518096924\n",
            "epoch 3 - batch 310 - loss 2.4914376735687256\n",
            "epoch 3 - batch 320 - loss 2.479008197784424\n",
            "epoch 3 - batch 330 - loss 2.5132250785827637\n",
            "epoch 3 - batch 340 - loss 2.524860382080078\n",
            "epoch 3 - batch 350 - loss 2.54481840133667\n",
            "epoch 3 - batch 360 - loss 2.527730703353882\n",
            "epoch 3 - batch 370 - loss 2.4862325191497803\n",
            "epoch 3 - batch 380 - loss 2.473287582397461\n",
            "epoch 3 - batch 390 - loss 2.4746785163879395\n",
            "epoch 3 - batch 400 - loss 2.469691514968872\n",
            "epoch 3 - batch 410 - loss 2.532832622528076\n",
            "epoch 3 - batch 420 - loss 2.561225414276123\n",
            "epoch 3 - batch 430 - loss 2.4275949001312256\n",
            "epoch 3 - batch 440 - loss 2.509530544281006\n",
            "epoch 3 - batch 450 - loss 2.436156988143921\n",
            "epoch 3 - batch 460 - loss 2.396524667739868\n",
            "epoch 3 - batch 470 - loss 2.455632448196411\n",
            "epoch 3 - batch 480 - loss 2.4859459400177\n",
            "epoch 3 - batch 490 - loss 2.426910400390625\n",
            "epoch 3 - batch 500 - loss 2.440290689468384\n",
            "epoch 3 - batch 510 - loss 2.438070774078369\n",
            "epoch 3 - batch 520 - loss 2.3866097927093506\n",
            "epoch 3 - batch 530 - loss 2.5014634132385254\n",
            "epoch 3 - batch 540 - loss 2.45428729057312\n",
            "epoch 3 - batch 550 - loss 2.3501498699188232\n",
            "epoch 3 - batch 560 - loss 2.390186071395874\n",
            "epoch 3 - batch 570 - loss 2.403413772583008\n",
            "epoch 3 - batch 580 - loss 2.444910764694214\n",
            "epoch 3 - batch 590 - loss 2.4277987480163574\n",
            "epoch 3 - batch 600 - loss 2.3454954624176025\n",
            "epoch 3 - batch 610 - loss 2.385399341583252\n",
            "epoch 3 training time: 220.61672568321228 sec\n",
            "evaluation of batch 0 took: 0.12289857864379883\n",
            "evaluation of batch 50 took: 0.12717962265014648\n",
            "evaluation of batch 100 took: 0.12138938903808594\n",
            "evaluation of batch 150 took: 0.11880683898925781\n",
            "evaluation of batch 200 took: 0.12231779098510742\n",
            "evaluation of batch 250 took: 0.12707209587097168\n",
            "evaluation of batch 300 took: 0.13389945030212402\n",
            "evaluation of batch 350 took: 0.14317989349365234\n",
            "evaluation of batch 400 took: 0.12866902351379395\n",
            "evaluation of batch 450 took: 0.13416361808776855\n",
            "evaluation of batch 500 took: 0.12909698486328125\n",
            "evaluation of batch 550 took: 0.1289055347442627\n",
            "evaluation of batch 600 took: 0.12590360641479492\n",
            "epoch 3 evaluation on training data time: 78.54027557373047 sec\n",
            "evaluation of batch 0 took: 0.1380162239074707\n",
            "evaluation of batch 50 took: 0.14347386360168457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 evaluation on test data time: 20.391706943511963 sec\n",
            "epoch evaluation:  {'epoch': 3, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.3986354>, 'test_rouge_1_p': 0.2667088817060917, 'test_rouge_1_r': 0.17542372062461353, 'test_rouge_1_f1': 0.2018100223076891, 'test_rouge_2_p': 0.030223341112012983, 'test_rouge_2_r': 0.026896053165584417, 'test_rouge_2_f1': 0.027802887737825844, 'test_rouge_3_p': 0.0017751454274891785, 'test_rouge_3_r': 0.0016330999729437235, 'test_rouge_3_f1': 0.001654449066558442, 'test_rouge_L_p': 0.26653262334763467, 'test_rouge_L_r': 0.1753132307610544, 'test_rouge_L_f1': 0.20167763340386655}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 saved checkpoint: models/checkpoints/baseline/ckpt-2\n",
            "epoch 4 - batch 10 - loss 2.4831624031066895\n",
            "epoch 4 - batch 20 - loss 2.486649990081787\n",
            "epoch 4 - batch 30 - loss 2.2113571166992188\n",
            "epoch 4 - batch 40 - loss 2.3289835453033447\n",
            "epoch 4 - batch 50 - loss 2.3829617500305176\n",
            "epoch 4 - batch 60 - loss 2.438739776611328\n",
            "epoch 4 - batch 70 - loss 2.429365634918213\n",
            "epoch 4 - batch 80 - loss 2.3589115142822266\n",
            "epoch 4 - batch 90 - loss 2.2790415287017822\n",
            "epoch 4 - batch 100 - loss 2.4807069301605225\n",
            "epoch 4 - batch 110 - loss 2.340177536010742\n",
            "epoch 4 - batch 120 - loss 2.3598339557647705\n",
            "epoch 4 - batch 130 - loss 2.2995970249176025\n",
            "epoch 4 - batch 140 - loss 2.4155433177948\n",
            "epoch 4 - batch 150 - loss 2.266570568084717\n",
            "epoch 4 - batch 160 - loss 2.329559326171875\n",
            "epoch 4 - batch 170 - loss 2.319685220718384\n",
            "epoch 4 - batch 180 - loss 2.4287028312683105\n",
            "epoch 4 - batch 190 - loss 2.407945156097412\n",
            "epoch 4 - batch 200 - loss 2.3949990272521973\n",
            "epoch 4 - batch 210 - loss 2.251565456390381\n",
            "epoch 4 - batch 220 - loss 2.2753946781158447\n",
            "epoch 4 - batch 230 - loss 2.3399770259857178\n",
            "epoch 4 - batch 240 - loss 2.2583770751953125\n",
            "epoch 4 - batch 250 - loss 2.2464873790740967\n",
            "epoch 4 - batch 260 - loss 2.29451584815979\n",
            "epoch 4 - batch 270 - loss 2.3613643646240234\n",
            "epoch 4 - batch 280 - loss 2.271822929382324\n",
            "epoch 4 - batch 290 - loss 2.2893338203430176\n",
            "epoch 4 - batch 300 - loss 2.2615599632263184\n",
            "epoch 4 - batch 310 - loss 2.2715752124786377\n",
            "epoch 4 - batch 320 - loss 2.353224515914917\n",
            "epoch 4 - batch 330 - loss 2.3053953647613525\n",
            "epoch 4 - batch 340 - loss 2.2788679599761963\n",
            "epoch 4 - batch 350 - loss 2.2554070949554443\n",
            "epoch 4 - batch 360 - loss 2.2225306034088135\n",
            "epoch 4 - batch 370 - loss 2.2587335109710693\n",
            "epoch 4 - batch 380 - loss 2.1596248149871826\n",
            "epoch 4 - batch 390 - loss 2.2480175495147705\n",
            "epoch 4 - batch 400 - loss 2.2766146659851074\n",
            "epoch 4 - batch 410 - loss 2.2304701805114746\n",
            "epoch 4 - batch 420 - loss 2.209024667739868\n",
            "epoch 4 - batch 430 - loss 2.1684162616729736\n",
            "epoch 4 - batch 440 - loss 2.2761495113372803\n",
            "epoch 4 - batch 450 - loss 2.177373170852661\n",
            "epoch 4 - batch 460 - loss 2.2090513706207275\n",
            "epoch 4 - batch 470 - loss 2.1147844791412354\n",
            "epoch 4 - batch 480 - loss 2.245206832885742\n",
            "epoch 4 - batch 490 - loss 2.1429827213287354\n",
            "epoch 4 - batch 500 - loss 2.168480157852173\n",
            "epoch 4 - batch 510 - loss 2.193227529525757\n",
            "epoch 4 - batch 520 - loss 2.336725950241089\n",
            "epoch 4 - batch 530 - loss 2.186063051223755\n",
            "epoch 4 - batch 540 - loss 2.114166259765625\n",
            "epoch 4 - batch 550 - loss 2.1877009868621826\n",
            "epoch 4 - batch 560 - loss 2.1828901767730713\n",
            "epoch 4 - batch 570 - loss 2.183042526245117\n",
            "epoch 4 - batch 580 - loss 2.1905014514923096\n",
            "epoch 4 - batch 590 - loss 2.1580305099487305\n",
            "epoch 4 - batch 600 - loss 2.154308795928955\n",
            "epoch 4 - batch 610 - loss 2.158505916595459\n",
            "epoch 4 training time: 223.05301189422607 sec\n",
            "evaluation of batch 0 took: 0.13641762733459473\n",
            "evaluation of batch 50 took: 0.1313774585723877\n",
            "evaluation of batch 100 took: 0.13166093826293945\n",
            "evaluation of batch 150 took: 0.13165545463562012\n",
            "evaluation of batch 200 took: 0.12836241722106934\n",
            "evaluation of batch 250 took: 0.13135123252868652\n",
            "evaluation of batch 300 took: 0.13596177101135254\n",
            "evaluation of batch 350 took: 0.12831735610961914\n",
            "evaluation of batch 400 took: 0.15120697021484375\n",
            "evaluation of batch 450 took: 0.1317744255065918\n",
            "evaluation of batch 500 took: 0.1338198184967041\n",
            "evaluation of batch 550 took: 0.13053488731384277\n",
            "evaluation of batch 600 took: 0.15656566619873047\n",
            "epoch 4 evaluation on training data time: 83.41543817520142 sec\n",
            "evaluation of batch 0 took: 0.14861059188842773\n",
            "evaluation of batch 50 took: 0.1369187831878662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 evaluation on test data time: 20.774030208587646 sec\n",
            "epoch evaluation:  {'epoch': 4, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.1844358>, 'test_rouge_1_p': 0.31231438065282935, 'test_rouge_1_r': 0.2243102159187538, 'test_rouge_1_f1': 0.2500262318146098, 'test_rouge_2_p': 0.05552497632575756, 'test_rouge_2_r': 0.04903633150703462, 'test_rouge_2_f1': 0.05081177524169101, 'test_rouge_3_p': 0.006419101731601729, 'test_rouge_3_r': 0.0057082403273809494, 'test_rouge_3_f1': 0.0058602809021335795, 'test_rouge_L_p': 0.3120278740047155, 'test_rouge_L_r': 0.22413438031462588, 'test_rouge_L_f1': 0.24981384598052153}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 - batch 10 - loss 2.12664532661438\n",
            "epoch 5 - batch 20 - loss 2.1810481548309326\n",
            "epoch 5 - batch 30 - loss 2.11553692817688\n",
            "epoch 5 - batch 40 - loss 2.098477602005005\n",
            "epoch 5 - batch 50 - loss 2.17517352104187\n",
            "epoch 5 - batch 60 - loss 2.165079355239868\n",
            "epoch 5 - batch 70 - loss 2.152127981185913\n",
            "epoch 5 - batch 80 - loss 2.1643168926239014\n",
            "epoch 5 - batch 90 - loss 2.1884894371032715\n",
            "epoch 5 - batch 100 - loss 2.1441149711608887\n",
            "epoch 5 - batch 110 - loss 2.1486804485321045\n",
            "epoch 5 - batch 120 - loss 2.232574701309204\n",
            "epoch 5 - batch 130 - loss 2.051133632659912\n",
            "epoch 5 - batch 140 - loss 2.1319656372070312\n",
            "epoch 5 - batch 150 - loss 2.156083583831787\n",
            "epoch 5 - batch 160 - loss 2.1506946086883545\n",
            "epoch 5 - batch 170 - loss 2.071988821029663\n",
            "epoch 5 - batch 180 - loss 2.082401990890503\n",
            "epoch 5 - batch 190 - loss 2.18689227104187\n",
            "epoch 5 - batch 200 - loss 2.1126134395599365\n",
            "epoch 5 - batch 210 - loss 2.116544008255005\n",
            "epoch 5 - batch 220 - loss 2.1058504581451416\n",
            "epoch 5 - batch 230 - loss 2.0213446617126465\n",
            "epoch 5 - batch 240 - loss 1.9837335348129272\n",
            "epoch 5 - batch 250 - loss 2.096595048904419\n",
            "epoch 5 - batch 260 - loss 2.0049355030059814\n",
            "epoch 5 - batch 270 - loss 2.130293130874634\n",
            "epoch 5 - batch 280 - loss 2.1197304725646973\n",
            "epoch 5 - batch 290 - loss 2.0836360454559326\n",
            "epoch 5 - batch 300 - loss 2.129101276397705\n",
            "epoch 5 - batch 310 - loss 2.144533634185791\n",
            "epoch 5 - batch 320 - loss 2.087385892868042\n",
            "epoch 5 - batch 330 - loss 2.0545341968536377\n",
            "epoch 5 - batch 340 - loss 2.0455875396728516\n",
            "epoch 5 - batch 350 - loss 2.042964458465576\n",
            "epoch 5 - batch 360 - loss 1.993648886680603\n",
            "epoch 5 - batch 370 - loss 2.0885708332061768\n",
            "epoch 5 - batch 380 - loss 2.0649805068969727\n",
            "epoch 5 - batch 390 - loss 2.1141207218170166\n",
            "epoch 5 - batch 400 - loss 2.0146403312683105\n",
            "epoch 5 - batch 410 - loss 2.1112420558929443\n",
            "epoch 5 - batch 420 - loss 1.9874082803726196\n",
            "epoch 5 - batch 430 - loss 1.9717260599136353\n",
            "epoch 5 - batch 440 - loss 2.0978293418884277\n",
            "epoch 5 - batch 450 - loss 2.0380845069885254\n",
            "epoch 5 - batch 460 - loss 2.0598528385162354\n",
            "epoch 5 - batch 470 - loss 2.0801050662994385\n",
            "epoch 5 - batch 480 - loss 2.0486278533935547\n",
            "epoch 5 - batch 490 - loss 1.9280534982681274\n",
            "epoch 5 - batch 500 - loss 1.9820642471313477\n",
            "epoch 5 - batch 510 - loss 2.0375478267669678\n",
            "epoch 5 - batch 520 - loss 1.9571858644485474\n",
            "epoch 5 - batch 530 - loss 1.9944387674331665\n",
            "epoch 5 - batch 540 - loss 2.0065150260925293\n",
            "epoch 5 - batch 550 - loss 2.0071234703063965\n",
            "epoch 5 - batch 560 - loss 2.003321409225464\n",
            "epoch 5 - batch 570 - loss 1.944852590560913\n",
            "epoch 5 - batch 580 - loss 2.0025174617767334\n",
            "epoch 5 - batch 590 - loss 2.004478693008423\n",
            "epoch 5 - batch 600 - loss 1.977072834968567\n",
            "epoch 5 - batch 610 - loss 2.011164426803589\n",
            "epoch 5 training time: 220.60644483566284 sec\n",
            "evaluation of batch 0 took: 0.1394360065460205\n",
            "evaluation of batch 50 took: 0.14009881019592285\n",
            "evaluation of batch 100 took: 0.136582612991333\n",
            "evaluation of batch 150 took: 0.14064264297485352\n",
            "evaluation of batch 200 took: 0.13542866706848145\n",
            "evaluation of batch 250 took: 0.13650941848754883\n",
            "evaluation of batch 300 took: 0.13705778121948242\n",
            "evaluation of batch 350 took: 0.1333308219909668\n",
            "evaluation of batch 400 took: 0.14468669891357422\n",
            "evaluation of batch 450 took: 0.13506460189819336\n",
            "evaluation of batch 500 took: 0.13594937324523926\n",
            "evaluation of batch 550 took: 0.13439154624938965\n",
            "evaluation of batch 600 took: 0.13834214210510254\n",
            "epoch 5 evaluation on training data time: 86.07609510421753 sec\n",
            "evaluation of batch 0 took: 0.15246820449829102\n",
            "evaluation of batch 50 took: 0.1374509334564209\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 5 evaluation on test data time: 21.111051559448242 sec\n",
            "epoch evaluation:  {'epoch': 5, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=2.0032144>, 'test_rouge_1_p': 0.3607671300054111, 'test_rouge_1_r': 0.27920631498337956, 'test_rouge_1_f1': 0.3035785252271403, 'test_rouge_2_p': 0.08632474296536792, 'test_rouge_2_r': 0.07673836580086581, 'test_rouge_2_f1': 0.07944720991659404, 'test_rouge_3_p': 0.013946665313852814, 'test_rouge_3_r': 0.012453285646645018, 'test_rouge_3_f1': 0.012839894480519483, 'test_rouge_L_p': 0.3600750811688309, 'test_rouge_L_r': 0.2787688850398114, 'test_rouge_L_f1': 0.3030532125702579}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 - batch 10 - loss 2.0647494792938232\n",
            "epoch 6 - batch 20 - loss 2.0875017642974854\n",
            "epoch 6 - batch 30 - loss 1.9549816846847534\n",
            "epoch 6 - batch 40 - loss 1.8992716073989868\n",
            "epoch 6 - batch 50 - loss 2.03450608253479\n",
            "epoch 6 - batch 60 - loss 1.9831187725067139\n",
            "epoch 6 - batch 70 - loss 2.0193779468536377\n",
            "epoch 6 - batch 80 - loss 1.9314823150634766\n",
            "epoch 6 - batch 90 - loss 1.9595266580581665\n",
            "epoch 6 - batch 100 - loss 2.0733489990234375\n",
            "epoch 6 - batch 110 - loss 1.9390345811843872\n",
            "epoch 6 - batch 120 - loss 1.8754891157150269\n",
            "epoch 6 - batch 130 - loss 1.9526342153549194\n",
            "epoch 6 - batch 140 - loss 1.9581118822097778\n",
            "epoch 6 - batch 150 - loss 2.017871379852295\n",
            "epoch 6 - batch 160 - loss 1.9270524978637695\n",
            "epoch 6 - batch 170 - loss 2.022871732711792\n",
            "epoch 6 - batch 180 - loss 1.9697858095169067\n",
            "epoch 6 - batch 190 - loss 1.9522230625152588\n",
            "epoch 6 - batch 200 - loss 1.8499810695648193\n",
            "epoch 6 - batch 210 - loss 1.9999738931655884\n",
            "epoch 6 - batch 220 - loss 1.9280550479888916\n",
            "epoch 6 - batch 230 - loss 1.9292242527008057\n",
            "epoch 6 - batch 240 - loss 1.8894833326339722\n",
            "epoch 6 - batch 250 - loss 1.9204021692276\n",
            "epoch 6 - batch 260 - loss 1.9023436307907104\n",
            "epoch 6 - batch 270 - loss 1.8528039455413818\n",
            "epoch 6 - batch 280 - loss 1.9458094835281372\n",
            "epoch 6 - batch 290 - loss 1.9375073909759521\n",
            "epoch 6 - batch 300 - loss 1.9669806957244873\n",
            "epoch 6 - batch 310 - loss 1.969704508781433\n",
            "epoch 6 - batch 320 - loss 1.9237116575241089\n",
            "epoch 6 - batch 330 - loss 1.9223616123199463\n",
            "epoch 6 - batch 340 - loss 1.9259980916976929\n",
            "epoch 6 - batch 350 - loss 1.9264202117919922\n",
            "epoch 6 - batch 360 - loss 1.8874359130859375\n",
            "epoch 6 - batch 370 - loss 1.8783323764801025\n",
            "epoch 6 - batch 380 - loss 1.8648490905761719\n",
            "epoch 6 - batch 390 - loss 1.893848180770874\n",
            "epoch 6 - batch 400 - loss 1.9128834009170532\n",
            "epoch 6 - batch 410 - loss 1.8331538438796997\n",
            "epoch 6 - batch 420 - loss 1.8893811702728271\n",
            "epoch 6 - batch 430 - loss 1.8545793294906616\n",
            "epoch 6 - batch 440 - loss 1.8774880170822144\n",
            "epoch 6 - batch 450 - loss 1.798222303390503\n",
            "epoch 6 - batch 460 - loss 1.8691308498382568\n",
            "epoch 6 - batch 470 - loss 1.8640096187591553\n",
            "epoch 6 - batch 480 - loss 1.9190419912338257\n",
            "epoch 6 - batch 490 - loss 1.8211686611175537\n",
            "epoch 6 - batch 500 - loss 1.8515994548797607\n",
            "epoch 6 - batch 510 - loss 1.9616111516952515\n",
            "epoch 6 - batch 520 - loss 1.9058434963226318\n",
            "epoch 6 - batch 530 - loss 1.9199445247650146\n",
            "epoch 6 - batch 540 - loss 1.8603761196136475\n",
            "epoch 6 - batch 550 - loss 1.8293039798736572\n",
            "epoch 6 - batch 560 - loss 1.8848789930343628\n",
            "epoch 6 - batch 570 - loss 1.82466721534729\n",
            "epoch 6 - batch 580 - loss 1.9079352617263794\n",
            "epoch 6 - batch 590 - loss 1.970687747001648\n",
            "epoch 6 - batch 600 - loss 1.8289835453033447\n",
            "epoch 6 - batch 610 - loss 1.8332959413528442\n",
            "epoch 6 training time: 219.87558507919312 sec\n",
            "evaluation of batch 0 took: 0.13651251792907715\n",
            "evaluation of batch 50 took: 0.1604170799255371\n",
            "evaluation of batch 100 took: 0.15120267868041992\n",
            "evaluation of batch 150 took: 0.13883566856384277\n",
            "evaluation of batch 200 took: 0.14135956764221191\n",
            "evaluation of batch 250 took: 0.14541268348693848\n",
            "evaluation of batch 300 took: 0.1424560546875\n",
            "evaluation of batch 350 took: 0.14757204055786133\n",
            "evaluation of batch 400 took: 0.1441347599029541\n",
            "evaluation of batch 450 took: 0.1378021240234375\n",
            "evaluation of batch 500 took: 0.14958405494689941\n",
            "evaluation of batch 550 took: 0.14543533325195312\n",
            "evaluation of batch 600 took: 0.1441338062286377\n",
            "epoch 6 evaluation on training data time: 88.45782279968262 sec\n",
            "evaluation of batch 0 took: 0.1613922119140625\n",
            "evaluation of batch 50 took: 0.14286494255065918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 evaluation on test data time: 21.402127027511597 sec\n",
            "epoch evaluation:  {'epoch': 6, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.8590795>, 'test_rouge_1_p': 0.3979529027520098, 'test_rouge_1_r': 0.3239263851847555, 'test_rouge_1_f1': 0.34638753518813886, 'test_rouge_2_p': 0.11606001420454541, 'test_rouge_2_r': 0.10488978794642856, 'test_rouge_2_f1': 0.10805066624618169, 'test_rouge_3_p': 0.02411876860119047, 'test_rouge_3_r': 0.021729783887987013, 'test_rouge_3_f1': 0.022375601519016682, 'test_rouge_L_p': 0.39688620298971855, 'test_rouge_L_r': 0.32323645011982044, 'test_rouge_L_f1': 0.34556750124424623}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6 saved checkpoint: models/checkpoints/baseline/ckpt-3\n",
            "epoch 7 - batch 10 - loss 1.9472792148590088\n",
            "epoch 7 - batch 20 - loss 1.8283274173736572\n",
            "epoch 7 - batch 30 - loss 1.788931965827942\n",
            "epoch 7 - batch 40 - loss 1.828764796257019\n",
            "epoch 7 - batch 50 - loss 1.8244102001190186\n",
            "epoch 7 - batch 60 - loss 1.82291579246521\n",
            "epoch 7 - batch 70 - loss 1.8459504842758179\n",
            "epoch 7 - batch 80 - loss 1.8036831617355347\n",
            "epoch 7 - batch 90 - loss 1.7753640413284302\n",
            "epoch 7 - batch 100 - loss 1.86374032497406\n",
            "epoch 7 - batch 110 - loss 1.8450106382369995\n",
            "epoch 7 - batch 120 - loss 1.8609575033187866\n",
            "epoch 7 - batch 130 - loss 1.814111351966858\n",
            "epoch 7 - batch 140 - loss 1.7023141384124756\n",
            "epoch 7 - batch 150 - loss 1.836013913154602\n",
            "epoch 7 - batch 160 - loss 1.7601238489151\n",
            "epoch 7 - batch 170 - loss 1.8419643640518188\n",
            "epoch 7 - batch 180 - loss 1.7534615993499756\n",
            "epoch 7 - batch 190 - loss 1.825987458229065\n",
            "epoch 7 - batch 200 - loss 1.8418753147125244\n",
            "epoch 7 - batch 210 - loss 1.849437952041626\n",
            "epoch 7 - batch 220 - loss 1.8017938137054443\n",
            "epoch 7 - batch 230 - loss 1.8103724718093872\n",
            "epoch 7 - batch 240 - loss 1.7699559926986694\n",
            "epoch 7 - batch 250 - loss 1.83883535861969\n",
            "epoch 7 - batch 260 - loss 1.7584635019302368\n",
            "epoch 7 - batch 270 - loss 1.750986099243164\n",
            "epoch 7 - batch 280 - loss 1.7537490129470825\n",
            "epoch 7 - batch 290 - loss 1.792451024055481\n",
            "epoch 7 - batch 300 - loss 1.781359076499939\n",
            "epoch 7 - batch 310 - loss 1.7830133438110352\n",
            "epoch 7 - batch 320 - loss 1.6959089040756226\n",
            "epoch 7 - batch 330 - loss 1.8403195142745972\n",
            "epoch 7 - batch 340 - loss 1.7559421062469482\n",
            "epoch 7 - batch 350 - loss 1.841787338256836\n",
            "epoch 7 - batch 360 - loss 1.8100897073745728\n",
            "epoch 7 - batch 370 - loss 1.814729928970337\n",
            "epoch 7 - batch 380 - loss 1.7635616064071655\n",
            "epoch 7 - batch 390 - loss 1.7931663990020752\n",
            "epoch 7 - batch 400 - loss 1.746756672859192\n",
            "epoch 7 - batch 410 - loss 1.8253470659255981\n",
            "epoch 7 - batch 420 - loss 1.800310730934143\n",
            "epoch 7 - batch 430 - loss 1.6834934949874878\n",
            "epoch 7 - batch 440 - loss 1.7476791143417358\n",
            "epoch 7 - batch 450 - loss 1.7056761980056763\n",
            "epoch 7 - batch 460 - loss 1.6379402875900269\n",
            "epoch 7 - batch 470 - loss 1.7186700105667114\n",
            "epoch 7 - batch 480 - loss 1.7269409894943237\n",
            "epoch 7 - batch 490 - loss 1.7505664825439453\n",
            "epoch 7 - batch 500 - loss 1.6980985403060913\n",
            "epoch 7 - batch 510 - loss 1.6999372243881226\n",
            "epoch 7 - batch 520 - loss 1.76107656955719\n",
            "epoch 7 - batch 530 - loss 1.7416349649429321\n",
            "epoch 7 - batch 540 - loss 1.793976068496704\n",
            "epoch 7 - batch 550 - loss 1.7462352514266968\n",
            "epoch 7 - batch 560 - loss 1.7089015245437622\n",
            "epoch 7 - batch 570 - loss 1.7892731428146362\n",
            "epoch 7 - batch 580 - loss 1.7319564819335938\n",
            "epoch 7 - batch 590 - loss 1.7899080514907837\n",
            "epoch 7 - batch 600 - loss 1.7649860382080078\n",
            "epoch 7 - batch 610 - loss 1.7779961824417114\n",
            "epoch 7 training time: 221.02406549453735 sec\n",
            "evaluation of batch 0 took: 0.1449744701385498\n",
            "evaluation of batch 50 took: 0.1444871425628662\n",
            "evaluation of batch 100 took: 0.1500546932220459\n",
            "evaluation of batch 150 took: 0.15542268753051758\n",
            "evaluation of batch 200 took: 0.14857864379882812\n",
            "evaluation of batch 250 took: 0.1465620994567871\n",
            "evaluation of batch 300 took: 0.14654231071472168\n",
            "evaluation of batch 350 took: 0.14706826210021973\n",
            "evaluation of batch 400 took: 0.14818191528320312\n",
            "evaluation of batch 450 took: 0.1434464454650879\n",
            "evaluation of batch 500 took: 0.13882756233215332\n",
            "evaluation of batch 550 took: 0.1445636749267578\n",
            "evaluation of batch 600 took: 0.14766860008239746\n",
            "epoch 7 evaluation on training data time: 90.85119199752808 sec\n",
            "evaluation of batch 0 took: 0.16169166564941406\n",
            "evaluation of batch 50 took: 0.14207792282104492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 7 evaluation on test data time: 21.686089277267456 sec\n",
            "epoch evaluation:  {'epoch': 7, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.7540041>, 'test_rouge_1_p': 0.42538431388180253, 'test_rouge_1_r': 0.3576005792942174, 'test_rouge_1_f1': 0.37792260224725205, 'test_rouge_2_p': 0.14273264170725108, 'test_rouge_2_r': 0.13075791396103897, 'test_rouge_2_f1': 0.1341164811525506, 'test_rouge_3_p': 0.03490872734036795, 'test_rouge_3_r': 0.031914147050865795, 'test_rouge_3_f1': 0.032719946998685835, 'test_rouge_L_p': 0.4239770855074982, 'test_rouge_L_r': 0.3566116361317251, 'test_rouge_L_f1': 0.37678989037488925}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8 - batch 10 - loss 1.7474123239517212\n",
            "epoch 8 - batch 20 - loss 1.6897251605987549\n",
            "epoch 8 - batch 30 - loss 1.7502647638320923\n",
            "epoch 8 - batch 40 - loss 1.705978274345398\n",
            "epoch 8 - batch 50 - loss 1.6573587656021118\n",
            "epoch 8 - batch 60 - loss 1.8349756002426147\n",
            "epoch 8 - batch 70 - loss 1.7538490295410156\n",
            "epoch 8 - batch 80 - loss 1.6576203107833862\n",
            "epoch 8 - batch 90 - loss 1.7692075967788696\n",
            "epoch 8 - batch 100 - loss 1.6696139574050903\n",
            "epoch 8 - batch 110 - loss 1.8286036252975464\n",
            "epoch 8 - batch 120 - loss 1.6902215480804443\n",
            "epoch 8 - batch 130 - loss 1.7151178121566772\n",
            "epoch 8 - batch 140 - loss 1.705649495124817\n",
            "epoch 8 - batch 150 - loss 1.7230020761489868\n",
            "epoch 8 - batch 160 - loss 1.6807674169540405\n",
            "epoch 8 - batch 170 - loss 1.6912108659744263\n",
            "epoch 8 - batch 180 - loss 1.6658172607421875\n",
            "epoch 8 - batch 190 - loss 1.69294011592865\n",
            "epoch 8 - batch 200 - loss 1.6868951320648193\n",
            "epoch 8 - batch 210 - loss 1.7238246202468872\n",
            "epoch 8 - batch 220 - loss 1.6979774236679077\n",
            "epoch 8 - batch 230 - loss 1.7229461669921875\n",
            "epoch 8 - batch 240 - loss 1.6619421243667603\n",
            "epoch 8 - batch 250 - loss 1.7711048126220703\n",
            "epoch 8 - batch 260 - loss 1.7007782459259033\n",
            "epoch 8 - batch 270 - loss 1.5924532413482666\n",
            "epoch 8 - batch 280 - loss 1.7832872867584229\n",
            "epoch 8 - batch 290 - loss 1.6563146114349365\n",
            "epoch 8 - batch 300 - loss 1.711235761642456\n",
            "epoch 8 - batch 310 - loss 1.7510201930999756\n",
            "epoch 8 - batch 320 - loss 1.6346514225006104\n",
            "epoch 8 - batch 330 - loss 1.6626521348953247\n",
            "epoch 8 - batch 340 - loss 1.687531590461731\n",
            "epoch 8 - batch 350 - loss 1.683044672012329\n",
            "epoch 8 - batch 360 - loss 1.669035792350769\n",
            "epoch 8 - batch 370 - loss 1.7331392765045166\n",
            "epoch 8 - batch 380 - loss 1.598335862159729\n",
            "epoch 8 - batch 390 - loss 1.6891648769378662\n",
            "epoch 8 - batch 400 - loss 1.6611849069595337\n",
            "epoch 8 - batch 410 - loss 1.663601040840149\n",
            "epoch 8 - batch 420 - loss 1.7032434940338135\n",
            "epoch 8 - batch 430 - loss 1.703268051147461\n",
            "epoch 8 - batch 440 - loss 1.6954222917556763\n",
            "epoch 8 - batch 450 - loss 1.5929616689682007\n",
            "epoch 8 - batch 460 - loss 1.633745789527893\n",
            "epoch 8 - batch 470 - loss 1.6647179126739502\n",
            "epoch 8 - batch 480 - loss 1.6574698686599731\n",
            "epoch 8 - batch 490 - loss 1.5922106504440308\n",
            "epoch 8 - batch 500 - loss 1.6350315809249878\n",
            "epoch 8 - batch 510 - loss 1.6622674465179443\n",
            "epoch 8 - batch 520 - loss 1.5250259637832642\n",
            "epoch 8 - batch 530 - loss 1.6157065629959106\n",
            "epoch 8 - batch 540 - loss 1.6187981367111206\n",
            "epoch 8 - batch 550 - loss 1.5624455213546753\n",
            "epoch 8 - batch 560 - loss 1.6528520584106445\n",
            "epoch 8 - batch 570 - loss 1.6338108777999878\n",
            "epoch 8 - batch 580 - loss 1.6446224451065063\n",
            "epoch 8 - batch 590 - loss 1.6510229110717773\n",
            "epoch 8 - batch 600 - loss 1.6988204717636108\n",
            "epoch 8 - batch 610 - loss 1.6553058624267578\n",
            "epoch 8 training time: 219.83564114570618 sec\n",
            "evaluation of batch 0 took: 0.1476597785949707\n",
            "evaluation of batch 50 took: 0.14640116691589355\n",
            "evaluation of batch 100 took: 0.14957833290100098\n",
            "evaluation of batch 150 took: 0.14524078369140625\n",
            "evaluation of batch 200 took: 0.1459958553314209\n",
            "evaluation of batch 250 took: 0.15777134895324707\n",
            "evaluation of batch 300 took: 0.14238667488098145\n",
            "evaluation of batch 350 took: 0.1454761028289795\n",
            "evaluation of batch 400 took: 0.15189695358276367\n",
            "evaluation of batch 450 took: 0.14339590072631836\n",
            "evaluation of batch 500 took: 0.14647841453552246\n",
            "evaluation of batch 550 took: 0.15298223495483398\n",
            "evaluation of batch 600 took: 0.15300726890563965\n",
            "epoch 8 evaluation on training data time: 92.23297643661499 sec\n",
            "evaluation of batch 0 took: 0.15719914436340332\n",
            "evaluation of batch 50 took: 0.14611458778381348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 8 evaluation on test data time: 21.939770698547363 sec\n",
            "epoch evaluation:  {'epoch': 8, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.6734524>, 'test_rouge_1_p': 0.44859600267663874, 'test_rouge_1_r': 0.38320067906423905, 'test_rouge_1_f1': 0.40279547081368305, 'test_rouge_2_p': 0.16426119453463198, 'test_rouge_2_r': 0.1510663977949134, 'test_rouge_2_f1': 0.15480373273405434, 'test_rouge_3_p': 0.0452025838744589, 'test_rouge_3_r': 0.041158093208874466, 'test_rouge_3_f1': 0.042281794346526486, 'test_rouge_L_p': 0.44693318911371355, 'test_rouge_L_r': 0.38201778225494726, 'test_rouge_L_f1': 0.4014472875496618}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 - batch 10 - loss 1.6413993835449219\n",
            "epoch 9 - batch 20 - loss 1.668987512588501\n",
            "epoch 9 - batch 30 - loss 1.6956901550292969\n",
            "epoch 9 - batch 40 - loss 1.6144123077392578\n",
            "epoch 9 - batch 50 - loss 1.6119827032089233\n",
            "epoch 9 - batch 60 - loss 1.6241663694381714\n",
            "epoch 9 - batch 70 - loss 1.6565409898757935\n",
            "epoch 9 - batch 80 - loss 1.6088831424713135\n",
            "epoch 9 - batch 90 - loss 1.6200664043426514\n",
            "epoch 9 - batch 100 - loss 1.6295642852783203\n",
            "epoch 9 - batch 110 - loss 1.659321904182434\n",
            "epoch 9 - batch 120 - loss 1.6260366439819336\n",
            "epoch 9 - batch 130 - loss 1.6095483303070068\n",
            "epoch 9 - batch 140 - loss 1.5873174667358398\n",
            "epoch 9 - batch 150 - loss 1.7701711654663086\n",
            "epoch 9 - batch 160 - loss 1.6154687404632568\n",
            "epoch 9 - batch 170 - loss 1.5866957902908325\n",
            "epoch 9 - batch 180 - loss 1.498767614364624\n",
            "epoch 9 - batch 190 - loss 1.6327584981918335\n",
            "epoch 9 - batch 200 - loss 1.5433968305587769\n",
            "epoch 9 - batch 210 - loss 1.6094783544540405\n",
            "epoch 9 - batch 220 - loss 1.6146706342697144\n",
            "epoch 9 - batch 230 - loss 1.5518494844436646\n",
            "epoch 9 - batch 240 - loss 1.6015781164169312\n",
            "epoch 9 - batch 250 - loss 1.6489944458007812\n",
            "epoch 9 - batch 260 - loss 1.5525940656661987\n",
            "epoch 9 - batch 270 - loss 1.625001072883606\n",
            "epoch 9 - batch 280 - loss 1.605254888534546\n",
            "epoch 9 - batch 290 - loss 1.6335901021957397\n",
            "epoch 9 - batch 300 - loss 1.6875237226486206\n",
            "epoch 9 - batch 310 - loss 1.5705955028533936\n",
            "epoch 9 - batch 320 - loss 1.6593637466430664\n",
            "epoch 9 - batch 330 - loss 1.4881246089935303\n",
            "epoch 9 - batch 340 - loss 1.592332124710083\n",
            "epoch 9 - batch 350 - loss 1.5845948457717896\n",
            "epoch 9 - batch 360 - loss 1.5904704332351685\n",
            "epoch 9 - batch 370 - loss 1.6031097173690796\n",
            "epoch 9 - batch 380 - loss 1.6387931108474731\n",
            "epoch 9 - batch 390 - loss 1.6021039485931396\n",
            "epoch 9 - batch 400 - loss 1.5998831987380981\n",
            "epoch 9 - batch 410 - loss 1.5939092636108398\n",
            "epoch 9 - batch 420 - loss 1.566264033317566\n",
            "epoch 9 - batch 430 - loss 1.5846933126449585\n",
            "epoch 9 - batch 440 - loss 1.5637438297271729\n",
            "epoch 9 - batch 450 - loss 1.60894775390625\n",
            "epoch 9 - batch 460 - loss 1.5423529148101807\n",
            "epoch 9 - batch 470 - loss 1.6149107217788696\n",
            "epoch 9 - batch 480 - loss 1.5899595022201538\n",
            "epoch 9 - batch 490 - loss 1.6100962162017822\n",
            "epoch 9 - batch 500 - loss 1.5848877429962158\n",
            "epoch 9 - batch 510 - loss 1.5600560903549194\n",
            "epoch 9 - batch 520 - loss 1.5649627447128296\n",
            "epoch 9 - batch 530 - loss 1.664926528930664\n",
            "epoch 9 - batch 540 - loss 1.5940148830413818\n",
            "epoch 9 - batch 550 - loss 1.5419842004776\n",
            "epoch 9 - batch 560 - loss 1.585712194442749\n",
            "epoch 9 - batch 570 - loss 1.518315315246582\n",
            "epoch 9 - batch 580 - loss 1.5992227792739868\n",
            "epoch 9 - batch 590 - loss 1.5746222734451294\n",
            "epoch 9 - batch 600 - loss 1.5202585458755493\n",
            "epoch 9 - batch 610 - loss 1.6788533926010132\n",
            "epoch 9 training time: 221.5629301071167 sec\n",
            "evaluation of batch 0 took: 0.15275025367736816\n",
            "evaluation of batch 50 took: 0.14726781845092773\n",
            "evaluation of batch 100 took: 0.16407346725463867\n",
            "evaluation of batch 150 took: 0.1475062370300293\n",
            "evaluation of batch 200 took: 0.1570136547088623\n",
            "evaluation of batch 250 took: 0.1561737060546875\n",
            "evaluation of batch 300 took: 0.14761567115783691\n",
            "evaluation of batch 350 took: 0.14766883850097656\n",
            "evaluation of batch 400 took: 0.14742684364318848\n",
            "evaluation of batch 450 took: 0.15174150466918945\n",
            "evaluation of batch 500 took: 0.1497056484222412\n",
            "evaluation of batch 550 took: 0.14491724967956543\n",
            "evaluation of batch 600 took: 0.15257501602172852\n",
            "epoch 9 evaluation on training data time: 93.35366487503052 sec\n",
            "evaluation of batch 0 took: 0.1574993133544922\n",
            "evaluation of batch 50 took: 0.14345693588256836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 evaluation on test data time: 21.85681915283203 sec\n",
            "epoch evaluation:  {'epoch': 9, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.6118608>, 'test_rouge_1_p': 0.4700130087546381, 'test_rouge_1_r': 0.40408256875193227, 'test_rouge_1_f1': 0.42390291844811906, 'test_rouge_2_p': 0.1836476511769481, 'test_rouge_2_r': 0.16871935031114718, 'test_rouge_2_f1': 0.17296446444237132, 'test_rouge_3_p': 0.05523179619859305, 'test_rouge_3_r': 0.050032763460497835, 'test_rouge_3_f1': 0.051494476815347336, 'test_rouge_L_p': 0.4681838413632498, 'test_rouge_L_r': 0.40283287675865787, 'test_rouge_L_f1': 0.42245234825262196}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 9 saved checkpoint: models/checkpoints/baseline/ckpt-4\n",
            "epoch 10 - batch 10 - loss 1.5961015224456787\n",
            "epoch 10 - batch 20 - loss 1.6421016454696655\n",
            "epoch 10 - batch 30 - loss 1.5606803894042969\n",
            "epoch 10 - batch 40 - loss 1.5814260244369507\n",
            "epoch 10 - batch 50 - loss 1.6139395236968994\n",
            "epoch 10 - batch 60 - loss 1.5710819959640503\n",
            "epoch 10 - batch 70 - loss 1.5279358625411987\n",
            "epoch 10 - batch 80 - loss 1.556713342666626\n",
            "epoch 10 - batch 90 - loss 1.5573618412017822\n",
            "epoch 10 - batch 100 - loss 1.6474195718765259\n",
            "epoch 10 - batch 110 - loss 1.668014407157898\n",
            "epoch 10 - batch 120 - loss 1.5969961881637573\n",
            "epoch 10 - batch 130 - loss 1.5299131870269775\n",
            "epoch 10 - batch 140 - loss 1.5887595415115356\n",
            "epoch 10 - batch 150 - loss 1.5573501586914062\n",
            "epoch 10 - batch 160 - loss 1.5480194091796875\n",
            "epoch 10 - batch 170 - loss 1.586846113204956\n",
            "epoch 10 - batch 180 - loss 1.5971463918685913\n",
            "epoch 10 - batch 190 - loss 1.4340076446533203\n",
            "epoch 10 - batch 200 - loss 1.5861815214157104\n",
            "epoch 10 - batch 210 - loss 1.5528229475021362\n",
            "epoch 10 - batch 220 - loss 1.6529585123062134\n",
            "epoch 10 - batch 230 - loss 1.5833746194839478\n",
            "epoch 10 - batch 240 - loss 1.5402761697769165\n",
            "epoch 10 - batch 250 - loss 1.5552338361740112\n",
            "epoch 10 - batch 260 - loss 1.555420994758606\n",
            "epoch 10 - batch 270 - loss 1.5246578454971313\n",
            "epoch 10 - batch 280 - loss 1.515086054801941\n",
            "epoch 10 - batch 290 - loss 1.508072853088379\n",
            "epoch 10 - batch 300 - loss 1.554087519645691\n",
            "epoch 10 - batch 310 - loss 1.5077168941497803\n",
            "epoch 10 - batch 320 - loss 1.5490256547927856\n",
            "epoch 10 - batch 330 - loss 1.5402783155441284\n",
            "epoch 10 - batch 340 - loss 1.4980213642120361\n",
            "epoch 10 - batch 350 - loss 1.527413010597229\n",
            "epoch 10 - batch 360 - loss 1.5923155546188354\n",
            "epoch 10 - batch 370 - loss 1.5035561323165894\n",
            "epoch 10 - batch 380 - loss 1.5044809579849243\n",
            "epoch 10 - batch 390 - loss 1.5122549533843994\n",
            "epoch 10 - batch 400 - loss 1.5380706787109375\n",
            "epoch 10 - batch 410 - loss 1.4676200151443481\n",
            "epoch 10 - batch 420 - loss 1.5548094511032104\n",
            "epoch 10 - batch 430 - loss 1.4904792308807373\n",
            "epoch 10 - batch 440 - loss 1.5496978759765625\n",
            "epoch 10 - batch 450 - loss 1.5304181575775146\n",
            "epoch 10 - batch 460 - loss 1.5460376739501953\n",
            "epoch 10 - batch 470 - loss 1.459375023841858\n",
            "epoch 10 - batch 480 - loss 1.4794490337371826\n",
            "epoch 10 - batch 490 - loss 1.6416571140289307\n",
            "epoch 10 - batch 500 - loss 1.5358989238739014\n",
            "epoch 10 - batch 510 - loss 1.5272676944732666\n",
            "epoch 10 - batch 520 - loss 1.5688207149505615\n",
            "epoch 10 - batch 530 - loss 1.5841282606124878\n",
            "epoch 10 - batch 540 - loss 1.5129048824310303\n",
            "epoch 10 - batch 550 - loss 1.519052505493164\n",
            "epoch 10 - batch 560 - loss 1.5020008087158203\n",
            "epoch 10 - batch 570 - loss 1.500614047050476\n",
            "epoch 10 - batch 580 - loss 1.5528773069381714\n",
            "epoch 10 - batch 590 - loss 1.5374038219451904\n",
            "epoch 10 - batch 600 - loss 1.4595140218734741\n",
            "epoch 10 - batch 610 - loss 1.5001577138900757\n",
            "epoch 10 training time: 221.6218032836914 sec\n",
            "evaluation of batch 0 took: 0.14902186393737793\n",
            "evaluation of batch 50 took: 0.15335679054260254\n",
            "evaluation of batch 100 took: 0.14988970756530762\n",
            "evaluation of batch 150 took: 0.16932988166809082\n",
            "evaluation of batch 200 took: 0.15345025062561035\n",
            "evaluation of batch 250 took: 0.14778828620910645\n",
            "evaluation of batch 300 took: 0.15526509284973145\n",
            "evaluation of batch 350 took: 0.15438413619995117\n",
            "evaluation of batch 400 took: 0.1558690071105957\n",
            "evaluation of batch 450 took: 0.14958429336547852\n",
            "evaluation of batch 500 took: 0.1465137004852295\n",
            "evaluation of batch 550 took: 0.15305233001708984\n",
            "evaluation of batch 600 took: 0.1569530963897705\n",
            "epoch 10 evaluation on training data time: 94.87494373321533 sec\n",
            "evaluation of batch 0 took: 0.1625373363494873\n",
            "evaluation of batch 50 took: 0.165907621383667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 10 evaluation on test data time: 22.12309432029724 sec\n",
            "epoch evaluation:  {'epoch': 10, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.5625168>, 'test_rouge_1_p': 0.4824582318722942, 'test_rouge_1_r': 0.4194859850900585, 'test_rouge_1_f1': 0.4381676454379627, 'test_rouge_2_p': 0.19931513798701303, 'test_rouge_2_r': 0.18416637073863631, 'test_rouge_2_f1': 0.1884396748971628, 'test_rouge_3_p': 0.06548908448322512, 'test_rouge_3_r': 0.05990957284902596, 'test_rouge_3_f1': 0.061467996289424835, 'test_rouge_L_p': 0.48059358331400737, 'test_rouge_L_r': 0.4181931721552255, 'test_rouge_L_f1': 0.4366751920152131}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11 - batch 10 - loss 1.5398139953613281\n",
            "epoch 11 - batch 20 - loss 1.5438823699951172\n",
            "epoch 11 - batch 30 - loss 1.4982645511627197\n",
            "epoch 11 - batch 40 - loss 1.5348039865493774\n",
            "epoch 11 - batch 50 - loss 1.4279192686080933\n",
            "epoch 11 - batch 60 - loss 1.5421675443649292\n",
            "epoch 11 - batch 70 - loss 1.4925124645233154\n",
            "epoch 11 - batch 80 - loss 1.5515611171722412\n",
            "epoch 11 - batch 90 - loss 1.5267927646636963\n",
            "epoch 11 - batch 100 - loss 1.5400810241699219\n",
            "epoch 11 - batch 110 - loss 1.4728237390518188\n",
            "epoch 11 - batch 120 - loss 1.5618330240249634\n",
            "epoch 11 - batch 130 - loss 1.4312026500701904\n",
            "epoch 11 - batch 140 - loss 1.5086431503295898\n",
            "epoch 11 - batch 150 - loss 1.512807011604309\n",
            "epoch 11 - batch 160 - loss 1.5298737287521362\n",
            "epoch 11 - batch 170 - loss 1.4918161630630493\n",
            "epoch 11 - batch 180 - loss 1.5485295057296753\n",
            "epoch 11 - batch 190 - loss 1.491958498954773\n",
            "epoch 11 - batch 200 - loss 1.5000686645507812\n",
            "epoch 11 - batch 210 - loss 1.5340813398361206\n",
            "epoch 11 - batch 220 - loss 1.5205252170562744\n",
            "epoch 11 - batch 230 - loss 1.435977578163147\n",
            "epoch 11 - batch 240 - loss 1.4717276096343994\n",
            "epoch 11 - batch 250 - loss 1.493544340133667\n",
            "epoch 11 - batch 260 - loss 1.4927139282226562\n",
            "epoch 11 - batch 270 - loss 1.4934536218643188\n",
            "epoch 11 - batch 280 - loss 1.5152168273925781\n",
            "epoch 11 - batch 290 - loss 1.5322682857513428\n",
            "epoch 11 - batch 300 - loss 1.487547516822815\n",
            "epoch 11 - batch 310 - loss 1.4783693552017212\n",
            "epoch 11 - batch 320 - loss 1.4878904819488525\n",
            "epoch 11 - batch 330 - loss 1.4187812805175781\n",
            "epoch 11 - batch 340 - loss 1.474548101425171\n",
            "epoch 11 - batch 350 - loss 1.5702507495880127\n",
            "epoch 11 - batch 360 - loss 1.440778136253357\n",
            "epoch 11 - batch 370 - loss 1.494947075843811\n",
            "epoch 11 - batch 380 - loss 1.4548733234405518\n",
            "epoch 11 - batch 390 - loss 1.45846688747406\n",
            "epoch 11 - batch 400 - loss 1.4273782968521118\n",
            "epoch 11 - batch 410 - loss 1.4657741785049438\n",
            "epoch 11 - batch 420 - loss 1.4984956979751587\n",
            "epoch 11 - batch 430 - loss 1.4182788133621216\n",
            "epoch 11 - batch 440 - loss 1.4661033153533936\n",
            "epoch 11 - batch 450 - loss 1.443158745765686\n",
            "epoch 11 - batch 460 - loss 1.4618290662765503\n",
            "epoch 11 - batch 470 - loss 1.4648877382278442\n",
            "epoch 11 - batch 480 - loss 1.5095010995864868\n",
            "epoch 11 - batch 490 - loss 1.429396390914917\n",
            "epoch 11 - batch 500 - loss 1.3910917043685913\n",
            "epoch 11 - batch 510 - loss 1.4745614528656006\n",
            "epoch 11 - batch 520 - loss 1.468366265296936\n",
            "epoch 11 - batch 530 - loss 1.4998918771743774\n",
            "epoch 11 - batch 540 - loss 1.4737757444381714\n",
            "epoch 11 - batch 550 - loss 1.4819291830062866\n",
            "epoch 11 - batch 560 - loss 1.4883633852005005\n",
            "epoch 11 - batch 570 - loss 1.417705774307251\n",
            "epoch 11 - batch 580 - loss 1.4431923627853394\n",
            "epoch 11 - batch 590 - loss 1.4405462741851807\n",
            "epoch 11 - batch 600 - loss 1.4223928451538086\n",
            "epoch 11 - batch 610 - loss 1.4426615238189697\n",
            "epoch 11 training time: 220.47585487365723 sec\n",
            "evaluation of batch 0 took: 0.14800071716308594\n",
            "evaluation of batch 50 took: 0.1546790599822998\n",
            "evaluation of batch 100 took: 0.15338993072509766\n",
            "evaluation of batch 150 took: 0.1575331687927246\n",
            "evaluation of batch 200 took: 0.14977145195007324\n",
            "evaluation of batch 250 took: 0.14525651931762695\n",
            "evaluation of batch 300 took: 0.15803194046020508\n",
            "evaluation of batch 350 took: 0.14604759216308594\n",
            "evaluation of batch 400 took: 0.15117383003234863\n",
            "evaluation of batch 450 took: 0.14875245094299316\n",
            "evaluation of batch 500 took: 0.14950108528137207\n",
            "evaluation of batch 550 took: 0.14814257621765137\n",
            "evaluation of batch 600 took: 0.14733648300170898\n",
            "epoch 11 evaluation on training data time: 93.95392441749573 sec\n",
            "evaluation of batch 0 took: 0.16467595100402832\n",
            "evaluation of batch 50 took: 0.15079689025878906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11 evaluation on test data time: 22.202613830566406 sec\n",
            "epoch evaluation:  {'epoch': 11, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.5201755>, 'test_rouge_1_p': 0.4958772997835497, 'test_rouge_1_r': 0.43342171918483297, 'test_rouge_1_f1': 0.45188807207628234, 'test_rouge_2_p': 0.21280311485389614, 'test_rouge_2_r': 0.19718720407196969, 'test_rouge_2_f1': 0.2015121062198985, 'test_rouge_3_p': 0.0728420505275974, 'test_rouge_3_r': 0.06708646171536793, 'test_rouge_3_f1': 0.06866421106794988, 'test_rouge_L_p': 0.49405495685490086, 'test_rouge_L_r': 0.4321590425943102, 'test_rouge_L_f1': 0.45043024051559455}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 - batch 10 - loss 1.4920419454574585\n",
            "epoch 12 - batch 20 - loss 1.4398648738861084\n",
            "epoch 12 - batch 30 - loss 1.4338592290878296\n",
            "epoch 12 - batch 40 - loss 1.3854318857192993\n",
            "epoch 12 - batch 50 - loss 1.4488667249679565\n",
            "epoch 12 - batch 60 - loss 1.4849557876586914\n",
            "epoch 12 - batch 70 - loss 1.4330236911773682\n",
            "epoch 12 - batch 80 - loss 1.508362054824829\n",
            "epoch 12 - batch 90 - loss 1.46840500831604\n",
            "epoch 12 - batch 100 - loss 1.4552891254425049\n",
            "epoch 12 - batch 110 - loss 1.4407685995101929\n",
            "epoch 12 - batch 120 - loss 1.452716588973999\n",
            "epoch 12 - batch 130 - loss 1.4138457775115967\n",
            "epoch 12 - batch 140 - loss 1.4166017770767212\n",
            "epoch 12 - batch 150 - loss 1.5135257244110107\n",
            "epoch 12 - batch 160 - loss 1.4711873531341553\n",
            "epoch 12 - batch 170 - loss 1.428000569343567\n",
            "epoch 12 - batch 180 - loss 1.4084374904632568\n",
            "epoch 12 - batch 190 - loss 1.4365206956863403\n",
            "epoch 12 - batch 200 - loss 1.4915580749511719\n",
            "epoch 12 - batch 210 - loss 1.4234931468963623\n",
            "epoch 12 - batch 220 - loss 1.4622217416763306\n",
            "epoch 12 - batch 230 - loss 1.4388024806976318\n",
            "epoch 12 - batch 240 - loss 1.4212684631347656\n",
            "epoch 12 - batch 250 - loss 1.4766302108764648\n",
            "epoch 12 - batch 260 - loss 1.3455978631973267\n",
            "epoch 12 - batch 270 - loss 1.5268791913986206\n",
            "epoch 12 - batch 280 - loss 1.4797859191894531\n",
            "epoch 12 - batch 290 - loss 1.450388789176941\n",
            "epoch 12 - batch 300 - loss 1.3630321025848389\n",
            "epoch 12 - batch 310 - loss 1.412240982055664\n",
            "epoch 12 - batch 320 - loss 1.4416061639785767\n",
            "epoch 12 - batch 330 - loss 1.4850112199783325\n",
            "epoch 12 - batch 340 - loss 1.446361780166626\n",
            "epoch 12 - batch 350 - loss 1.4250593185424805\n",
            "epoch 12 - batch 360 - loss 1.482322335243225\n",
            "epoch 12 - batch 370 - loss 1.4230047464370728\n",
            "epoch 12 - batch 380 - loss 1.3672752380371094\n",
            "epoch 12 - batch 390 - loss 1.379414439201355\n",
            "epoch 12 - batch 400 - loss 1.389410138130188\n",
            "epoch 12 - batch 410 - loss 1.4485777616500854\n",
            "epoch 12 - batch 420 - loss 1.4336774349212646\n",
            "epoch 12 - batch 430 - loss 1.389866590499878\n",
            "epoch 12 - batch 440 - loss 1.4153120517730713\n",
            "epoch 12 - batch 450 - loss 1.4512064456939697\n",
            "epoch 12 - batch 460 - loss 1.3695772886276245\n",
            "epoch 12 - batch 470 - loss 1.4336985349655151\n",
            "epoch 12 - batch 480 - loss 1.479765772819519\n",
            "epoch 12 - batch 490 - loss 1.3680182695388794\n",
            "epoch 12 - batch 500 - loss 1.4161274433135986\n",
            "epoch 12 - batch 510 - loss 1.4332106113433838\n",
            "epoch 12 - batch 520 - loss 1.4263858795166016\n",
            "epoch 12 - batch 530 - loss 1.4133408069610596\n",
            "epoch 12 - batch 540 - loss 1.4482176303863525\n",
            "epoch 12 - batch 550 - loss 1.3152083158493042\n",
            "epoch 12 - batch 560 - loss 1.4050761461257935\n",
            "epoch 12 - batch 570 - loss 1.4244474172592163\n",
            "epoch 12 - batch 580 - loss 1.4799766540527344\n",
            "epoch 12 - batch 590 - loss 1.4065272808074951\n",
            "epoch 12 - batch 600 - loss 1.4095815420150757\n",
            "epoch 12 - batch 610 - loss 1.4527405500411987\n",
            "epoch 12 training time: 221.49246048927307 sec\n",
            "evaluation of batch 0 took: 0.1574249267578125\n",
            "evaluation of batch 50 took: 0.16118621826171875\n",
            "evaluation of batch 100 took: 0.14913463592529297\n",
            "evaluation of batch 150 took: 0.1533949375152588\n",
            "evaluation of batch 200 took: 0.16126799583435059\n",
            "evaluation of batch 250 took: 0.15815114974975586\n",
            "evaluation of batch 300 took: 0.16087865829467773\n",
            "evaluation of batch 350 took: 0.1480088233947754\n",
            "evaluation of batch 400 took: 0.153580904006958\n",
            "evaluation of batch 450 took: 0.14620494842529297\n",
            "evaluation of batch 500 took: 0.14744210243225098\n",
            "evaluation of batch 550 took: 0.15553808212280273\n",
            "evaluation of batch 600 took: 0.15043067932128906\n",
            "epoch 12 evaluation on training data time: 96.36714434623718 sec\n",
            "evaluation of batch 0 took: 0.16592073440551758\n",
            "evaluation of batch 50 took: 0.14791440963745117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 evaluation on test data time: 22.17828917503357 sec\n",
            "epoch evaluation:  {'epoch': 12, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.486303>, 'test_rouge_1_p': 0.5050337055987167, 'test_rouge_1_r': 0.44357099373840425, 'test_rouge_1_f1': 0.46171185458548475, 'test_rouge_2_p': 0.22368100649350653, 'test_rouge_2_r': 0.20763917072510832, 'test_rouge_2_f1': 0.21215966171894035, 'test_rouge_3_p': 0.07994347774621212, 'test_rouge_3_r': 0.07397228422619048, 'test_rouge_3_f1': 0.07564950163303957, 'test_rouge_L_p': 0.503201880894403, 'test_rouge_L_r': 0.4422792376893934, 'test_rouge_L_f1': 0.46023237259730243}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 12 saved checkpoint: models/checkpoints/baseline/ckpt-5\n",
            "epoch 13 - batch 10 - loss 1.4568617343902588\n",
            "epoch 13 - batch 20 - loss 1.3540518283843994\n",
            "epoch 13 - batch 30 - loss 1.4113914966583252\n",
            "epoch 13 - batch 40 - loss 1.424275279045105\n",
            "epoch 13 - batch 50 - loss 1.3847646713256836\n",
            "epoch 13 - batch 60 - loss 1.4234552383422852\n",
            "epoch 13 - batch 70 - loss 1.3491344451904297\n",
            "epoch 13 - batch 80 - loss 1.412907600402832\n",
            "epoch 13 - batch 90 - loss 1.3847748041152954\n",
            "epoch 13 - batch 100 - loss 1.4105606079101562\n",
            "epoch 13 - batch 110 - loss 1.424458622932434\n",
            "epoch 13 - batch 120 - loss 1.4116970300674438\n",
            "epoch 13 - batch 130 - loss 1.4377187490463257\n",
            "epoch 13 - batch 140 - loss 1.4658159017562866\n",
            "epoch 13 - batch 150 - loss 1.4075639247894287\n",
            "epoch 13 - batch 160 - loss 1.3551183938980103\n",
            "epoch 13 - batch 170 - loss 1.3521605730056763\n",
            "epoch 13 - batch 180 - loss 1.3864024877548218\n",
            "epoch 13 - batch 190 - loss 1.324575424194336\n",
            "epoch 13 - batch 200 - loss 1.4006788730621338\n",
            "epoch 13 - batch 210 - loss 1.4069255590438843\n",
            "epoch 13 - batch 220 - loss 1.3877772092819214\n",
            "epoch 13 - batch 230 - loss 1.408687710762024\n",
            "epoch 13 - batch 240 - loss 1.3537894487380981\n",
            "epoch 13 - batch 250 - loss 1.435260534286499\n",
            "epoch 13 - batch 260 - loss 1.4140758514404297\n",
            "epoch 13 - batch 270 - loss 1.3178846836090088\n",
            "epoch 13 - batch 280 - loss 1.468993067741394\n",
            "epoch 13 - batch 290 - loss 1.3904975652694702\n",
            "epoch 13 - batch 300 - loss 1.38935387134552\n",
            "epoch 13 - batch 310 - loss 1.393724799156189\n",
            "epoch 13 - batch 320 - loss 1.421727180480957\n",
            "epoch 13 - batch 330 - loss 1.4007495641708374\n",
            "epoch 13 - batch 340 - loss 1.4726524353027344\n",
            "epoch 13 - batch 350 - loss 1.385109305381775\n",
            "epoch 13 - batch 360 - loss 1.4401130676269531\n",
            "epoch 13 - batch 370 - loss 1.3459415435791016\n",
            "epoch 13 - batch 380 - loss 1.3520185947418213\n",
            "epoch 13 - batch 390 - loss 1.4220631122589111\n",
            "epoch 13 - batch 400 - loss 1.3935985565185547\n",
            "epoch 13 - batch 410 - loss 1.337683081626892\n",
            "epoch 13 - batch 420 - loss 1.3396167755126953\n",
            "epoch 13 - batch 430 - loss 1.4124563932418823\n",
            "epoch 13 - batch 440 - loss 1.4322550296783447\n",
            "epoch 13 - batch 450 - loss 1.3782252073287964\n",
            "epoch 13 - batch 460 - loss 1.396026849746704\n",
            "epoch 13 - batch 470 - loss 1.4536666870117188\n",
            "epoch 13 - batch 480 - loss 1.402970314025879\n",
            "epoch 13 - batch 490 - loss 1.3661954402923584\n",
            "epoch 13 - batch 500 - loss 1.3800458908081055\n",
            "epoch 13 - batch 510 - loss 1.4498969316482544\n",
            "epoch 13 - batch 520 - loss 1.4149000644683838\n",
            "epoch 13 - batch 530 - loss 1.3500316143035889\n",
            "epoch 13 - batch 540 - loss 1.3973922729492188\n",
            "epoch 13 - batch 550 - loss 1.3917800188064575\n",
            "epoch 13 - batch 560 - loss 1.4398143291473389\n",
            "epoch 13 - batch 570 - loss 1.3031034469604492\n",
            "epoch 13 - batch 580 - loss 1.396323323249817\n",
            "epoch 13 - batch 590 - loss 1.3234527111053467\n",
            "epoch 13 - batch 600 - loss 1.3566328287124634\n",
            "epoch 13 - batch 610 - loss 1.3687463998794556\n",
            "epoch 13 training time: 220.19109773635864 sec\n",
            "evaluation of batch 0 took: 0.1538100242614746\n",
            "evaluation of batch 50 took: 0.15415310859680176\n",
            "evaluation of batch 100 took: 0.15177083015441895\n",
            "evaluation of batch 150 took: 0.15975499153137207\n",
            "evaluation of batch 200 took: 0.15523505210876465\n",
            "evaluation of batch 250 took: 0.15665125846862793\n",
            "evaluation of batch 300 took: 0.14745807647705078\n",
            "evaluation of batch 350 took: 0.1507120132446289\n",
            "evaluation of batch 400 took: 0.15104055404663086\n",
            "evaluation of batch 450 took: 0.15158915519714355\n",
            "evaluation of batch 500 took: 0.1502246856689453\n",
            "evaluation of batch 550 took: 0.14746451377868652\n",
            "evaluation of batch 600 took: 0.1507420539855957\n",
            "epoch 13 evaluation on training data time: 95.29390621185303 sec\n",
            "evaluation of batch 0 took: 0.16350173950195312\n",
            "evaluation of batch 50 took: 0.1541914939880371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 13 evaluation on test data time: 22.11387300491333 sec\n",
            "epoch evaluation:  {'epoch': 13, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4574617>, 'test_rouge_1_p': 0.5092755319457332, 'test_rouge_1_r': 0.45649389905109744, 'test_rouge_1_f1': 0.4712418181199887, 'test_rouge_2_p': 0.23194310572240268, 'test_rouge_2_r': 0.21857582521645025, 'test_rouge_2_f1': 0.22200725164592355, 'test_rouge_3_p': 0.08643085430194805, 'test_rouge_3_r': 0.08127050358495672, 'test_rouge_3_f1': 0.08255559622049578, 'test_rouge_L_p': 0.5073913762948361, 'test_rouge_L_r': 0.45510980137793733, 'test_rouge_L_f1': 0.4696861815076608}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14 - batch 10 - loss 1.4113134145736694\n",
            "epoch 14 - batch 20 - loss 1.4181183576583862\n",
            "epoch 14 - batch 30 - loss 1.3250887393951416\n",
            "epoch 14 - batch 40 - loss 1.3311814069747925\n",
            "epoch 14 - batch 50 - loss 1.3152612447738647\n",
            "epoch 14 - batch 60 - loss 1.3798818588256836\n",
            "epoch 14 - batch 70 - loss 1.3720550537109375\n",
            "epoch 14 - batch 80 - loss 1.4473762512207031\n",
            "epoch 14 - batch 90 - loss 1.3778553009033203\n",
            "epoch 14 - batch 100 - loss 1.3653370141983032\n",
            "epoch 14 - batch 110 - loss 1.426531195640564\n",
            "epoch 14 - batch 120 - loss 1.4046411514282227\n",
            "epoch 14 - batch 130 - loss 1.4009439945220947\n",
            "epoch 14 - batch 140 - loss 1.4532874822616577\n",
            "epoch 14 - batch 150 - loss 1.357667088508606\n",
            "epoch 14 - batch 160 - loss 1.3340775966644287\n",
            "epoch 14 - batch 170 - loss 1.3965113162994385\n",
            "epoch 14 - batch 180 - loss 1.415242314338684\n",
            "epoch 14 - batch 190 - loss 1.3946647644042969\n",
            "epoch 14 - batch 200 - loss 1.2804815769195557\n",
            "epoch 14 - batch 210 - loss 1.3635375499725342\n",
            "epoch 14 - batch 220 - loss 1.3986505270004272\n",
            "epoch 14 - batch 230 - loss 1.3249512910842896\n",
            "epoch 14 - batch 240 - loss 1.4092693328857422\n",
            "epoch 14 - batch 250 - loss 1.3481309413909912\n",
            "epoch 14 - batch 260 - loss 1.3698633909225464\n",
            "epoch 14 - batch 270 - loss 1.3663747310638428\n",
            "epoch 14 - batch 280 - loss 1.3285882472991943\n",
            "epoch 14 - batch 290 - loss 1.3969972133636475\n",
            "epoch 14 - batch 300 - loss 1.3078829050064087\n",
            "epoch 14 - batch 310 - loss 1.3145778179168701\n",
            "epoch 14 - batch 320 - loss 1.3793150186538696\n",
            "epoch 14 - batch 330 - loss 1.3298124074935913\n",
            "epoch 14 - batch 340 - loss 1.4205801486968994\n",
            "epoch 14 - batch 350 - loss 1.352495789527893\n",
            "epoch 14 - batch 360 - loss 1.3254649639129639\n",
            "epoch 14 - batch 370 - loss 1.342524766921997\n",
            "epoch 14 - batch 380 - loss 1.3011959791183472\n",
            "epoch 14 - batch 390 - loss 1.3487608432769775\n",
            "epoch 14 - batch 400 - loss 1.3197599649429321\n",
            "epoch 14 - batch 410 - loss 1.3329139947891235\n",
            "epoch 14 - batch 420 - loss 1.3678325414657593\n",
            "epoch 14 - batch 430 - loss 1.370827078819275\n",
            "epoch 14 - batch 440 - loss 1.3087003231048584\n",
            "epoch 14 - batch 450 - loss 1.3130605220794678\n",
            "epoch 14 - batch 460 - loss 1.3481587171554565\n",
            "epoch 14 - batch 470 - loss 1.3426297903060913\n",
            "epoch 14 - batch 480 - loss 1.3287090063095093\n",
            "epoch 14 - batch 490 - loss 1.333119511604309\n",
            "epoch 14 - batch 500 - loss 1.2666064500808716\n",
            "epoch 14 - batch 510 - loss 1.3713223934173584\n",
            "epoch 14 - batch 520 - loss 1.38979971408844\n",
            "epoch 14 - batch 530 - loss 1.398848295211792\n",
            "epoch 14 - batch 540 - loss 1.303565263748169\n",
            "epoch 14 - batch 550 - loss 1.3071576356887817\n",
            "epoch 14 - batch 560 - loss 1.3403581380844116\n",
            "epoch 14 - batch 570 - loss 1.3219300508499146\n",
            "epoch 14 - batch 580 - loss 1.386950135231018\n",
            "epoch 14 - batch 590 - loss 1.3743208646774292\n",
            "epoch 14 - batch 600 - loss 1.2759958505630493\n",
            "epoch 14 - batch 610 - loss 1.3280375003814697\n",
            "epoch 14 training time: 219.6083197593689 sec\n",
            "evaluation of batch 0 took: 0.1564314365386963\n",
            "evaluation of batch 50 took: 0.14907479286193848\n",
            "evaluation of batch 100 took: 0.15680193901062012\n",
            "evaluation of batch 150 took: 0.16429448127746582\n",
            "evaluation of batch 200 took: 0.15270185470581055\n",
            "evaluation of batch 250 took: 0.15418195724487305\n",
            "evaluation of batch 300 took: 0.15193915367126465\n",
            "evaluation of batch 350 took: 0.15343403816223145\n",
            "evaluation of batch 400 took: 0.15785956382751465\n",
            "evaluation of batch 450 took: 0.1468048095703125\n",
            "evaluation of batch 500 took: 0.15353631973266602\n",
            "evaluation of batch 550 took: 0.1537914276123047\n",
            "evaluation of batch 600 took: 0.15233898162841797\n",
            "epoch 14 evaluation on training data time: 94.60196876525879 sec\n",
            "evaluation of batch 0 took: 0.1811206340789795\n",
            "evaluation of batch 50 took: 0.15590572357177734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 14 evaluation on test data time: 22.043648958206177 sec\n",
            "epoch evaluation:  {'epoch': 14, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4315338>, 'test_rouge_1_p': 0.5168475451260051, 'test_rouge_1_r': 0.4659479239138836, 'test_rouge_1_f1': 0.47994853313060487, 'test_rouge_2_p': 0.24118747463474027, 'test_rouge_2_r': 0.22822075385551951, 'test_rouge_2_f1': 0.2314576071475423, 'test_rouge_3_p': 0.09240479572510822, 'test_rouge_3_r': 0.08738479944534633, 'test_rouge_3_f1': 0.08862396284271286, 'test_rouge_L_p': 0.5150117042555656, 'test_rouge_L_r': 0.46457780733031845, 'test_rouge_L_f1': 0.47842002621071195}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 - batch 10 - loss 1.3188730478286743\n",
            "epoch 15 - batch 20 - loss 1.2999902963638306\n",
            "epoch 15 - batch 30 - loss 1.3267905712127686\n",
            "epoch 15 - batch 40 - loss 1.3174508810043335\n",
            "epoch 15 - batch 50 - loss 1.3118751049041748\n",
            "epoch 15 - batch 60 - loss 1.3727606534957886\n",
            "epoch 15 - batch 70 - loss 1.3096843957901\n",
            "epoch 15 - batch 80 - loss 1.3557898998260498\n",
            "epoch 15 - batch 90 - loss 1.3003082275390625\n",
            "epoch 15 - batch 100 - loss 1.315325140953064\n",
            "epoch 15 - batch 110 - loss 1.3500020503997803\n",
            "epoch 15 - batch 120 - loss 1.3460986614227295\n",
            "epoch 15 - batch 130 - loss 1.2715047597885132\n",
            "epoch 15 - batch 140 - loss 1.3324631452560425\n",
            "epoch 15 - batch 150 - loss 1.362257957458496\n",
            "epoch 15 - batch 160 - loss 1.3862597942352295\n",
            "epoch 15 - batch 170 - loss 1.3480112552642822\n",
            "epoch 15 - batch 180 - loss 1.3423540592193604\n",
            "epoch 15 - batch 190 - loss 1.284311294555664\n",
            "epoch 15 - batch 200 - loss 1.2854411602020264\n",
            "epoch 15 - batch 210 - loss 1.2640342712402344\n",
            "epoch 15 - batch 220 - loss 1.3939990997314453\n",
            "epoch 15 - batch 230 - loss 1.319801926612854\n",
            "epoch 15 - batch 240 - loss 1.329842209815979\n",
            "epoch 15 - batch 250 - loss 1.409883737564087\n",
            "epoch 15 - batch 260 - loss 1.4119818210601807\n",
            "epoch 15 - batch 270 - loss 1.3047467470169067\n",
            "epoch 15 - batch 280 - loss 1.3664504289627075\n",
            "epoch 15 - batch 290 - loss 1.2544125318527222\n",
            "epoch 15 - batch 300 - loss 1.3827325105667114\n",
            "epoch 15 - batch 310 - loss 1.2677080631256104\n",
            "epoch 15 - batch 320 - loss 1.3080662488937378\n",
            "epoch 15 - batch 330 - loss 1.2881147861480713\n",
            "epoch 15 - batch 340 - loss 1.3272165060043335\n",
            "epoch 15 - batch 350 - loss 1.338381052017212\n",
            "epoch 15 - batch 360 - loss 1.299378514289856\n",
            "epoch 15 - batch 370 - loss 1.326582908630371\n",
            "epoch 15 - batch 380 - loss 1.3409357070922852\n",
            "epoch 15 - batch 390 - loss 1.359271764755249\n",
            "epoch 15 - batch 400 - loss 1.31978440284729\n",
            "epoch 15 - batch 410 - loss 1.2434319257736206\n",
            "epoch 15 - batch 420 - loss 1.32339608669281\n",
            "epoch 15 - batch 430 - loss 1.259575605392456\n",
            "epoch 15 - batch 440 - loss 1.2878211736679077\n",
            "epoch 15 - batch 450 - loss 1.2750288248062134\n",
            "epoch 15 - batch 460 - loss 1.3657687902450562\n",
            "epoch 15 - batch 470 - loss 1.319252371788025\n",
            "epoch 15 - batch 480 - loss 1.369105339050293\n",
            "epoch 15 - batch 490 - loss 1.2804616689682007\n",
            "epoch 15 - batch 500 - loss 1.3072386980056763\n",
            "epoch 15 - batch 510 - loss 1.2781413793563843\n",
            "epoch 15 - batch 520 - loss 1.2704006433486938\n",
            "epoch 15 - batch 530 - loss 1.317216157913208\n",
            "epoch 15 - batch 540 - loss 1.3727065324783325\n",
            "epoch 15 - batch 550 - loss 1.2545640468597412\n",
            "epoch 15 - batch 560 - loss 1.332041621208191\n",
            "epoch 15 - batch 570 - loss 1.3040968179702759\n",
            "epoch 15 - batch 580 - loss 1.2673866748809814\n",
            "epoch 15 - batch 590 - loss 1.262673258781433\n",
            "epoch 15 - batch 600 - loss 1.2895599603652954\n",
            "epoch 15 - batch 610 - loss 1.3188539743423462\n",
            "epoch 15 training time: 219.5998125076294 sec\n",
            "evaluation of batch 0 took: 0.15971660614013672\n",
            "evaluation of batch 50 took: 0.1537613868713379\n",
            "evaluation of batch 100 took: 0.14708733558654785\n",
            "evaluation of batch 150 took: 0.14818549156188965\n",
            "evaluation of batch 200 took: 0.16286396980285645\n",
            "evaluation of batch 250 took: 0.15706920623779297\n",
            "evaluation of batch 300 took: 0.15781617164611816\n",
            "evaluation of batch 350 took: 0.14593219757080078\n",
            "evaluation of batch 400 took: 0.15511012077331543\n",
            "evaluation of batch 450 took: 0.14910483360290527\n",
            "evaluation of batch 500 took: 0.15115714073181152\n",
            "evaluation of batch 550 took: 0.14882850646972656\n",
            "evaluation of batch 600 took: 0.16048908233642578\n",
            "epoch 15 evaluation on training data time: 94.92514705657959 sec\n",
            "evaluation of batch 0 took: 0.1637279987335205\n",
            "evaluation of batch 50 took: 0.1498703956604004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 evaluation on test data time: 22.074350833892822 sec\n",
            "epoch evaluation:  {'epoch': 15, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.4118397>, 'test_rouge_1_p': 0.5249720378208101, 'test_rouge_1_r': 0.47031648596938747, 'test_rouge_1_f1': 0.4857828979112967, 'test_rouge_2_p': 0.2478617086038961, 'test_rouge_2_r': 0.2333901937905844, 'test_rouge_2_f1': 0.237256885149032, 'test_rouge_3_p': 0.09707601968344155, 'test_rouge_3_r': 0.09106085971320343, 'test_rouge_3_f1': 0.09260703334364047, 'test_rouge_L_p': 0.5231364989177488, 'test_rouge_L_r': 0.468973939007421, 'test_rouge_L_f1': 0.484270084144526}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 15 saved checkpoint: models/checkpoints/baseline/ckpt-6\n",
            "epoch 16 - batch 10 - loss 1.3413892984390259\n",
            "epoch 16 - batch 20 - loss 1.3168531656265259\n",
            "epoch 16 - batch 30 - loss 1.3265506029129028\n",
            "epoch 16 - batch 40 - loss 1.295367956161499\n",
            "epoch 16 - batch 50 - loss 1.427797555923462\n",
            "epoch 16 - batch 60 - loss 1.363897681236267\n",
            "epoch 16 - batch 70 - loss 1.3253763914108276\n",
            "epoch 16 - batch 80 - loss 1.2847853899002075\n",
            "epoch 16 - batch 90 - loss 1.3402217626571655\n",
            "epoch 16 - batch 100 - loss 1.3245259523391724\n",
            "epoch 16 - batch 110 - loss 1.2843235731124878\n",
            "epoch 16 - batch 120 - loss 1.310788869857788\n",
            "epoch 16 - batch 130 - loss 1.2752691507339478\n",
            "epoch 16 - batch 140 - loss 1.2421761751174927\n",
            "epoch 16 - batch 150 - loss 1.2736510038375854\n",
            "epoch 16 - batch 160 - loss 1.2351289987564087\n",
            "epoch 16 - batch 170 - loss 1.2778303623199463\n",
            "epoch 16 - batch 180 - loss 1.2539759874343872\n",
            "epoch 16 - batch 190 - loss 1.3456462621688843\n",
            "epoch 16 - batch 200 - loss 1.263581395149231\n",
            "epoch 16 - batch 210 - loss 1.279205322265625\n",
            "epoch 16 - batch 220 - loss 1.3574981689453125\n",
            "epoch 16 - batch 230 - loss 1.2721383571624756\n",
            "epoch 16 - batch 240 - loss 1.3430869579315186\n",
            "epoch 16 - batch 250 - loss 1.3429783582687378\n",
            "epoch 16 - batch 260 - loss 1.305456519126892\n",
            "epoch 16 - batch 270 - loss 1.2971807718276978\n",
            "epoch 16 - batch 280 - loss 1.3112449645996094\n",
            "epoch 16 - batch 290 - loss 1.3486788272857666\n",
            "epoch 16 - batch 300 - loss 1.2467081546783447\n",
            "epoch 16 - batch 310 - loss 1.3577450513839722\n",
            "epoch 16 - batch 320 - loss 1.2946711778640747\n",
            "epoch 16 - batch 330 - loss 1.2966316938400269\n",
            "epoch 16 - batch 340 - loss 1.2485631704330444\n",
            "epoch 16 - batch 350 - loss 1.3852356672286987\n",
            "epoch 16 - batch 360 - loss 1.2909069061279297\n",
            "epoch 16 - batch 370 - loss 1.3014479875564575\n",
            "epoch 16 - batch 380 - loss 1.3003556728363037\n",
            "epoch 16 - batch 390 - loss 1.3653298616409302\n",
            "epoch 16 - batch 400 - loss 1.2896897792816162\n",
            "epoch 16 - batch 410 - loss 1.2130247354507446\n",
            "epoch 16 - batch 420 - loss 1.3198310136795044\n",
            "epoch 16 - batch 430 - loss 1.3219000101089478\n",
            "epoch 16 - batch 440 - loss 1.2650330066680908\n",
            "epoch 16 - batch 450 - loss 1.3157598972320557\n",
            "epoch 16 - batch 460 - loss 1.2877839803695679\n",
            "epoch 16 - batch 470 - loss 1.3037370443344116\n",
            "epoch 16 - batch 480 - loss 1.2508490085601807\n",
            "epoch 16 - batch 490 - loss 1.235683560371399\n",
            "epoch 16 - batch 500 - loss 1.2179348468780518\n",
            "epoch 16 - batch 510 - loss 1.2229506969451904\n",
            "epoch 16 - batch 520 - loss 1.2388057708740234\n",
            "epoch 16 - batch 530 - loss 1.255420446395874\n",
            "epoch 16 - batch 540 - loss 1.2767008543014526\n",
            "epoch 16 - batch 550 - loss 1.2551076412200928\n",
            "epoch 16 - batch 560 - loss 1.2839642763137817\n",
            "epoch 16 - batch 570 - loss 1.2661508321762085\n",
            "epoch 16 - batch 580 - loss 1.3288952112197876\n",
            "epoch 16 - batch 590 - loss 1.2553365230560303\n",
            "epoch 16 - batch 600 - loss 1.2846471071243286\n",
            "epoch 16 - batch 610 - loss 1.2860649824142456\n",
            "epoch 16 training time: 219.87542414665222 sec\n",
            "evaluation of batch 0 took: 0.15857744216918945\n",
            "evaluation of batch 50 took: 0.16103672981262207\n",
            "evaluation of batch 100 took: 0.1645808219909668\n",
            "evaluation of batch 150 took: 0.15950298309326172\n",
            "evaluation of batch 200 took: 0.1483016014099121\n",
            "evaluation of batch 250 took: 0.1566610336303711\n",
            "evaluation of batch 300 took: 0.14834976196289062\n",
            "evaluation of batch 350 took: 0.15569019317626953\n",
            "evaluation of batch 400 took: 0.1550915241241455\n",
            "evaluation of batch 450 took: 0.14771795272827148\n",
            "evaluation of batch 500 took: 0.1550920009613037\n",
            "evaluation of batch 550 took: 0.15926051139831543\n",
            "evaluation of batch 600 took: 0.16697144508361816\n",
            "epoch 16 evaluation on training data time: 95.85505843162537 sec\n",
            "evaluation of batch 0 took: 0.16117382049560547\n",
            "evaluation of batch 50 took: 0.16085267066955566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 16 evaluation on test data time: 22.225306272506714 sec\n",
            "epoch evaluation:  {'epoch': 16, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3936195>, 'test_rouge_1_p': 0.5299310668869822, 'test_rouge_1_r': 0.47662005498222004, 'test_rouge_1_f1': 0.4913197210719024, 'test_rouge_2_p': 0.2532907196969697, 'test_rouge_2_r': 0.23887606534090905, 'test_rouge_2_f1': 0.24263016862004788, 'test_rouge_3_p': 0.10035617052218616, 'test_rouge_3_r': 0.09454181885822509, 'test_rouge_3_f1': 0.09601307438221503, 'test_rouge_L_p': 0.5281999181064468, 'test_rouge_L_r': 0.47532944640924535, 'test_rouge_L_f1': 0.489880766695651}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 17 - batch 10 - loss 1.2999515533447266\n",
            "epoch 17 - batch 20 - loss 1.2804538011550903\n",
            "epoch 17 - batch 30 - loss 1.314685583114624\n",
            "epoch 17 - batch 40 - loss 1.2675293684005737\n",
            "epoch 17 - batch 50 - loss 1.267720103263855\n",
            "epoch 17 - batch 60 - loss 1.2159061431884766\n",
            "epoch 17 - batch 70 - loss 1.2607898712158203\n",
            "epoch 17 - batch 80 - loss 1.3152962923049927\n",
            "epoch 17 - batch 90 - loss 1.3161388635635376\n",
            "epoch 17 - batch 100 - loss 1.3212226629257202\n",
            "epoch 17 - batch 110 - loss 1.30653977394104\n",
            "epoch 17 - batch 120 - loss 1.3311716318130493\n",
            "epoch 17 - batch 130 - loss 1.280622124671936\n",
            "epoch 17 - batch 140 - loss 1.228394627571106\n",
            "epoch 17 - batch 150 - loss 1.3229892253875732\n",
            "epoch 17 - batch 160 - loss 1.285012125968933\n",
            "epoch 17 - batch 170 - loss 1.2660722732543945\n",
            "epoch 17 - batch 180 - loss 1.2525014877319336\n",
            "epoch 17 - batch 190 - loss 1.223523497581482\n",
            "epoch 17 - batch 200 - loss 1.3034255504608154\n",
            "epoch 17 - batch 210 - loss 1.2432700395584106\n",
            "epoch 17 - batch 220 - loss 1.2625876665115356\n",
            "epoch 17 - batch 230 - loss 1.221599817276001\n",
            "epoch 17 - batch 240 - loss 1.2625865936279297\n",
            "epoch 17 - batch 250 - loss 1.231949806213379\n",
            "epoch 17 - batch 260 - loss 1.1975146532058716\n",
            "epoch 17 - batch 270 - loss 1.3237388134002686\n",
            "epoch 17 - batch 280 - loss 1.2259613275527954\n",
            "epoch 17 - batch 290 - loss 1.3285863399505615\n",
            "epoch 17 - batch 300 - loss 1.3193098306655884\n",
            "epoch 17 - batch 310 - loss 1.2401096820831299\n",
            "epoch 17 - batch 320 - loss 1.239241600036621\n",
            "epoch 17 - batch 330 - loss 1.3103889226913452\n",
            "epoch 17 - batch 340 - loss 1.3586304187774658\n",
            "epoch 17 - batch 350 - loss 1.3089981079101562\n",
            "epoch 17 - batch 360 - loss 1.2635316848754883\n",
            "epoch 17 - batch 370 - loss 1.3296080827713013\n",
            "epoch 17 - batch 380 - loss 1.2805367708206177\n",
            "epoch 17 - batch 390 - loss 1.2935006618499756\n",
            "epoch 17 - batch 400 - loss 1.2936345338821411\n",
            "epoch 17 - batch 410 - loss 1.301087498664856\n",
            "epoch 17 - batch 420 - loss 1.317950963973999\n",
            "epoch 17 - batch 430 - loss 1.233864188194275\n",
            "epoch 17 - batch 440 - loss 1.295143485069275\n",
            "epoch 17 - batch 450 - loss 1.2324129343032837\n",
            "epoch 17 - batch 460 - loss 1.2723863124847412\n",
            "epoch 17 - batch 470 - loss 1.2947778701782227\n",
            "epoch 17 - batch 480 - loss 1.331002116203308\n",
            "epoch 17 - batch 490 - loss 1.2644002437591553\n",
            "epoch 17 - batch 500 - loss 1.1728487014770508\n",
            "epoch 17 - batch 510 - loss 1.2423763275146484\n",
            "epoch 17 - batch 520 - loss 1.2191174030303955\n",
            "epoch 17 - batch 530 - loss 1.3374277353286743\n",
            "epoch 17 - batch 540 - loss 1.2551498413085938\n",
            "epoch 17 - batch 550 - loss 1.2443631887435913\n",
            "epoch 17 - batch 560 - loss 1.2382936477661133\n",
            "epoch 17 - batch 570 - loss 1.277729868888855\n",
            "epoch 17 - batch 580 - loss 1.192816972732544\n",
            "epoch 17 - batch 590 - loss 1.3159868717193604\n",
            "epoch 17 - batch 600 - loss 1.2981138229370117\n",
            "epoch 17 - batch 610 - loss 1.3091505765914917\n",
            "epoch 17 training time: 219.6329436302185 sec\n",
            "evaluation of batch 0 took: 0.15192031860351562\n",
            "evaluation of batch 50 took: 0.1578841209411621\n",
            "evaluation of batch 100 took: 0.15069985389709473\n",
            "evaluation of batch 150 took: 0.1597301959991455\n",
            "evaluation of batch 200 took: 0.14992833137512207\n",
            "evaluation of batch 250 took: 0.15570855140686035\n",
            "evaluation of batch 300 took: 0.15426945686340332\n",
            "evaluation of batch 350 took: 0.155501127243042\n",
            "evaluation of batch 400 took: 0.15810322761535645\n",
            "evaluation of batch 450 took: 0.16321802139282227\n",
            "evaluation of batch 500 took: 0.15952062606811523\n",
            "evaluation of batch 550 took: 0.15305471420288086\n",
            "evaluation of batch 600 took: 0.1581118106842041\n",
            "epoch 17 evaluation on training data time: 96.46024298667908 sec\n",
            "evaluation of batch 0 took: 0.17051911354064941\n",
            "evaluation of batch 50 took: 0.15481305122375488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 17 evaluation on test data time: 22.275878429412842 sec\n",
            "epoch evaluation:  {'epoch': 17, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3768172>, 'test_rouge_1_p': 0.5342333471030458, 'test_rouge_1_r': 0.48596639827806093, 'test_rouge_1_f1': 0.499172138343709, 'test_rouge_2_p': 0.2604350564799784, 'test_rouge_2_r': 0.24631717566287892, 'test_rouge_2_f1': 0.25000771297887275, 'test_rouge_3_p': 0.10479001792478355, 'test_rouge_3_r': 0.09882051542207795, 'test_rouge_3_f1': 0.10034016625115959, 'test_rouge_L_p': 0.5323120796614101, 'test_rouge_L_r': 0.4845372470721241, 'test_rouge_L_f1': 0.4975756095061592}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 - batch 10 - loss 1.2701367139816284\n",
            "epoch 18 - batch 20 - loss 1.2531397342681885\n",
            "epoch 18 - batch 30 - loss 1.2852472066879272\n",
            "epoch 18 - batch 40 - loss 1.2632554769515991\n",
            "epoch 18 - batch 50 - loss 1.2287856340408325\n",
            "epoch 18 - batch 60 - loss 1.2945927381515503\n",
            "epoch 18 - batch 70 - loss 1.2259210348129272\n",
            "epoch 18 - batch 80 - loss 1.2287834882736206\n",
            "epoch 18 - batch 90 - loss 1.2558677196502686\n",
            "epoch 18 - batch 100 - loss 1.2596901655197144\n",
            "epoch 18 - batch 110 - loss 1.2847623825073242\n",
            "epoch 18 - batch 120 - loss 1.2355308532714844\n",
            "epoch 18 - batch 130 - loss 1.215286374092102\n",
            "epoch 18 - batch 140 - loss 1.1995782852172852\n",
            "epoch 18 - batch 150 - loss 1.2343703508377075\n",
            "epoch 18 - batch 160 - loss 1.2776838541030884\n",
            "epoch 18 - batch 170 - loss 1.2728983163833618\n",
            "epoch 18 - batch 180 - loss 1.2202682495117188\n",
            "epoch 18 - batch 190 - loss 1.2905484437942505\n",
            "epoch 18 - batch 200 - loss 1.26642906665802\n",
            "epoch 18 - batch 210 - loss 1.327738881111145\n",
            "epoch 18 - batch 220 - loss 1.2135878801345825\n",
            "epoch 18 - batch 230 - loss 1.208985686302185\n",
            "epoch 18 - batch 240 - loss 1.271361231803894\n",
            "epoch 18 - batch 250 - loss 1.2423003911972046\n",
            "epoch 18 - batch 260 - loss 1.2980462312698364\n",
            "epoch 18 - batch 270 - loss 1.2641830444335938\n",
            "epoch 18 - batch 280 - loss 1.23899245262146\n",
            "epoch 18 - batch 290 - loss 1.3690274953842163\n",
            "epoch 18 - batch 300 - loss 1.1730071306228638\n",
            "epoch 18 - batch 310 - loss 1.19901442527771\n",
            "epoch 18 - batch 320 - loss 1.2558754682540894\n",
            "epoch 18 - batch 330 - loss 1.242592453956604\n",
            "epoch 18 - batch 340 - loss 1.2032147645950317\n",
            "epoch 18 - batch 350 - loss 1.248465657234192\n",
            "epoch 18 - batch 360 - loss 1.2485660314559937\n",
            "epoch 18 - batch 370 - loss 1.1782385110855103\n",
            "epoch 18 - batch 380 - loss 1.2472617626190186\n",
            "epoch 18 - batch 390 - loss 1.2146507501602173\n",
            "epoch 18 - batch 400 - loss 1.2859134674072266\n",
            "epoch 18 - batch 410 - loss 1.2383594512939453\n",
            "epoch 18 - batch 420 - loss 1.2609422206878662\n",
            "epoch 18 - batch 430 - loss 1.1872988939285278\n",
            "epoch 18 - batch 440 - loss 1.27707839012146\n",
            "epoch 18 - batch 450 - loss 1.2250661849975586\n",
            "epoch 18 - batch 460 - loss 1.2175372838974\n",
            "epoch 18 - batch 470 - loss 1.2137362957000732\n",
            "epoch 18 - batch 480 - loss 1.3002946376800537\n",
            "epoch 18 - batch 490 - loss 1.1911855936050415\n",
            "epoch 18 - batch 500 - loss 1.1840869188308716\n",
            "epoch 18 - batch 510 - loss 1.282309889793396\n",
            "epoch 18 - batch 520 - loss 1.1952065229415894\n",
            "epoch 18 - batch 530 - loss 1.275469422340393\n",
            "epoch 18 - batch 540 - loss 1.2628566026687622\n",
            "epoch 18 - batch 550 - loss 1.191135287284851\n",
            "epoch 18 - batch 560 - loss 1.255484938621521\n",
            "epoch 18 - batch 570 - loss 1.2401689291000366\n",
            "epoch 18 - batch 580 - loss 1.2102832794189453\n",
            "epoch 18 - batch 590 - loss 1.1940821409225464\n",
            "epoch 18 - batch 600 - loss 1.2233412265777588\n",
            "epoch 18 - batch 610 - loss 1.251361608505249\n",
            "epoch 18 training time: 219.71045804023743 sec\n",
            "evaluation of batch 0 took: 0.15358519554138184\n",
            "evaluation of batch 50 took: 0.1536998748779297\n",
            "evaluation of batch 100 took: 0.15449070930480957\n",
            "evaluation of batch 150 took: 0.1604630947113037\n",
            "evaluation of batch 200 took: 0.1501467227935791\n",
            "evaluation of batch 250 took: 0.15336966514587402\n",
            "evaluation of batch 300 took: 0.1568291187286377\n",
            "evaluation of batch 350 took: 0.15053176879882812\n",
            "evaluation of batch 400 took: 0.15784931182861328\n",
            "evaluation of batch 450 took: 0.145920991897583\n",
            "evaluation of batch 500 took: 0.14840173721313477\n",
            "evaluation of batch 550 took: 0.15682291984558105\n",
            "evaluation of batch 600 took: 0.15496468544006348\n",
            "epoch 18 evaluation on training data time: 95.84803342819214 sec\n",
            "evaluation of batch 0 took: 0.16690754890441895\n",
            "evaluation of batch 50 took: 0.15487313270568848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 evaluation on test data time: 22.27597427368164 sec\n",
            "epoch evaluation:  {'epoch': 18, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.364183>, 'test_rouge_1_p': 0.5370624190727428, 'test_rouge_1_r': 0.4905682905264377, 'test_rouge_1_f1': 0.5030910842066483, 'test_rouge_2_p': 0.2642590807629871, 'test_rouge_2_r': 0.2507966805330087, 'test_rouge_2_f1': 0.2541849265425007, 'test_rouge_3_p': 0.10743371212121214, 'test_rouge_3_r': 0.10156355688582254, 'test_rouge_3_f1': 0.10304786746611529, 'test_rouge_L_p': 0.5351570653119202, 'test_rouge_L_r': 0.4891199039985313, 'test_rouge_L_f1': 0.5014879109605672}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 18 saved checkpoint: models/checkpoints/baseline/ckpt-7\n",
            "epoch 19 - batch 10 - loss 1.258319616317749\n",
            "epoch 19 - batch 20 - loss 1.2491806745529175\n",
            "epoch 19 - batch 30 - loss 1.2165786027908325\n",
            "epoch 19 - batch 40 - loss 1.2839330434799194\n",
            "epoch 19 - batch 50 - loss 1.2785189151763916\n",
            "epoch 19 - batch 60 - loss 1.275578498840332\n",
            "epoch 19 - batch 70 - loss 1.2531636953353882\n",
            "epoch 19 - batch 80 - loss 1.2650573253631592\n",
            "epoch 19 - batch 90 - loss 1.2064182758331299\n",
            "epoch 19 - batch 100 - loss 1.2441771030426025\n",
            "epoch 19 - batch 110 - loss 1.2399855852127075\n",
            "epoch 19 - batch 120 - loss 1.1902751922607422\n",
            "epoch 19 - batch 130 - loss 1.1982698440551758\n",
            "epoch 19 - batch 140 - loss 1.2973839044570923\n",
            "epoch 19 - batch 150 - loss 1.3040140867233276\n",
            "epoch 19 - batch 160 - loss 1.2822418212890625\n",
            "epoch 19 - batch 170 - loss 1.2511653900146484\n",
            "epoch 19 - batch 180 - loss 1.2254135608673096\n",
            "epoch 19 - batch 190 - loss 1.2635669708251953\n",
            "epoch 19 - batch 200 - loss 1.2468520402908325\n",
            "epoch 19 - batch 210 - loss 1.2332528829574585\n",
            "epoch 19 - batch 220 - loss 1.328144907951355\n",
            "epoch 19 - batch 230 - loss 1.2272688150405884\n",
            "epoch 19 - batch 240 - loss 1.2064119577407837\n",
            "epoch 19 - batch 250 - loss 1.2644022703170776\n",
            "epoch 19 - batch 260 - loss 1.2365423440933228\n",
            "epoch 19 - batch 270 - loss 1.2706037759780884\n",
            "epoch 19 - batch 280 - loss 1.2256020307540894\n",
            "epoch 19 - batch 290 - loss 1.2442586421966553\n",
            "epoch 19 - batch 300 - loss 1.2316495180130005\n",
            "epoch 19 - batch 310 - loss 1.2226028442382812\n",
            "epoch 19 - batch 320 - loss 1.249866247177124\n",
            "epoch 19 - batch 330 - loss 1.1964324712753296\n",
            "epoch 19 - batch 340 - loss 1.2276819944381714\n",
            "epoch 19 - batch 350 - loss 1.2512010335922241\n",
            "epoch 19 - batch 360 - loss 1.1688050031661987\n",
            "epoch 19 - batch 370 - loss 1.2307827472686768\n",
            "epoch 19 - batch 380 - loss 1.1939462423324585\n",
            "epoch 19 - batch 390 - loss 1.1869890689849854\n",
            "epoch 19 - batch 400 - loss 1.1766884326934814\n",
            "epoch 19 - batch 410 - loss 1.2276992797851562\n",
            "epoch 19 - batch 420 - loss 1.2757294178009033\n",
            "epoch 19 - batch 430 - loss 1.1092528104782104\n",
            "epoch 19 - batch 440 - loss 1.160808801651001\n",
            "epoch 19 - batch 450 - loss 1.1969159841537476\n",
            "epoch 19 - batch 460 - loss 1.23055899143219\n",
            "epoch 19 - batch 470 - loss 1.184319257736206\n",
            "epoch 19 - batch 480 - loss 1.2476060390472412\n",
            "epoch 19 - batch 490 - loss 1.1440593004226685\n",
            "epoch 19 - batch 500 - loss 1.204522967338562\n",
            "epoch 19 - batch 510 - loss 1.248919129371643\n",
            "epoch 19 - batch 520 - loss 1.1798924207687378\n",
            "epoch 19 - batch 530 - loss 1.2679195404052734\n",
            "epoch 19 - batch 540 - loss 1.2391096353530884\n",
            "epoch 19 - batch 550 - loss 1.216256856918335\n",
            "epoch 19 - batch 560 - loss 1.2249068021774292\n",
            "epoch 19 - batch 570 - loss 1.1315491199493408\n",
            "epoch 19 - batch 580 - loss 1.1881457567214966\n",
            "epoch 19 - batch 590 - loss 1.2041985988616943\n",
            "epoch 19 - batch 600 - loss 1.2116385698318481\n",
            "epoch 19 - batch 610 - loss 1.1415411233901978\n",
            "epoch 19 training time: 220.0223743915558 sec\n",
            "evaluation of batch 0 took: 0.15395164489746094\n",
            "evaluation of batch 50 took: 0.16203856468200684\n",
            "evaluation of batch 100 took: 0.15454697608947754\n",
            "evaluation of batch 150 took: 0.1517181396484375\n",
            "evaluation of batch 200 took: 0.16909527778625488\n",
            "evaluation of batch 250 took: 0.1527998447418213\n",
            "evaluation of batch 300 took: 0.1651933193206787\n",
            "evaluation of batch 350 took: 0.15300726890563965\n",
            "evaluation of batch 400 took: 0.15203452110290527\n",
            "evaluation of batch 450 took: 0.14734411239624023\n",
            "evaluation of batch 500 took: 0.15149950981140137\n",
            "evaluation of batch 550 took: 0.15442252159118652\n",
            "evaluation of batch 600 took: 0.18024921417236328\n",
            "epoch 19 evaluation on training data time: 96.50224781036377 sec\n",
            "evaluation of batch 0 took: 0.1689622402191162\n",
            "evaluation of batch 50 took: 0.15182256698608398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 19 evaluation on test data time: 22.735050201416016 sec\n",
            "epoch evaluation:  {'epoch': 19, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3509929>, 'test_rouge_1_p': 0.541845854108689, 'test_rouge_1_r': 0.4940648311881571, 'test_rouge_1_f1': 0.5070142279088607, 'test_rouge_2_p': 0.2674675747429655, 'test_rouge_2_r': 0.25357333096590917, 'test_rouge_2_f1': 0.25716582770978574, 'test_rouge_3_p': 0.10962505918560607, 'test_rouge_3_r': 0.1037198153409091, 'test_rouge_3_f1': 0.10519882136093077, 'test_rouge_L_p': 0.5399571991438619, 'test_rouge_L_r': 0.492629459454236, 'test_rouge_L_f1': 0.5054289307796055}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 20 - batch 10 - loss 1.1843721866607666\n",
            "epoch 20 - batch 20 - loss 1.2128217220306396\n",
            "epoch 20 - batch 30 - loss 1.1629003286361694\n",
            "epoch 20 - batch 40 - loss 1.1856689453125\n",
            "epoch 20 - batch 50 - loss 1.143454909324646\n",
            "epoch 20 - batch 60 - loss 1.255554437637329\n",
            "epoch 20 - batch 70 - loss 1.2205934524536133\n",
            "epoch 20 - batch 80 - loss 1.1977511644363403\n",
            "epoch 20 - batch 90 - loss 1.2276194095611572\n",
            "epoch 20 - batch 100 - loss 1.2704765796661377\n",
            "epoch 20 - batch 110 - loss 1.1769272089004517\n",
            "epoch 20 - batch 120 - loss 1.2388933897018433\n",
            "epoch 20 - batch 130 - loss 1.2280696630477905\n",
            "epoch 20 - batch 140 - loss 1.2066421508789062\n",
            "epoch 20 - batch 150 - loss 1.1978808641433716\n",
            "epoch 20 - batch 160 - loss 1.2056087255477905\n",
            "epoch 20 - batch 170 - loss 1.214509129524231\n",
            "epoch 20 - batch 180 - loss 1.1701723337173462\n",
            "epoch 20 - batch 190 - loss 1.1569287776947021\n",
            "epoch 20 - batch 200 - loss 1.1725705862045288\n",
            "epoch 20 - batch 210 - loss 1.2535406351089478\n",
            "epoch 20 - batch 220 - loss 1.2083903551101685\n",
            "epoch 20 - batch 230 - loss 1.1737042665481567\n",
            "epoch 20 - batch 240 - loss 1.2082794904708862\n",
            "epoch 20 - batch 250 - loss 1.3027387857437134\n",
            "epoch 20 - batch 260 - loss 1.1888700723648071\n",
            "epoch 20 - batch 270 - loss 1.203669786453247\n",
            "epoch 20 - batch 280 - loss 1.1820732355117798\n",
            "epoch 20 - batch 290 - loss 1.2713690996170044\n",
            "epoch 20 - batch 300 - loss 1.2346464395523071\n",
            "epoch 20 - batch 310 - loss 1.2284934520721436\n",
            "epoch 20 - batch 320 - loss 1.221056342124939\n",
            "epoch 20 - batch 330 - loss 1.159657597541809\n",
            "epoch 20 - batch 340 - loss 1.2279243469238281\n",
            "epoch 20 - batch 350 - loss 1.1972078084945679\n",
            "epoch 20 - batch 360 - loss 1.2212145328521729\n",
            "epoch 20 - batch 370 - loss 1.173567771911621\n",
            "epoch 20 - batch 380 - loss 1.17318856716156\n",
            "epoch 20 - batch 390 - loss 1.1763951778411865\n",
            "epoch 20 - batch 400 - loss 1.199613332748413\n",
            "epoch 20 - batch 410 - loss 1.2712326049804688\n",
            "epoch 20 - batch 420 - loss 1.2147282361984253\n",
            "epoch 20 - batch 430 - loss 1.1700079441070557\n",
            "epoch 20 - batch 440 - loss 1.2488383054733276\n",
            "epoch 20 - batch 450 - loss 1.2210878133773804\n",
            "epoch 20 - batch 460 - loss 1.1362838745117188\n",
            "epoch 20 - batch 470 - loss 1.1412831544876099\n",
            "epoch 20 - batch 480 - loss 1.175158143043518\n",
            "epoch 20 - batch 490 - loss 1.1403001546859741\n",
            "epoch 20 - batch 500 - loss 1.218518614768982\n",
            "epoch 20 - batch 510 - loss 1.2545945644378662\n",
            "epoch 20 - batch 520 - loss 1.160487174987793\n",
            "epoch 20 - batch 530 - loss 1.2105075120925903\n",
            "epoch 20 - batch 540 - loss 1.153850793838501\n",
            "epoch 20 - batch 550 - loss 1.2032417058944702\n",
            "epoch 20 - batch 560 - loss 1.235626459121704\n",
            "epoch 20 - batch 570 - loss 1.155583381652832\n",
            "epoch 20 - batch 580 - loss 1.1930681467056274\n",
            "epoch 20 - batch 590 - loss 1.2664700746536255\n",
            "epoch 20 - batch 600 - loss 1.1277759075164795\n",
            "epoch 20 - batch 610 - loss 1.231819748878479\n",
            "epoch 20 training time: 221.84840273857117 sec\n",
            "evaluation of batch 0 took: 0.15688657760620117\n",
            "evaluation of batch 50 took: 0.15479540824890137\n",
            "evaluation of batch 100 took: 0.15686249732971191\n",
            "evaluation of batch 150 took: 0.1511549949645996\n",
            "evaluation of batch 200 took: 0.156036376953125\n",
            "evaluation of batch 250 took: 0.16061687469482422\n",
            "evaluation of batch 300 took: 0.16023540496826172\n",
            "evaluation of batch 350 took: 0.15075945854187012\n",
            "evaluation of batch 400 took: 0.15709948539733887\n",
            "evaluation of batch 450 took: 0.14872074127197266\n",
            "evaluation of batch 500 took: 0.15410566329956055\n",
            "evaluation of batch 550 took: 0.15805435180664062\n",
            "evaluation of batch 600 took: 0.17706656455993652\n",
            "epoch 20 evaluation on training data time: 96.83347415924072 sec\n",
            "evaluation of batch 0 took: 0.16356682777404785\n",
            "evaluation of batch 50 took: 0.15693259239196777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 20 evaluation on test data time: 22.190263986587524 sec\n",
            "epoch evaluation:  {'epoch': 20, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3421707>, 'test_rouge_1_p': 0.5460092595276747, 'test_rouge_1_r': 0.49635622004483615, 'test_rouge_1_f1': 0.5100986601050013, 'test_rouge_2_p': 0.2709381764069264, 'test_rouge_2_r': 0.25592173126352813, 'test_rouge_2_f1': 0.25991703784862774, 'test_rouge_3_p': 0.11153633150703465, 'test_rouge_3_r': 0.10491240530303032, 'test_rouge_3_f1': 0.10665418333526588, 'test_rouge_L_p': 0.5440798389668366, 'test_rouge_L_r': 0.49491828158820333, 'test_rouge_L_f1': 0.5084932552566479}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 - batch 10 - loss 1.232672095298767\n",
            "epoch 21 - batch 20 - loss 1.2048145532608032\n",
            "epoch 21 - batch 30 - loss 1.183179497718811\n",
            "epoch 21 - batch 40 - loss 1.217676043510437\n",
            "epoch 21 - batch 50 - loss 1.2216930389404297\n",
            "epoch 21 - batch 60 - loss 1.1833093166351318\n",
            "epoch 21 - batch 70 - loss 1.2079002857208252\n",
            "epoch 21 - batch 80 - loss 1.2253122329711914\n",
            "epoch 21 - batch 90 - loss 1.1411129236221313\n",
            "epoch 21 - batch 100 - loss 1.1724730730056763\n",
            "epoch 21 - batch 110 - loss 1.163080096244812\n",
            "epoch 21 - batch 120 - loss 1.2274056673049927\n",
            "epoch 21 - batch 130 - loss 1.1563860177993774\n",
            "epoch 21 - batch 140 - loss 1.2071526050567627\n",
            "epoch 21 - batch 150 - loss 1.1341286897659302\n",
            "epoch 21 - batch 160 - loss 1.2260690927505493\n",
            "epoch 21 - batch 170 - loss 1.1025593280792236\n",
            "epoch 21 - batch 180 - loss 1.2623040676116943\n",
            "epoch 21 - batch 190 - loss 1.0711880922317505\n",
            "epoch 21 - batch 200 - loss 1.1775233745574951\n",
            "epoch 21 - batch 210 - loss 1.223444938659668\n",
            "epoch 21 - batch 220 - loss 1.1688063144683838\n",
            "epoch 21 - batch 230 - loss 1.121791124343872\n",
            "epoch 21 - batch 240 - loss 1.2213941812515259\n",
            "epoch 21 - batch 250 - loss 1.2571407556533813\n",
            "epoch 21 - batch 260 - loss 1.0839660167694092\n",
            "epoch 21 - batch 270 - loss 1.1633541584014893\n",
            "epoch 21 - batch 280 - loss 1.1556516885757446\n",
            "epoch 21 - batch 290 - loss 1.2673075199127197\n",
            "epoch 21 - batch 300 - loss 1.1819080114364624\n",
            "epoch 21 - batch 310 - loss 1.169947624206543\n",
            "epoch 21 - batch 320 - loss 1.2468129396438599\n",
            "epoch 21 - batch 330 - loss 1.1640260219573975\n",
            "epoch 21 - batch 340 - loss 1.2352800369262695\n",
            "epoch 21 - batch 350 - loss 1.2205846309661865\n",
            "epoch 21 - batch 360 - loss 1.1630183458328247\n",
            "epoch 21 - batch 370 - loss 1.2293177843093872\n",
            "epoch 21 - batch 380 - loss 1.176138997077942\n",
            "epoch 21 - batch 390 - loss 1.1477985382080078\n",
            "epoch 21 - batch 400 - loss 1.1557129621505737\n",
            "epoch 21 - batch 410 - loss 1.2249248027801514\n",
            "epoch 21 - batch 420 - loss 1.1730245351791382\n",
            "epoch 21 - batch 430 - loss 1.1406859159469604\n",
            "epoch 21 - batch 440 - loss 1.2493667602539062\n",
            "epoch 21 - batch 450 - loss 1.1980279684066772\n",
            "epoch 21 - batch 460 - loss 1.1734486818313599\n",
            "epoch 21 - batch 470 - loss 1.184923529624939\n",
            "epoch 21 - batch 480 - loss 1.1940196752548218\n",
            "epoch 21 - batch 490 - loss 1.1298116445541382\n",
            "epoch 21 - batch 500 - loss 1.2057253122329712\n",
            "epoch 21 - batch 510 - loss 1.248323678970337\n",
            "epoch 21 - batch 520 - loss 1.183722734451294\n",
            "epoch 21 - batch 530 - loss 1.2125627994537354\n",
            "epoch 21 - batch 540 - loss 1.1306970119476318\n",
            "epoch 21 - batch 550 - loss 1.1790045499801636\n",
            "epoch 21 - batch 560 - loss 1.1834298372268677\n",
            "epoch 21 - batch 570 - loss 1.1876057386398315\n",
            "epoch 21 - batch 580 - loss 1.22923743724823\n",
            "epoch 21 - batch 590 - loss 1.2530962228775024\n",
            "epoch 21 - batch 600 - loss 1.164559245109558\n",
            "epoch 21 - batch 610 - loss 1.1532323360443115\n",
            "epoch 21 training time: 220.83223271369934 sec\n",
            "evaluation of batch 0 took: 0.1517164707183838\n",
            "evaluation of batch 50 took: 0.15414810180664062\n",
            "evaluation of batch 100 took: 0.15784001350402832\n",
            "evaluation of batch 150 took: 0.1574702262878418\n",
            "evaluation of batch 200 took: 0.14863300323486328\n",
            "evaluation of batch 250 took: 0.1545238494873047\n",
            "evaluation of batch 300 took: 0.17094182968139648\n",
            "evaluation of batch 350 took: 0.1602487564086914\n",
            "evaluation of batch 400 took: 0.15522050857543945\n",
            "evaluation of batch 450 took: 0.15588116645812988\n",
            "evaluation of batch 500 took: 0.1576695442199707\n",
            "evaluation of batch 550 took: 0.1553184986114502\n",
            "evaluation of batch 600 took: 0.1530895233154297\n",
            "epoch 21 evaluation on training data time: 96.61398196220398 sec\n",
            "evaluation of batch 0 took: 0.16756677627563477\n",
            "evaluation of batch 50 took: 0.1521589756011963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 evaluation on test data time: 22.335967540740967 sec\n",
            "epoch evaluation:  {'epoch': 21, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3318821>, 'test_rouge_1_p': 0.5470421691403836, 'test_rouge_1_r': 0.5017425329506802, 'test_rouge_1_f1': 0.5139181671249701, 'test_rouge_2_p': 0.27473662405303034, 'test_rouge_2_r': 0.26095715807629866, 'test_rouge_2_f1': 0.26447100878919066, 'test_rouge_3_p': 0.11439076873647185, 'test_rouge_3_r': 0.1081919220102814, 'test_rouge_3_f1': 0.10975353623930635, 'test_rouge_L_p': 0.5451016361800403, 'test_rouge_L_r': 0.5002864160579004, 'test_rouge_L_f1': 0.512299015018171}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 21 saved checkpoint: models/checkpoints/baseline/ckpt-8\n",
            "epoch 22 - batch 10 - loss 1.213186264038086\n",
            "epoch 22 - batch 20 - loss 1.219605803489685\n",
            "epoch 22 - batch 30 - loss 1.1176245212554932\n",
            "epoch 22 - batch 40 - loss 1.18204665184021\n",
            "epoch 22 - batch 50 - loss 1.225081205368042\n",
            "epoch 22 - batch 60 - loss 1.1981958150863647\n",
            "epoch 22 - batch 70 - loss 1.2226017713546753\n",
            "epoch 22 - batch 80 - loss 1.0822656154632568\n",
            "epoch 22 - batch 90 - loss 1.1412725448608398\n",
            "epoch 22 - batch 100 - loss 1.1588475704193115\n",
            "epoch 22 - batch 110 - loss 1.178571105003357\n",
            "epoch 22 - batch 120 - loss 1.2721360921859741\n",
            "epoch 22 - batch 130 - loss 1.1898066997528076\n",
            "epoch 22 - batch 140 - loss 1.1617528200149536\n",
            "epoch 22 - batch 150 - loss 1.1471163034439087\n",
            "epoch 22 - batch 160 - loss 1.1487394571304321\n",
            "epoch 22 - batch 170 - loss 1.1847763061523438\n",
            "epoch 22 - batch 180 - loss 1.1284323930740356\n",
            "epoch 22 - batch 190 - loss 1.2001060247421265\n",
            "epoch 22 - batch 200 - loss 1.196055293083191\n",
            "epoch 22 - batch 210 - loss 1.1787383556365967\n",
            "epoch 22 - batch 220 - loss 1.2203716039657593\n",
            "epoch 22 - batch 230 - loss 1.1239200830459595\n",
            "epoch 22 - batch 240 - loss 1.1925055980682373\n",
            "epoch 22 - batch 250 - loss 1.1732118129730225\n",
            "epoch 22 - batch 260 - loss 1.1269237995147705\n",
            "epoch 22 - batch 270 - loss 1.1986747980117798\n",
            "epoch 22 - batch 280 - loss 1.2126286029815674\n",
            "epoch 22 - batch 290 - loss 1.1745942831039429\n",
            "epoch 22 - batch 300 - loss 1.1612873077392578\n",
            "epoch 22 - batch 310 - loss 1.1978750228881836\n",
            "epoch 22 - batch 320 - loss 1.1490174531936646\n",
            "epoch 22 - batch 330 - loss 1.1294986009597778\n",
            "epoch 22 - batch 340 - loss 1.1418111324310303\n",
            "epoch 22 - batch 350 - loss 1.202057957649231\n",
            "epoch 22 - batch 360 - loss 1.1796741485595703\n",
            "epoch 22 - batch 370 - loss 1.1316574811935425\n",
            "epoch 22 - batch 380 - loss 1.2385421991348267\n",
            "epoch 22 - batch 390 - loss 1.1586058139801025\n",
            "epoch 22 - batch 400 - loss 1.185710072517395\n",
            "epoch 22 - batch 410 - loss 1.1771677732467651\n",
            "epoch 22 - batch 420 - loss 1.1482797861099243\n",
            "epoch 22 - batch 430 - loss 1.1649960279464722\n",
            "epoch 22 - batch 440 - loss 1.1657651662826538\n",
            "epoch 22 - batch 450 - loss 1.1913384199142456\n",
            "epoch 22 - batch 460 - loss 1.1361733675003052\n",
            "epoch 22 - batch 470 - loss 1.2051531076431274\n",
            "epoch 22 - batch 480 - loss 1.2183659076690674\n",
            "epoch 22 - batch 490 - loss 1.1517592668533325\n",
            "epoch 22 - batch 500 - loss 1.1326204538345337\n",
            "epoch 22 - batch 510 - loss 1.1429014205932617\n",
            "epoch 22 - batch 520 - loss 1.1287972927093506\n",
            "epoch 22 - batch 530 - loss 1.2024755477905273\n",
            "epoch 22 - batch 540 - loss 1.213990569114685\n",
            "epoch 22 - batch 550 - loss 1.1352733373641968\n",
            "epoch 22 - batch 560 - loss 1.1432567834854126\n",
            "epoch 22 - batch 570 - loss 1.1351463794708252\n",
            "epoch 22 - batch 580 - loss 1.1727670431137085\n",
            "epoch 22 - batch 590 - loss 1.146992802619934\n",
            "epoch 22 - batch 600 - loss 1.1095637083053589\n",
            "epoch 22 - batch 610 - loss 1.1491209268569946\n",
            "epoch 22 training time: 219.97963047027588 sec\n",
            "evaluation of batch 0 took: 0.1564335823059082\n",
            "evaluation of batch 50 took: 0.15391135215759277\n",
            "evaluation of batch 100 took: 0.16328072547912598\n",
            "evaluation of batch 150 took: 0.15836453437805176\n",
            "evaluation of batch 200 took: 0.16026973724365234\n",
            "evaluation of batch 250 took: 0.15115070343017578\n",
            "evaluation of batch 300 took: 0.1574413776397705\n",
            "evaluation of batch 350 took: 0.15700268745422363\n",
            "evaluation of batch 400 took: 0.15524005889892578\n",
            "evaluation of batch 450 took: 0.147047758102417\n",
            "evaluation of batch 500 took: 0.1609175205230713\n",
            "evaluation of batch 550 took: 0.15350770950317383\n",
            "evaluation of batch 600 took: 0.15253281593322754\n",
            "epoch 22 evaluation on training data time: 96.93734645843506 sec\n",
            "evaluation of batch 0 took: 0.1684105396270752\n",
            "evaluation of batch 50 took: 0.16176152229309082\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 22 evaluation on test data time: 22.256251573562622 sec\n",
            "epoch evaluation:  {'epoch': 22, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3255581>, 'test_rouge_1_p': 0.5487067643108381, 'test_rouge_1_r': 0.5038918763528137, 'test_rouge_1_f1': 0.5157543889388704, 'test_rouge_2_p': 0.27770541632846335, 'test_rouge_2_r': 0.2636503145292208, 'test_rouge_2_f1': 0.26722356222533783, 'test_rouge_3_p': 0.11694082284902595, 'test_rouge_3_r': 0.110454291801948, 'test_rouge_3_f1': 0.11209139787543802, 'test_rouge_L_p': 0.5468476719523039, 'test_rouge_L_r': 0.5025035813331016, 'test_rouge_L_f1': 0.5142100248602633}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 23 - batch 10 - loss 1.1703126430511475\n",
            "epoch 23 - batch 20 - loss 1.153443455696106\n",
            "epoch 23 - batch 30 - loss 1.1627347469329834\n",
            "epoch 23 - batch 40 - loss 1.1066843271255493\n",
            "epoch 23 - batch 50 - loss 1.1782758235931396\n",
            "epoch 23 - batch 60 - loss 1.145196557044983\n",
            "epoch 23 - batch 70 - loss 1.2115336656570435\n",
            "epoch 23 - batch 80 - loss 1.0906869173049927\n",
            "epoch 23 - batch 90 - loss 1.1248596906661987\n",
            "epoch 23 - batch 100 - loss 1.227658987045288\n",
            "epoch 23 - batch 110 - loss 1.1095579862594604\n",
            "epoch 23 - batch 120 - loss 1.1794987916946411\n",
            "epoch 23 - batch 130 - loss 1.096166729927063\n",
            "epoch 23 - batch 140 - loss 1.1275217533111572\n",
            "epoch 23 - batch 150 - loss 1.1895313262939453\n",
            "epoch 23 - batch 160 - loss 1.2149715423583984\n",
            "epoch 23 - batch 170 - loss 1.1606991291046143\n",
            "epoch 23 - batch 180 - loss 1.1789809465408325\n",
            "epoch 23 - batch 190 - loss 1.1269291639328003\n",
            "epoch 23 - batch 200 - loss 1.1787588596343994\n",
            "epoch 23 - batch 210 - loss 1.189310073852539\n",
            "epoch 23 - batch 220 - loss 1.1574280261993408\n",
            "epoch 23 - batch 230 - loss 1.1143382787704468\n",
            "epoch 23 - batch 240 - loss 1.1341885328292847\n",
            "epoch 23 - batch 250 - loss 1.1513078212738037\n",
            "epoch 23 - batch 260 - loss 1.1182973384857178\n",
            "epoch 23 - batch 270 - loss 1.2220323085784912\n",
            "epoch 23 - batch 280 - loss 1.1656163930892944\n",
            "epoch 23 - batch 290 - loss 1.2247483730316162\n",
            "epoch 23 - batch 300 - loss 1.1911709308624268\n",
            "epoch 23 - batch 310 - loss 1.1379480361938477\n",
            "epoch 23 - batch 320 - loss 1.1340007781982422\n",
            "epoch 23 - batch 330 - loss 1.1449029445648193\n",
            "epoch 23 - batch 340 - loss 1.1327356100082397\n",
            "epoch 23 - batch 350 - loss 1.2459591627120972\n",
            "epoch 23 - batch 360 - loss 1.1403815746307373\n",
            "epoch 23 - batch 370 - loss 1.1649240255355835\n",
            "epoch 23 - batch 380 - loss 1.1090431213378906\n",
            "epoch 23 - batch 390 - loss 1.1484222412109375\n",
            "epoch 23 - batch 400 - loss 1.1466245651245117\n",
            "epoch 23 - batch 410 - loss 1.1508833169937134\n",
            "epoch 23 - batch 420 - loss 1.158072829246521\n",
            "epoch 23 - batch 430 - loss 1.1469601392745972\n",
            "epoch 23 - batch 440 - loss 1.139151692390442\n",
            "epoch 23 - batch 450 - loss 1.133108139038086\n",
            "epoch 23 - batch 460 - loss 1.0938973426818848\n",
            "epoch 23 - batch 470 - loss 1.1332039833068848\n",
            "epoch 23 - batch 480 - loss 1.1283667087554932\n",
            "epoch 23 - batch 490 - loss 1.1137316226959229\n",
            "epoch 23 - batch 500 - loss 1.1094205379486084\n",
            "epoch 23 - batch 510 - loss 1.1254351139068604\n",
            "epoch 23 - batch 520 - loss 1.1177282333374023\n",
            "epoch 23 - batch 530 - loss 1.1659401655197144\n",
            "epoch 23 - batch 540 - loss 1.1020748615264893\n",
            "epoch 23 - batch 550 - loss 1.0995123386383057\n",
            "epoch 23 - batch 560 - loss 1.1414988040924072\n",
            "epoch 23 - batch 570 - loss 1.1320650577545166\n",
            "epoch 23 - batch 580 - loss 1.2268863916397095\n",
            "epoch 23 - batch 590 - loss 1.1251522302627563\n",
            "epoch 23 - batch 600 - loss 1.1635897159576416\n",
            "epoch 23 - batch 610 - loss 1.1564496755599976\n",
            "epoch 23 training time: 220.18621587753296 sec\n",
            "evaluation of batch 0 took: 0.15907788276672363\n",
            "evaluation of batch 50 took: 0.1526174545288086\n",
            "evaluation of batch 100 took: 0.15980744361877441\n",
            "evaluation of batch 150 took: 0.15603303909301758\n",
            "evaluation of batch 200 took: 0.15274333953857422\n",
            "evaluation of batch 250 took: 0.15941572189331055\n",
            "evaluation of batch 300 took: 0.1573624610900879\n",
            "evaluation of batch 350 took: 0.15597915649414062\n",
            "evaluation of batch 400 took: 0.15825438499450684\n",
            "evaluation of batch 450 took: 0.1527388095855713\n",
            "evaluation of batch 500 took: 0.1726534366607666\n",
            "evaluation of batch 550 took: 0.15517425537109375\n",
            "evaluation of batch 600 took: 0.15752530097961426\n",
            "epoch 23 evaluation on training data time: 97.10722208023071 sec\n",
            "evaluation of batch 0 took: 0.17038631439208984\n",
            "evaluation of batch 50 took: 0.154435396194458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 23 evaluation on test data time: 22.379567861557007 sec\n",
            "epoch evaluation:  {'epoch': 23, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3195822>, 'test_rouge_1_p': 0.5504149333739178, 'test_rouge_1_r': 0.5067791374362245, 'test_rouge_1_f1': 0.5181364317810392, 'test_rouge_2_p': 0.2788001386634199, 'test_rouge_2_r': 0.26580467058982693, 'test_rouge_2_f1': 0.26898641941065343, 'test_rouge_3_p': 0.11758594595508658, 'test_rouge_3_r': 0.11168492965367963, 'test_rouge_3_f1': 0.11313049781939288, 'test_rouge_L_p': 0.548467304180195, 'test_rouge_L_r': 0.5052917065263605, 'test_rouge_L_f1': 0.5164960815643139}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 - batch 10 - loss 1.1533492803573608\n",
            "epoch 24 - batch 20 - loss 1.1982184648513794\n",
            "epoch 24 - batch 30 - loss 1.1127513647079468\n",
            "epoch 24 - batch 40 - loss 1.1234441995620728\n",
            "epoch 24 - batch 50 - loss 1.1604470014572144\n",
            "epoch 24 - batch 60 - loss 1.2233089208602905\n",
            "epoch 24 - batch 70 - loss 1.1166496276855469\n",
            "epoch 24 - batch 80 - loss 1.1888806819915771\n",
            "epoch 24 - batch 90 - loss 1.102886438369751\n",
            "epoch 24 - batch 100 - loss 1.1590287685394287\n",
            "epoch 24 - batch 110 - loss 1.1387251615524292\n",
            "epoch 24 - batch 120 - loss 1.1460952758789062\n",
            "epoch 24 - batch 130 - loss 1.1204768419265747\n",
            "epoch 24 - batch 140 - loss 1.17600417137146\n",
            "epoch 24 - batch 150 - loss 1.1186949014663696\n",
            "epoch 24 - batch 160 - loss 1.1185909509658813\n",
            "epoch 24 - batch 170 - loss 1.1790727376937866\n",
            "epoch 24 - batch 180 - loss 1.1969469785690308\n",
            "epoch 24 - batch 190 - loss 1.1530126333236694\n",
            "epoch 24 - batch 200 - loss 1.1702378988265991\n",
            "epoch 24 - batch 210 - loss 1.1677700281143188\n",
            "epoch 24 - batch 220 - loss 1.1970058679580688\n",
            "epoch 24 - batch 230 - loss 1.1339037418365479\n",
            "epoch 24 - batch 240 - loss 1.1557430028915405\n",
            "epoch 24 - batch 250 - loss 1.196512222290039\n",
            "epoch 24 - batch 260 - loss 1.1429489850997925\n",
            "epoch 24 - batch 270 - loss 1.2284297943115234\n",
            "epoch 24 - batch 280 - loss 1.2265806198120117\n",
            "epoch 24 - batch 290 - loss 1.1623504161834717\n",
            "epoch 24 - batch 300 - loss 1.0758640766143799\n",
            "epoch 24 - batch 310 - loss 1.1399983167648315\n",
            "epoch 24 - batch 320 - loss 1.1828279495239258\n",
            "epoch 24 - batch 330 - loss 1.2011547088623047\n",
            "epoch 24 - batch 340 - loss 1.123929738998413\n",
            "epoch 24 - batch 350 - loss 1.1449086666107178\n",
            "epoch 24 - batch 360 - loss 1.177251935005188\n",
            "epoch 24 - batch 370 - loss 1.2029587030410767\n",
            "epoch 24 - batch 380 - loss 1.08743155002594\n",
            "epoch 24 - batch 390 - loss 1.1764341592788696\n",
            "epoch 24 - batch 400 - loss 1.1269830465316772\n",
            "epoch 24 - batch 410 - loss 1.1277014017105103\n",
            "epoch 24 - batch 420 - loss 1.0997027158737183\n",
            "epoch 24 - batch 430 - loss 1.0942598581314087\n",
            "epoch 24 - batch 440 - loss 1.217193365097046\n",
            "epoch 24 - batch 450 - loss 1.1236311197280884\n",
            "epoch 24 - batch 460 - loss 1.0724292993545532\n",
            "epoch 24 - batch 470 - loss 1.115386724472046\n",
            "epoch 24 - batch 480 - loss 1.1339317560195923\n",
            "epoch 24 - batch 490 - loss 1.1342417001724243\n",
            "epoch 24 - batch 500 - loss 1.099339246749878\n",
            "epoch 24 - batch 510 - loss 1.1213070154190063\n",
            "epoch 24 - batch 520 - loss 1.1629170179367065\n",
            "epoch 24 - batch 530 - loss 1.1361885070800781\n",
            "epoch 24 - batch 540 - loss 1.1705650091171265\n",
            "epoch 24 - batch 550 - loss 1.09353506565094\n",
            "epoch 24 - batch 560 - loss 1.1374629735946655\n",
            "epoch 24 - batch 570 - loss 1.107715368270874\n",
            "epoch 24 - batch 580 - loss 1.129603385925293\n",
            "epoch 24 - batch 590 - loss 1.1414128541946411\n",
            "epoch 24 - batch 600 - loss 1.0852479934692383\n",
            "epoch 24 - batch 610 - loss 1.1680442094802856\n",
            "epoch 24 training time: 220.8882977962494 sec\n",
            "evaluation of batch 0 took: 0.15729522705078125\n",
            "evaluation of batch 50 took: 0.15261220932006836\n",
            "evaluation of batch 100 took: 0.1547553539276123\n",
            "evaluation of batch 150 took: 0.15860700607299805\n",
            "evaluation of batch 200 took: 0.156388521194458\n",
            "evaluation of batch 250 took: 0.15862345695495605\n",
            "evaluation of batch 300 took: 0.15586495399475098\n",
            "evaluation of batch 350 took: 0.15067315101623535\n",
            "evaluation of batch 400 took: 0.16044259071350098\n",
            "evaluation of batch 450 took: 0.1491222381591797\n",
            "evaluation of batch 500 took: 0.15460634231567383\n",
            "evaluation of batch 550 took: 0.16773581504821777\n",
            "evaluation of batch 600 took: 0.16471099853515625\n",
            "epoch 24 evaluation on training data time: 97.42928981781006 sec\n",
            "evaluation of batch 0 took: 0.16767311096191406\n",
            "evaluation of batch 50 took: 0.15475845336914062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 evaluation on test data time: 22.35595440864563 sec\n",
            "epoch evaluation:  {'epoch': 24, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3117524>, 'test_rouge_1_p': 0.5536307047677027, 'test_rouge_1_r': 0.5088655810335498, 'test_rouge_1_f1': 0.5206533840043884, 'test_rouge_2_p': 0.28204118472673156, 'test_rouge_2_r': 0.2680027817234848, 'test_rouge_2_f1': 0.2715859220539111, 'test_rouge_3_p': 0.11993751691017314, 'test_rouge_3_r': 0.11304366206709954, 'test_rouge_3_f1': 0.11481316070462273, 'test_rouge_L_p': 0.5516789990143784, 'test_rouge_L_r': 0.5073917688524274, 'test_rouge_L_f1': 0.5190209493451173}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 24 saved checkpoint: models/checkpoints/baseline/ckpt-9\n",
            "epoch 25 - batch 10 - loss 1.1197468042373657\n",
            "epoch 25 - batch 20 - loss 1.1762971878051758\n",
            "epoch 25 - batch 30 - loss 1.1341387033462524\n",
            "epoch 25 - batch 40 - loss 1.0991278886795044\n",
            "epoch 25 - batch 50 - loss 1.1774351596832275\n",
            "epoch 25 - batch 60 - loss 1.1329635381698608\n",
            "epoch 25 - batch 70 - loss 1.1651031970977783\n",
            "epoch 25 - batch 80 - loss 1.1651214361190796\n",
            "epoch 25 - batch 90 - loss 1.1320174932479858\n",
            "epoch 25 - batch 100 - loss 1.1334760189056396\n",
            "epoch 25 - batch 110 - loss 1.1540743112564087\n",
            "epoch 25 - batch 120 - loss 1.1528230905532837\n",
            "epoch 25 - batch 130 - loss 1.0927276611328125\n",
            "epoch 25 - batch 140 - loss 1.1337406635284424\n",
            "epoch 25 - batch 150 - loss 1.1243001222610474\n",
            "epoch 25 - batch 160 - loss 1.1113642454147339\n",
            "epoch 25 - batch 170 - loss 1.1300842761993408\n",
            "epoch 25 - batch 180 - loss 1.1648069620132446\n",
            "epoch 25 - batch 190 - loss 1.159936785697937\n",
            "epoch 25 - batch 200 - loss 1.1412990093231201\n",
            "epoch 25 - batch 210 - loss 1.1624090671539307\n",
            "epoch 25 - batch 220 - loss 1.1792205572128296\n",
            "epoch 25 - batch 230 - loss 1.1715363264083862\n",
            "epoch 25 - batch 240 - loss 1.1079858541488647\n",
            "epoch 25 - batch 250 - loss 1.1520130634307861\n",
            "epoch 25 - batch 260 - loss 1.1167123317718506\n",
            "epoch 25 - batch 270 - loss 1.1623834371566772\n",
            "epoch 25 - batch 280 - loss 1.1332749128341675\n",
            "epoch 25 - batch 290 - loss 1.184162974357605\n",
            "epoch 25 - batch 300 - loss 1.165284514427185\n",
            "epoch 25 - batch 310 - loss 1.1406269073486328\n",
            "epoch 25 - batch 320 - loss 1.196945309638977\n",
            "epoch 25 - batch 330 - loss 1.140296220779419\n",
            "epoch 25 - batch 340 - loss 1.1556552648544312\n",
            "epoch 25 - batch 350 - loss 1.2217519283294678\n",
            "epoch 25 - batch 360 - loss 1.0760043859481812\n",
            "epoch 25 - batch 370 - loss 1.1384875774383545\n",
            "epoch 25 - batch 380 - loss 1.0618667602539062\n",
            "epoch 25 - batch 390 - loss 1.1311618089675903\n",
            "epoch 25 - batch 400 - loss 1.1023331880569458\n",
            "epoch 25 - batch 410 - loss 1.12531316280365\n",
            "epoch 25 - batch 420 - loss 1.1868454217910767\n",
            "epoch 25 - batch 430 - loss 1.123031497001648\n",
            "epoch 25 - batch 440 - loss 1.1050870418548584\n",
            "epoch 25 - batch 450 - loss 1.195354700088501\n",
            "epoch 25 - batch 460 - loss 1.0734903812408447\n",
            "epoch 25 - batch 470 - loss 1.1303998231887817\n",
            "epoch 25 - batch 480 - loss 1.1370959281921387\n",
            "epoch 25 - batch 490 - loss 1.1124933958053589\n",
            "epoch 25 - batch 500 - loss 1.0884451866149902\n",
            "epoch 25 - batch 510 - loss 1.122275948524475\n",
            "epoch 25 - batch 520 - loss 1.1028592586517334\n",
            "epoch 25 - batch 530 - loss 1.1743017435073853\n",
            "epoch 25 - batch 540 - loss 1.093619704246521\n",
            "epoch 25 - batch 550 - loss 1.0885168313980103\n",
            "epoch 25 - batch 560 - loss 1.080660343170166\n",
            "epoch 25 - batch 570 - loss 1.135141134262085\n",
            "epoch 25 - batch 580 - loss 1.1642590761184692\n",
            "epoch 25 - batch 590 - loss 1.1599491834640503\n",
            "epoch 25 - batch 600 - loss 1.1103439331054688\n",
            "epoch 25 - batch 610 - loss 1.0842792987823486\n",
            "epoch 25 training time: 219.5521593093872 sec\n",
            "evaluation of batch 0 took: 0.14850711822509766\n",
            "evaluation of batch 50 took: 0.17833352088928223\n",
            "evaluation of batch 100 took: 0.15208864212036133\n",
            "evaluation of batch 150 took: 0.15288710594177246\n",
            "evaluation of batch 200 took: 0.1560053825378418\n",
            "evaluation of batch 250 took: 0.1554117202758789\n",
            "evaluation of batch 300 took: 0.16211724281311035\n",
            "evaluation of batch 350 took: 0.15494656562805176\n",
            "evaluation of batch 400 took: 0.14865779876708984\n",
            "evaluation of batch 450 took: 0.15451788902282715\n",
            "evaluation of batch 500 took: 0.15505194664001465\n",
            "evaluation of batch 550 took: 0.14822745323181152\n",
            "evaluation of batch 600 took: 0.16025853157043457\n",
            "epoch 25 evaluation on training data time: 96.49924039840698 sec\n",
            "evaluation of batch 0 took: 0.16121172904968262\n",
            "evaluation of batch 50 took: 0.15575790405273438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 25 evaluation on test data time: 22.181720972061157 sec\n",
            "epoch evaluation:  {'epoch': 25, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3071252>, 'test_rouge_1_p': 0.5535097668328696, 'test_rouge_1_r': 0.5102523360196352, 'test_rouge_1_f1': 0.5214476841334845, 'test_rouge_2_p': 0.28425599465638524, 'test_rouge_2_r': 0.2706069483901517, 'test_rouge_2_f1': 0.27392290066422575, 'test_rouge_3_p': 0.12178093716179653, 'test_rouge_3_r': 0.11523268398268398, 'test_rouge_3_f1': 0.11685377571956815, 'test_rouge_L_p': 0.5515734916125542, 'test_rouge_L_r': 0.5087838686611006, 'test_rouge_L_f1': 0.5198201683045939}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 26 - batch 10 - loss 1.0867855548858643\n",
            "epoch 26 - batch 20 - loss 1.0663796663284302\n",
            "epoch 26 - batch 30 - loss 1.080135703086853\n",
            "epoch 26 - batch 40 - loss 1.1506717205047607\n",
            "epoch 26 - batch 50 - loss 1.1286163330078125\n",
            "epoch 26 - batch 60 - loss 1.1088628768920898\n",
            "epoch 26 - batch 70 - loss 1.093036413192749\n",
            "epoch 26 - batch 80 - loss 1.1778725385665894\n",
            "epoch 26 - batch 90 - loss 1.0506926774978638\n",
            "epoch 26 - batch 100 - loss 1.15754234790802\n",
            "epoch 26 - batch 110 - loss 1.1501859426498413\n",
            "epoch 26 - batch 120 - loss 1.1740806102752686\n",
            "epoch 26 - batch 130 - loss 1.1667125225067139\n",
            "epoch 26 - batch 140 - loss 1.176586389541626\n",
            "epoch 26 - batch 150 - loss 1.0979326963424683\n",
            "epoch 26 - batch 160 - loss 1.077335238456726\n",
            "epoch 26 - batch 170 - loss 1.1136080026626587\n",
            "epoch 26 - batch 180 - loss 1.081807017326355\n",
            "epoch 26 - batch 190 - loss 1.1472499370574951\n",
            "epoch 26 - batch 200 - loss 1.1318002939224243\n",
            "epoch 26 - batch 210 - loss 1.1408008337020874\n",
            "epoch 26 - batch 220 - loss 1.1026322841644287\n",
            "epoch 26 - batch 230 - loss 1.1443312168121338\n",
            "epoch 26 - batch 240 - loss 1.1128780841827393\n",
            "epoch 26 - batch 250 - loss 1.1456880569458008\n",
            "epoch 26 - batch 260 - loss 1.114461064338684\n",
            "epoch 26 - batch 270 - loss 1.0919277667999268\n",
            "epoch 26 - batch 280 - loss 1.1476510763168335\n",
            "epoch 26 - batch 290 - loss 1.1346282958984375\n",
            "epoch 26 - batch 300 - loss 1.1122796535491943\n",
            "epoch 26 - batch 310 - loss 1.0827008485794067\n",
            "epoch 26 - batch 320 - loss 1.1021242141723633\n",
            "epoch 26 - batch 330 - loss 1.0934317111968994\n",
            "epoch 26 - batch 340 - loss 1.10164475440979\n",
            "epoch 26 - batch 350 - loss 1.1622947454452515\n",
            "epoch 26 - batch 360 - loss 1.16116201877594\n",
            "epoch 26 - batch 370 - loss 1.0729949474334717\n",
            "epoch 26 - batch 380 - loss 1.1435922384262085\n",
            "epoch 26 - batch 390 - loss 1.06013822555542\n",
            "epoch 26 - batch 400 - loss 1.044663667678833\n",
            "epoch 26 - batch 410 - loss 1.080239176750183\n",
            "epoch 26 - batch 420 - loss 1.1317970752716064\n",
            "epoch 26 - batch 430 - loss 1.102396011352539\n",
            "epoch 26 - batch 440 - loss 1.1293809413909912\n",
            "epoch 26 - batch 450 - loss 1.0940622091293335\n",
            "epoch 26 - batch 460 - loss 1.1091457605361938\n",
            "epoch 26 - batch 470 - loss 1.1188790798187256\n",
            "epoch 26 - batch 480 - loss 1.0884779691696167\n",
            "epoch 26 - batch 490 - loss 1.0843945741653442\n",
            "epoch 26 - batch 500 - loss 1.0967692136764526\n",
            "epoch 26 - batch 510 - loss 1.129604458808899\n",
            "epoch 26 - batch 520 - loss 1.0950030088424683\n",
            "epoch 26 - batch 530 - loss 1.1645177602767944\n",
            "epoch 26 - batch 540 - loss 1.0790830850601196\n",
            "epoch 26 - batch 550 - loss 1.0448973178863525\n",
            "epoch 26 - batch 560 - loss 1.0999183654785156\n",
            "epoch 26 - batch 570 - loss 1.1547834873199463\n",
            "epoch 26 - batch 580 - loss 1.1378296613693237\n",
            "epoch 26 - batch 590 - loss 1.1428680419921875\n",
            "epoch 26 - batch 600 - loss 1.0911206007003784\n",
            "epoch 26 - batch 610 - loss 1.0826637744903564\n",
            "epoch 26 training time: 218.43396639823914 sec\n",
            "evaluation of batch 0 took: 0.15692377090454102\n",
            "evaluation of batch 50 took: 0.16136717796325684\n",
            "evaluation of batch 100 took: 0.15833187103271484\n",
            "evaluation of batch 150 took: 0.1629951000213623\n",
            "evaluation of batch 200 took: 0.15952706336975098\n",
            "evaluation of batch 250 took: 0.1671607494354248\n",
            "evaluation of batch 300 took: 0.15500617027282715\n",
            "evaluation of batch 350 took: 0.157545804977417\n",
            "evaluation of batch 400 took: 0.15691065788269043\n",
            "evaluation of batch 450 took: 0.1523733139038086\n",
            "evaluation of batch 500 took: 0.1540839672088623\n",
            "evaluation of batch 550 took: 0.15073561668395996\n",
            "evaluation of batch 600 took: 0.15622329711914062\n",
            "epoch 26 evaluation on training data time: 96.4022741317749 sec\n",
            "evaluation of batch 0 took: 0.16561436653137207\n",
            "evaluation of batch 50 took: 0.15913796424865723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 26 evaluation on test data time: 22.194093227386475 sec\n",
            "epoch evaluation:  {'epoch': 26, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.3007345>, 'test_rouge_1_p': 0.5570568870303805, 'test_rouge_1_r': 0.5140679354128015, 'test_rouge_1_f1': 0.5251510029754247, 'test_rouge_2_p': 0.28617508793290053, 'test_rouge_2_r': 0.27258734104437243, 'test_rouge_2_f1': 0.2759158176442065, 'test_rouge_3_p': 0.12202550054112554, 'test_rouge_3_r': 0.11550134435876623, 'test_rouge_3_f1': 0.11708828358521442, 'test_rouge_L_p': 0.555159867569187, 'test_rouge_L_r': 0.5126084968788652, 'test_rouge_L_f1': 0.5235474732107495}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 - batch 10 - loss 1.147300362586975\n",
            "epoch 27 - batch 20 - loss 1.1297341585159302\n",
            "epoch 27 - batch 30 - loss 1.075102686882019\n",
            "epoch 27 - batch 40 - loss 1.1624603271484375\n",
            "epoch 27 - batch 50 - loss 1.145591139793396\n",
            "epoch 27 - batch 60 - loss 1.0795997381210327\n",
            "epoch 27 - batch 70 - loss 1.0395302772521973\n",
            "epoch 27 - batch 80 - loss 1.1897937059402466\n",
            "epoch 27 - batch 90 - loss 1.1473947763442993\n",
            "epoch 27 - batch 100 - loss 1.148962378501892\n",
            "epoch 27 - batch 110 - loss 1.139589548110962\n",
            "epoch 27 - batch 120 - loss 1.0898396968841553\n",
            "epoch 27 - batch 130 - loss 1.0890485048294067\n",
            "epoch 27 - batch 140 - loss 1.1055573225021362\n",
            "epoch 27 - batch 150 - loss 1.085634469985962\n",
            "epoch 27 - batch 160 - loss 1.0909560918807983\n",
            "epoch 27 - batch 170 - loss 1.0889313220977783\n",
            "epoch 27 - batch 180 - loss 1.1509398221969604\n",
            "epoch 27 - batch 190 - loss 1.1145923137664795\n",
            "epoch 27 - batch 200 - loss 1.0760587453842163\n",
            "epoch 27 - batch 210 - loss 1.1463985443115234\n",
            "epoch 27 - batch 220 - loss 1.0938336849212646\n",
            "epoch 27 - batch 230 - loss 1.1020324230194092\n",
            "epoch 27 - batch 240 - loss 1.122238278388977\n",
            "epoch 27 - batch 250 - loss 1.0997315645217896\n",
            "epoch 27 - batch 260 - loss 1.1264758110046387\n",
            "epoch 27 - batch 270 - loss 1.1717649698257446\n",
            "epoch 27 - batch 280 - loss 1.0769388675689697\n",
            "epoch 27 - batch 290 - loss 1.128661870956421\n",
            "epoch 27 - batch 300 - loss 1.1499779224395752\n",
            "epoch 27 - batch 310 - loss 1.086410403251648\n",
            "epoch 27 - batch 320 - loss 1.0862705707550049\n",
            "epoch 27 - batch 330 - loss 1.0600812435150146\n",
            "epoch 27 - batch 340 - loss 1.134365200996399\n",
            "epoch 27 - batch 350 - loss 1.0949243307113647\n",
            "epoch 27 - batch 360 - loss 1.0802756547927856\n",
            "epoch 27 - batch 370 - loss 1.1032295227050781\n",
            "epoch 27 - batch 380 - loss 1.0686713457107544\n",
            "epoch 27 - batch 390 - loss 1.1351968050003052\n",
            "epoch 27 - batch 400 - loss 1.0958805084228516\n",
            "epoch 27 - batch 410 - loss 1.1689245700836182\n",
            "epoch 27 - batch 420 - loss 1.016059160232544\n",
            "epoch 27 - batch 430 - loss 1.0811668634414673\n",
            "epoch 27 - batch 440 - loss 1.1320728063583374\n",
            "epoch 27 - batch 450 - loss 1.066977858543396\n",
            "epoch 27 - batch 460 - loss 1.063586711883545\n",
            "epoch 27 - batch 470 - loss 1.143503189086914\n",
            "epoch 27 - batch 480 - loss 1.108561635017395\n",
            "epoch 27 - batch 490 - loss 1.1004306077957153\n",
            "epoch 27 - batch 500 - loss 1.098789095878601\n",
            "epoch 27 - batch 510 - loss 1.139362096786499\n",
            "epoch 27 - batch 520 - loss 1.1044703722000122\n",
            "epoch 27 - batch 530 - loss 1.0939621925354004\n",
            "epoch 27 - batch 540 - loss 1.1217573881149292\n",
            "epoch 27 - batch 550 - loss 1.093428611755371\n",
            "epoch 27 - batch 560 - loss 1.0643423795700073\n",
            "epoch 27 - batch 570 - loss 1.0466536283493042\n",
            "epoch 27 - batch 580 - loss 1.083240032196045\n",
            "epoch 27 - batch 590 - loss 1.095266342163086\n",
            "epoch 27 - batch 600 - loss 1.0521714687347412\n",
            "epoch 27 - batch 610 - loss 1.0780847072601318\n",
            "epoch 27 training time: 218.61441612243652 sec\n",
            "evaluation of batch 0 took: 0.16369843482971191\n",
            "evaluation of batch 50 took: 0.1607973575592041\n",
            "evaluation of batch 100 took: 0.163407564163208\n",
            "evaluation of batch 150 took: 0.1552112102508545\n",
            "evaluation of batch 200 took: 0.15282678604125977\n",
            "evaluation of batch 250 took: 0.16217255592346191\n",
            "evaluation of batch 300 took: 0.15878868103027344\n",
            "evaluation of batch 350 took: 0.14931464195251465\n",
            "evaluation of batch 400 took: 0.1571958065032959\n",
            "evaluation of batch 450 took: 0.15291500091552734\n",
            "evaluation of batch 500 took: 0.15444421768188477\n",
            "evaluation of batch 550 took: 0.1652660369873047\n",
            "evaluation of batch 600 took: 0.15953564643859863\n",
            "epoch 27 evaluation on training data time: 96.63959693908691 sec\n",
            "evaluation of batch 0 took: 0.16762781143188477\n",
            "evaluation of batch 50 took: 0.15539884567260742\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 evaluation on test data time: 22.150665044784546 sec\n",
            "epoch evaluation:  {'epoch': 27, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2967826>, 'test_rouge_1_p': 0.5587876432533242, 'test_rouge_1_r': 0.5156577936572356, 'test_rouge_1_f1': 0.5268615594108946, 'test_rouge_2_p': 0.28704701873647187, 'test_rouge_2_r': 0.27327600784632045, 'test_rouge_2_f1': 0.27670097308896013, 'test_rouge_3_p': 0.12315869352002164, 'test_rouge_3_r': 0.11671845407196973, 'test_rouge_3_f1': 0.11830525238030816, 'test_rouge_L_p': 0.5568151923411409, 'test_rouge_L_r': 0.5141382938118427, 'test_rouge_L_f1': 0.525191376084233}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 27 saved checkpoint: models/checkpoints/baseline/ckpt-10\n",
            "epoch 28 - batch 10 - loss 1.1132419109344482\n",
            "epoch 28 - batch 20 - loss 1.1115883588790894\n",
            "epoch 28 - batch 30 - loss 1.0369033813476562\n",
            "epoch 28 - batch 40 - loss 1.0720959901809692\n",
            "epoch 28 - batch 50 - loss 1.0601972341537476\n",
            "epoch 28 - batch 60 - loss 1.0520087480545044\n",
            "epoch 28 - batch 70 - loss 1.0916792154312134\n",
            "epoch 28 - batch 80 - loss 1.0315459966659546\n",
            "epoch 28 - batch 90 - loss 1.1202396154403687\n",
            "epoch 28 - batch 100 - loss 1.16153085231781\n",
            "epoch 28 - batch 110 - loss 1.0668507814407349\n",
            "epoch 28 - batch 120 - loss 1.1037399768829346\n",
            "epoch 28 - batch 130 - loss 1.063071846961975\n",
            "epoch 28 - batch 140 - loss 1.1265565156936646\n",
            "epoch 28 - batch 150 - loss 1.1026567220687866\n",
            "epoch 28 - batch 160 - loss 1.104694128036499\n",
            "epoch 28 - batch 170 - loss 1.120240569114685\n",
            "epoch 28 - batch 180 - loss 1.0870616436004639\n",
            "epoch 28 - batch 190 - loss 1.1497457027435303\n",
            "epoch 28 - batch 200 - loss 1.12124502658844\n",
            "epoch 28 - batch 210 - loss 1.1648298501968384\n",
            "epoch 28 - batch 220 - loss 1.1209708452224731\n",
            "epoch 28 - batch 230 - loss 1.0861670970916748\n",
            "epoch 28 - batch 240 - loss 1.0801050662994385\n",
            "epoch 28 - batch 250 - loss 1.1073801517486572\n",
            "epoch 28 - batch 260 - loss 1.1130528450012207\n",
            "epoch 28 - batch 270 - loss 1.1048706769943237\n",
            "epoch 28 - batch 280 - loss 1.1578433513641357\n",
            "epoch 28 - batch 290 - loss 1.066938042640686\n",
            "epoch 28 - batch 300 - loss 1.1346147060394287\n",
            "epoch 28 - batch 310 - loss 1.1082651615142822\n",
            "epoch 28 - batch 320 - loss 1.102490782737732\n",
            "epoch 28 - batch 330 - loss 1.0917435884475708\n",
            "epoch 28 - batch 340 - loss 1.1193437576293945\n",
            "epoch 28 - batch 350 - loss 1.0778506994247437\n",
            "epoch 28 - batch 360 - loss 1.1095163822174072\n",
            "epoch 28 - batch 370 - loss 1.1006453037261963\n",
            "epoch 28 - batch 380 - loss 0.9878309369087219\n",
            "epoch 28 - batch 390 - loss 1.0658615827560425\n",
            "epoch 28 - batch 400 - loss 1.0939829349517822\n",
            "epoch 28 - batch 410 - loss 1.0671969652175903\n",
            "epoch 28 - batch 420 - loss 1.065907597541809\n",
            "epoch 28 - batch 430 - loss 1.0889261960983276\n",
            "epoch 28 - batch 440 - loss 1.1091516017913818\n",
            "epoch 28 - batch 450 - loss 1.0659152269363403\n",
            "epoch 28 - batch 460 - loss 1.1135886907577515\n",
            "epoch 28 - batch 470 - loss 0.9645131230354309\n",
            "epoch 28 - batch 480 - loss 1.0778576135635376\n",
            "epoch 28 - batch 490 - loss 1.0930732488632202\n",
            "epoch 28 - batch 500 - loss 1.0954028367996216\n",
            "epoch 28 - batch 510 - loss 1.0669339895248413\n",
            "epoch 28 - batch 520 - loss 1.0864423513412476\n",
            "epoch 28 - batch 530 - loss 1.093761920928955\n",
            "epoch 28 - batch 540 - loss 1.0520614385604858\n",
            "epoch 28 - batch 550 - loss 1.09699547290802\n",
            "epoch 28 - batch 560 - loss 1.112707257270813\n",
            "epoch 28 - batch 570 - loss 1.1314671039581299\n",
            "epoch 28 - batch 580 - loss 1.081732988357544\n",
            "epoch 28 - batch 590 - loss 1.038763165473938\n",
            "epoch 28 - batch 600 - loss 0.9813732504844666\n",
            "epoch 28 - batch 610 - loss 1.1188608407974243\n",
            "epoch 28 training time: 218.67248392105103 sec\n",
            "evaluation of batch 0 took: 0.15909886360168457\n",
            "evaluation of batch 50 took: 0.1542503833770752\n",
            "evaluation of batch 100 took: 0.15560460090637207\n",
            "evaluation of batch 150 took: 0.15194988250732422\n",
            "evaluation of batch 200 took: 0.15610766410827637\n",
            "evaluation of batch 250 took: 0.1542034149169922\n",
            "evaluation of batch 300 took: 0.15100717544555664\n",
            "evaluation of batch 350 took: 0.15292692184448242\n",
            "evaluation of batch 400 took: 0.15384697914123535\n",
            "evaluation of batch 450 took: 0.1626753807067871\n",
            "evaluation of batch 500 took: 0.15535926818847656\n",
            "evaluation of batch 550 took: 0.1585526466369629\n",
            "evaluation of batch 600 took: 0.15354132652282715\n",
            "epoch 28 evaluation on training data time: 96.36721706390381 sec\n",
            "evaluation of batch 0 took: 0.1620008945465088\n",
            "evaluation of batch 50 took: 0.15095186233520508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 28 evaluation on test data time: 22.055460453033447 sec\n",
            "epoch evaluation:  {'epoch': 28, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2935146>, 'test_rouge_1_p': 0.5596382551600185, 'test_rouge_1_r': 0.5163230277906619, 'test_rouge_1_f1': 0.5275866169420855, 'test_rouge_2_p': 0.28835396374458877, 'test_rouge_2_r': 0.27481250845508653, 'test_rouge_2_f1': 0.2781203012045901, 'test_rouge_3_p': 0.12424644040854983, 'test_rouge_3_r': 0.11764555431547621, 'test_rouge_3_f1': 0.11924478361420324, 'test_rouge_L_p': 0.5576645661815862, 'test_rouge_L_r': 0.5148327281907079, 'test_rouge_L_f1': 0.5259345432818441}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 29 - batch 10 - loss 1.1089807748794556\n",
            "epoch 29 - batch 20 - loss 1.0809876918792725\n",
            "epoch 29 - batch 30 - loss 1.0531611442565918\n",
            "epoch 29 - batch 40 - loss 1.0113111734390259\n",
            "epoch 29 - batch 50 - loss 1.1059010028839111\n",
            "epoch 29 - batch 60 - loss 1.0780761241912842\n",
            "epoch 29 - batch 70 - loss 1.094280481338501\n",
            "epoch 29 - batch 80 - loss 1.0573407411575317\n",
            "epoch 29 - batch 90 - loss 1.072959065437317\n",
            "epoch 29 - batch 100 - loss 1.1232120990753174\n",
            "epoch 29 - batch 110 - loss 1.0995341539382935\n",
            "epoch 29 - batch 120 - loss 1.1205474138259888\n",
            "epoch 29 - batch 130 - loss 1.0831021070480347\n",
            "epoch 29 - batch 140 - loss 1.0665626525878906\n",
            "epoch 29 - batch 150 - loss 1.1374551057815552\n",
            "epoch 29 - batch 160 - loss 1.07112717628479\n",
            "epoch 29 - batch 170 - loss 1.0502030849456787\n",
            "epoch 29 - batch 180 - loss 1.2141562700271606\n",
            "epoch 29 - batch 190 - loss 1.0733798742294312\n",
            "epoch 29 - batch 200 - loss 1.072177767753601\n",
            "epoch 29 - batch 210 - loss 1.1632856130599976\n",
            "epoch 29 - batch 220 - loss 1.0792869329452515\n",
            "epoch 29 - batch 230 - loss 1.045638084411621\n",
            "epoch 29 - batch 240 - loss 1.0982444286346436\n",
            "epoch 29 - batch 250 - loss 1.1129001379013062\n",
            "epoch 29 - batch 260 - loss 1.030007243156433\n",
            "epoch 29 - batch 270 - loss 1.0869489908218384\n",
            "epoch 29 - batch 280 - loss 1.0470584630966187\n",
            "epoch 29 - batch 290 - loss 1.0904871225357056\n",
            "epoch 29 - batch 300 - loss 1.0934536457061768\n",
            "epoch 29 - batch 310 - loss 1.0475318431854248\n",
            "epoch 29 - batch 320 - loss 1.1380587816238403\n",
            "epoch 29 - batch 330 - loss 1.0575908422470093\n",
            "epoch 29 - batch 340 - loss 1.103900671005249\n",
            "epoch 29 - batch 350 - loss 1.063073992729187\n",
            "epoch 29 - batch 360 - loss 1.1140135526657104\n",
            "epoch 29 - batch 370 - loss 1.0775896310806274\n",
            "epoch 29 - batch 380 - loss 1.048841118812561\n",
            "epoch 29 - batch 390 - loss 1.0242688655853271\n",
            "epoch 29 - batch 400 - loss 1.0306651592254639\n",
            "epoch 29 - batch 410 - loss 1.0916352272033691\n",
            "epoch 29 - batch 420 - loss 1.023744821548462\n",
            "epoch 29 - batch 430 - loss 1.0591460466384888\n",
            "epoch 29 - batch 440 - loss 1.111561894416809\n",
            "epoch 29 - batch 450 - loss 1.1235979795455933\n",
            "epoch 29 - batch 460 - loss 1.0293713808059692\n",
            "epoch 29 - batch 470 - loss 1.0798380374908447\n",
            "epoch 29 - batch 480 - loss 1.0445456504821777\n",
            "epoch 29 - batch 490 - loss 1.0835849046707153\n",
            "epoch 29 - batch 500 - loss 1.0246856212615967\n",
            "epoch 29 - batch 510 - loss 1.056015133857727\n",
            "epoch 29 - batch 520 - loss 1.0030803680419922\n",
            "epoch 29 - batch 530 - loss 1.0948703289031982\n",
            "epoch 29 - batch 540 - loss 1.0543357133865356\n",
            "epoch 29 - batch 550 - loss 0.9841474890708923\n",
            "epoch 29 - batch 560 - loss 1.057274580001831\n",
            "epoch 29 - batch 570 - loss 1.0966300964355469\n",
            "epoch 29 - batch 580 - loss 1.042413353919983\n",
            "epoch 29 - batch 590 - loss 1.0718523263931274\n",
            "epoch 29 - batch 600 - loss 1.0886032581329346\n",
            "epoch 29 - batch 610 - loss 1.0940864086151123\n",
            "epoch 29 training time: 218.5134494304657 sec\n",
            "evaluation of batch 0 took: 0.16085195541381836\n",
            "evaluation of batch 50 took: 0.16004443168640137\n",
            "evaluation of batch 100 took: 0.15731215476989746\n",
            "evaluation of batch 150 took: 0.15146374702453613\n",
            "evaluation of batch 200 took: 0.15505695343017578\n",
            "evaluation of batch 250 took: 0.15764856338500977\n",
            "evaluation of batch 300 took: 0.15036964416503906\n",
            "evaluation of batch 350 took: 0.15535306930541992\n",
            "evaluation of batch 400 took: 0.15723061561584473\n",
            "evaluation of batch 450 took: 0.14922809600830078\n",
            "evaluation of batch 500 took: 0.15721964836120605\n",
            "evaluation of batch 550 took: 0.16451430320739746\n",
            "evaluation of batch 600 took: 0.15259695053100586\n",
            "epoch 29 evaluation on training data time: 96.36687803268433 sec\n",
            "evaluation of batch 0 took: 0.16477537155151367\n",
            "evaluation of batch 50 took: 0.15381455421447754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 29 evaluation on test data time: 22.358375310897827 sec\n",
            "epoch evaluation:  {'epoch': 29, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.290091>, 'test_rouge_1_p': 0.5593813715116731, 'test_rouge_1_r': 0.5195944819689239, 'test_rouge_1_f1': 0.5293654472162483, 'test_rouge_2_p': 0.2896418848079004, 'test_rouge_2_r': 0.27767666903409105, 'test_rouge_2_f1': 0.28031804174192054, 'test_rouge_3_p': 0.1257195278679654, 'test_rouge_3_r': 0.12006561147186148, 'test_rouge_3_f1': 0.1212984134231344, 'test_rouge_L_p': 0.5574075315495516, 'test_rouge_L_r': 0.518070241235699, 'test_rouge_L_f1': 0.5276913776609393}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 - batch 10 - loss 1.055533766746521\n",
            "epoch 30 - batch 20 - loss 1.1132229566574097\n",
            "epoch 30 - batch 30 - loss 0.9982962608337402\n",
            "epoch 30 - batch 40 - loss 1.0553390979766846\n",
            "epoch 30 - batch 50 - loss 1.0322762727737427\n",
            "epoch 30 - batch 60 - loss 1.0625249147415161\n",
            "epoch 30 - batch 70 - loss 1.143920660018921\n",
            "epoch 30 - batch 80 - loss 1.0688869953155518\n",
            "epoch 30 - batch 90 - loss 1.0211657285690308\n",
            "epoch 30 - batch 100 - loss 1.0670545101165771\n",
            "epoch 30 - batch 110 - loss 1.089206337928772\n",
            "epoch 30 - batch 120 - loss 1.12930166721344\n",
            "epoch 30 - batch 130 - loss 1.04274320602417\n",
            "epoch 30 - batch 140 - loss 1.0428160429000854\n",
            "epoch 30 - batch 150 - loss 1.0960959196090698\n",
            "epoch 30 - batch 160 - loss 1.0640040636062622\n",
            "epoch 30 - batch 170 - loss 1.0500918626785278\n",
            "epoch 30 - batch 180 - loss 1.0897769927978516\n",
            "epoch 30 - batch 190 - loss 1.0296815633773804\n",
            "epoch 30 - batch 200 - loss 1.0909487009048462\n",
            "epoch 30 - batch 210 - loss 1.0696910619735718\n",
            "epoch 30 - batch 220 - loss 1.085727334022522\n",
            "epoch 30 - batch 230 - loss 1.0184577703475952\n",
            "epoch 30 - batch 240 - loss 1.1403638124465942\n",
            "epoch 30 - batch 250 - loss 1.061251163482666\n",
            "epoch 30 - batch 260 - loss 1.0609170198440552\n",
            "epoch 30 - batch 270 - loss 1.0635868310928345\n",
            "epoch 30 - batch 280 - loss 1.078731656074524\n",
            "epoch 30 - batch 290 - loss 1.1051210165023804\n",
            "epoch 30 - batch 300 - loss 1.0875917673110962\n",
            "epoch 30 - batch 310 - loss 1.064961314201355\n",
            "epoch 30 - batch 320 - loss 1.0320682525634766\n",
            "epoch 30 - batch 330 - loss 1.0416938066482544\n",
            "epoch 30 - batch 340 - loss 1.025030255317688\n",
            "epoch 30 - batch 350 - loss 1.0550764799118042\n",
            "epoch 30 - batch 360 - loss 1.0479642152786255\n",
            "epoch 30 - batch 370 - loss 1.0530612468719482\n",
            "epoch 30 - batch 380 - loss 1.0804253816604614\n",
            "epoch 30 - batch 390 - loss 1.0154567956924438\n",
            "epoch 30 - batch 400 - loss 1.073632836341858\n",
            "epoch 30 - batch 410 - loss 1.0112634897232056\n",
            "epoch 30 - batch 420 - loss 1.0705677270889282\n",
            "epoch 30 - batch 430 - loss 1.0691673755645752\n",
            "epoch 30 - batch 440 - loss 1.074668049812317\n",
            "epoch 30 - batch 450 - loss 1.1122856140136719\n",
            "epoch 30 - batch 460 - loss 1.064833402633667\n",
            "epoch 30 - batch 470 - loss 1.0696552991867065\n",
            "epoch 30 - batch 480 - loss 1.0766888856887817\n",
            "epoch 30 - batch 490 - loss 1.0721360445022583\n",
            "epoch 30 - batch 500 - loss 1.0451526641845703\n",
            "epoch 30 - batch 510 - loss 1.0766940116882324\n",
            "epoch 30 - batch 520 - loss 1.050658106803894\n",
            "epoch 30 - batch 530 - loss 1.0088022947311401\n",
            "epoch 30 - batch 540 - loss 1.0927928686141968\n",
            "epoch 30 - batch 550 - loss 1.0020829439163208\n",
            "epoch 30 - batch 560 - loss 1.0839694738388062\n",
            "epoch 30 - batch 570 - loss 1.0557787418365479\n",
            "epoch 30 - batch 580 - loss 1.0110868215560913\n",
            "epoch 30 - batch 590 - loss 1.123552680015564\n",
            "epoch 30 - batch 600 - loss 0.9920401573181152\n",
            "epoch 30 - batch 610 - loss 1.0737088918685913\n",
            "epoch 30 training time: 219.33998107910156 sec\n",
            "evaluation of batch 0 took: 0.16101527214050293\n",
            "evaluation of batch 50 took: 0.16003894805908203\n",
            "evaluation of batch 100 took: 0.15461277961730957\n",
            "evaluation of batch 150 took: 0.15718603134155273\n",
            "evaluation of batch 200 took: 0.14937710762023926\n",
            "evaluation of batch 250 took: 0.15926742553710938\n",
            "evaluation of batch 300 took: 0.15839886665344238\n",
            "evaluation of batch 350 took: 0.15423178672790527\n",
            "evaluation of batch 400 took: 0.15497827529907227\n",
            "evaluation of batch 450 took: 0.15747594833374023\n",
            "evaluation of batch 500 took: 0.15511035919189453\n",
            "evaluation of batch 550 took: 0.1534414291381836\n",
            "evaluation of batch 600 took: 0.16417622566223145\n",
            "epoch 30 evaluation on training data time: 96.83660316467285 sec\n",
            "evaluation of batch 0 took: 0.16909289360046387\n",
            "evaluation of batch 50 took: 0.15977144241333008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 evaluation on test data time: 22.196547985076904 sec\n",
            "epoch evaluation:  {'epoch': 30, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2849343>, 'test_rouge_1_p': 0.5613983022186146, 'test_rouge_1_r': 0.5198207763218924, 'test_rouge_1_f1': 0.5303433523096672, 'test_rouge_2_p': 0.2904590689258658, 'test_rouge_2_r': 0.27750946969696977, 'test_rouge_2_f1': 0.2805656211348176, 'test_rouge_3_p': 0.12551195549242422, 'test_rouge_3_r': 0.11934714048971863, 'test_rouge_3_f1': 0.12081285994511444, 'test_rouge_L_p': 0.5593518692988558, 'test_rouge_L_r': 0.5182608430446044, 'test_rouge_L_f1': 0.5286232017582212}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 30 saved checkpoint: models/checkpoints/baseline/ckpt-11\n",
            "epoch 31 - batch 10 - loss 1.0952955484390259\n",
            "epoch 31 - batch 20 - loss 1.1054432392120361\n",
            "epoch 31 - batch 30 - loss 1.0514734983444214\n",
            "epoch 31 - batch 40 - loss 1.1231015920639038\n",
            "epoch 31 - batch 50 - loss 1.0557152032852173\n",
            "epoch 31 - batch 60 - loss 1.1373326778411865\n",
            "epoch 31 - batch 70 - loss 1.09940505027771\n",
            "epoch 31 - batch 80 - loss 1.08151113986969\n",
            "epoch 31 - batch 90 - loss 1.0863428115844727\n",
            "epoch 31 - batch 100 - loss 1.1016137599945068\n",
            "epoch 31 - batch 110 - loss 1.0826696157455444\n",
            "epoch 31 - batch 120 - loss 1.0522922277450562\n",
            "epoch 31 - batch 130 - loss 1.0346364974975586\n",
            "epoch 31 - batch 140 - loss 1.0620640516281128\n",
            "epoch 31 - batch 150 - loss 1.082657814025879\n",
            "epoch 31 - batch 160 - loss 1.131685733795166\n",
            "epoch 31 - batch 170 - loss 1.125875473022461\n",
            "epoch 31 - batch 180 - loss 1.0919415950775146\n",
            "epoch 31 - batch 190 - loss 1.0698928833007812\n",
            "epoch 31 - batch 200 - loss 1.051834225654602\n",
            "epoch 31 - batch 210 - loss 1.0530544519424438\n",
            "epoch 31 - batch 220 - loss 1.0990532636642456\n",
            "epoch 31 - batch 230 - loss 1.0941407680511475\n",
            "epoch 31 - batch 240 - loss 1.0900241136550903\n",
            "epoch 31 - batch 250 - loss 1.1505929231643677\n",
            "epoch 31 - batch 260 - loss 1.0702570676803589\n",
            "epoch 31 - batch 270 - loss 1.0441038608551025\n",
            "epoch 31 - batch 280 - loss 1.075326681137085\n",
            "epoch 31 - batch 290 - loss 1.1090848445892334\n",
            "epoch 31 - batch 300 - loss 1.0325298309326172\n",
            "epoch 31 - batch 310 - loss 1.0432674884796143\n",
            "epoch 31 - batch 320 - loss 1.055091142654419\n",
            "epoch 31 - batch 330 - loss 1.0660207271575928\n",
            "epoch 31 - batch 340 - loss 1.0161845684051514\n",
            "epoch 31 - batch 350 - loss 1.0696628093719482\n",
            "epoch 31 - batch 360 - loss 1.032028317451477\n",
            "epoch 31 - batch 370 - loss 1.0285671949386597\n",
            "epoch 31 - batch 380 - loss 1.015194296836853\n",
            "epoch 31 - batch 390 - loss 1.0401331186294556\n",
            "epoch 31 - batch 400 - loss 1.074017882347107\n",
            "epoch 31 - batch 410 - loss 1.0638859272003174\n",
            "epoch 31 - batch 420 - loss 1.1083714962005615\n",
            "epoch 31 - batch 430 - loss 1.0617504119873047\n",
            "epoch 31 - batch 440 - loss 1.0293840169906616\n",
            "epoch 31 - batch 450 - loss 1.090476632118225\n",
            "epoch 31 - batch 460 - loss 1.054608941078186\n",
            "epoch 31 - batch 470 - loss 1.044978141784668\n",
            "epoch 31 - batch 480 - loss 1.1036981344223022\n",
            "epoch 31 - batch 490 - loss 1.0478123426437378\n",
            "epoch 31 - batch 500 - loss 1.072778582572937\n",
            "epoch 31 - batch 510 - loss 1.1026360988616943\n",
            "epoch 31 - batch 520 - loss 0.9983487129211426\n",
            "epoch 31 - batch 530 - loss 1.0053632259368896\n",
            "epoch 31 - batch 540 - loss 1.0373915433883667\n",
            "epoch 31 - batch 550 - loss 1.0836237668991089\n",
            "epoch 31 - batch 560 - loss 1.136165738105774\n",
            "epoch 31 - batch 570 - loss 1.0299981832504272\n",
            "epoch 31 - batch 580 - loss 1.0280182361602783\n",
            "epoch 31 - batch 590 - loss 1.0685993432998657\n",
            "epoch 31 - batch 600 - loss 1.022275447845459\n",
            "epoch 31 - batch 610 - loss 1.0348371267318726\n",
            "epoch 31 training time: 219.84315133094788 sec\n",
            "evaluation of batch 0 took: 0.16217589378356934\n",
            "evaluation of batch 50 took: 0.15198230743408203\n",
            "evaluation of batch 100 took: 0.1606001853942871\n",
            "evaluation of batch 150 took: 0.1615002155303955\n",
            "evaluation of batch 200 took: 0.1534416675567627\n",
            "evaluation of batch 250 took: 0.15025591850280762\n",
            "evaluation of batch 300 took: 0.1511073112487793\n",
            "evaluation of batch 350 took: 0.16896891593933105\n",
            "evaluation of batch 400 took: 0.1529073715209961\n",
            "evaluation of batch 450 took: 0.15709614753723145\n",
            "evaluation of batch 500 took: 0.15337228775024414\n",
            "evaluation of batch 550 took: 0.15413427352905273\n",
            "evaluation of batch 600 took: 0.15679693222045898\n",
            "epoch 31 evaluation on training data time: 97.8092725276947 sec\n",
            "evaluation of batch 0 took: 0.16707062721252441\n",
            "evaluation of batch 50 took: 0.15672874450683594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 31 evaluation on test data time: 22.272334337234497 sec\n",
            "epoch evaluation:  {'epoch': 31, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2852743>, 'test_rouge_1_p': 0.5638847318046536, 'test_rouge_1_r': 0.5215736667536333, 'test_rouge_1_f1': 0.5325223078717143, 'test_rouge_2_p': 0.29344434862012997, 'test_rouge_2_r': 0.2797864245129871, 'test_rouge_2_f1': 0.28313399643173387, 'test_rouge_3_p': 0.1276253043831169, 'test_rouge_3_r': 0.12095952550054116, 'test_rouge_3_f1': 0.12258681763425075, 'test_rouge_L_p': 0.5619298251971246, 'test_rouge_L_r': 0.5201024514919603, 'test_rouge_L_f1': 0.5308896174904191}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 32 - batch 10 - loss 1.0134146213531494\n",
            "epoch 32 - batch 20 - loss 1.073593258857727\n",
            "epoch 32 - batch 30 - loss 1.0917458534240723\n",
            "epoch 32 - batch 40 - loss 1.0408731698989868\n",
            "epoch 32 - batch 50 - loss 1.1240110397338867\n",
            "epoch 32 - batch 60 - loss 1.0033923387527466\n",
            "epoch 32 - batch 70 - loss 1.0852415561676025\n",
            "epoch 32 - batch 80 - loss 1.0639744997024536\n",
            "epoch 32 - batch 90 - loss 1.082862377166748\n",
            "epoch 32 - batch 100 - loss 1.0470902919769287\n",
            "epoch 32 - batch 110 - loss 1.0458892583847046\n",
            "epoch 32 - batch 120 - loss 1.0728601217269897\n",
            "epoch 32 - batch 130 - loss 1.0783870220184326\n",
            "epoch 32 - batch 140 - loss 1.098183274269104\n",
            "epoch 32 - batch 150 - loss 1.0684105157852173\n",
            "epoch 32 - batch 160 - loss 1.0327707529067993\n",
            "epoch 32 - batch 170 - loss 1.0695525407791138\n",
            "epoch 32 - batch 180 - loss 1.092288613319397\n",
            "epoch 32 - batch 190 - loss 1.0276926755905151\n",
            "epoch 32 - batch 200 - loss 1.0207087993621826\n",
            "epoch 32 - batch 210 - loss 1.1000226736068726\n",
            "epoch 32 - batch 220 - loss 1.1019728183746338\n",
            "epoch 32 - batch 230 - loss 1.0550167560577393\n",
            "epoch 32 - batch 240 - loss 1.0284501314163208\n",
            "epoch 32 - batch 250 - loss 1.050241231918335\n",
            "epoch 32 - batch 260 - loss 1.0442997217178345\n",
            "epoch 32 - batch 270 - loss 1.0717097520828247\n",
            "epoch 32 - batch 280 - loss 1.0900592803955078\n",
            "epoch 32 - batch 290 - loss 1.0536073446273804\n",
            "epoch 32 - batch 300 - loss 1.1086889505386353\n",
            "epoch 32 - batch 310 - loss 1.0783717632293701\n",
            "epoch 32 - batch 320 - loss 1.0461255311965942\n",
            "epoch 32 - batch 330 - loss 1.076337218284607\n",
            "epoch 32 - batch 340 - loss 1.0285803079605103\n",
            "epoch 32 - batch 350 - loss 1.0558847188949585\n",
            "epoch 32 - batch 360 - loss 1.063598871231079\n",
            "epoch 32 - batch 370 - loss 1.047424554824829\n",
            "epoch 32 - batch 380 - loss 1.0020606517791748\n",
            "epoch 32 - batch 390 - loss 1.0309784412384033\n",
            "epoch 32 - batch 400 - loss 1.0547937154769897\n",
            "epoch 32 - batch 410 - loss 1.054668664932251\n",
            "epoch 32 - batch 420 - loss 0.9790223836898804\n",
            "epoch 32 - batch 430 - loss 1.0710771083831787\n",
            "epoch 32 - batch 440 - loss 1.071918249130249\n",
            "epoch 32 - batch 450 - loss 1.075859785079956\n",
            "epoch 32 - batch 460 - loss 1.0155214071273804\n",
            "epoch 32 - batch 470 - loss 1.0166441202163696\n",
            "epoch 32 - batch 480 - loss 1.050554871559143\n",
            "epoch 32 - batch 490 - loss 1.0768426656723022\n",
            "epoch 32 - batch 500 - loss 1.0577319860458374\n",
            "epoch 32 - batch 510 - loss 1.0702401399612427\n",
            "epoch 32 - batch 520 - loss 1.058708906173706\n",
            "epoch 32 - batch 530 - loss 1.0839518308639526\n",
            "epoch 32 - batch 540 - loss 1.0210514068603516\n",
            "epoch 32 - batch 550 - loss 1.0856271982192993\n",
            "epoch 32 - batch 560 - loss 1.065751075744629\n",
            "epoch 32 - batch 570 - loss 1.005630612373352\n",
            "epoch 32 - batch 580 - loss 1.0925246477127075\n",
            "epoch 32 - batch 590 - loss 1.0171793699264526\n",
            "epoch 32 - batch 600 - loss 0.986646294593811\n",
            "epoch 32 - batch 610 - loss 1.0822690725326538\n",
            "epoch 32 training time: 219.6311068534851 sec\n",
            "evaluation of batch 0 took: 0.16029691696166992\n",
            "evaluation of batch 50 took: 0.16833806037902832\n",
            "evaluation of batch 100 took: 0.1619582176208496\n",
            "evaluation of batch 150 took: 0.15708160400390625\n",
            "evaluation of batch 200 took: 0.14662718772888184\n",
            "evaluation of batch 250 took: 0.16121721267700195\n",
            "evaluation of batch 300 took: 0.1629033088684082\n",
            "evaluation of batch 350 took: 0.1503448486328125\n",
            "evaluation of batch 400 took: 0.15604829788208008\n",
            "evaluation of batch 450 took: 0.1579151153564453\n",
            "evaluation of batch 500 took: 0.15515851974487305\n",
            "evaluation of batch 550 took: 0.16296625137329102\n",
            "evaluation of batch 600 took: 0.1560964584350586\n",
            "epoch 32 evaluation on training data time: 97.64657616615295 sec\n",
            "evaluation of batch 0 took: 0.16695666313171387\n",
            "evaluation of batch 50 took: 0.15775752067565918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 32 evaluation on test data time: 22.562070608139038 sec\n",
            "epoch evaluation:  {'epoch': 32, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2803031>, 'test_rouge_1_p': 0.5640611713435372, 'test_rouge_1_r': 0.5246951639320502, 'test_rouge_1_f1': 0.5342751319973315, 'test_rouge_2_p': 0.2937800155573594, 'test_rouge_2_r': 0.281585455560065, 'test_rouge_2_f1': 0.28432387326271763, 'test_rouge_3_p': 0.1281004802489178, 'test_rouge_3_r': 0.12210307596049784, 'test_rouge_3_f1': 0.12348005726911981, 'test_rouge_L_p': 0.5621099185412803, 'test_rouge_L_r': 0.5231784421865338, 'test_rouge_L_f1': 0.5326154657228368}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 - batch 10 - loss 1.0949970483779907\n",
            "epoch 33 - batch 20 - loss 1.070420503616333\n",
            "epoch 33 - batch 30 - loss 1.0555157661437988\n",
            "epoch 33 - batch 40 - loss 1.0639053583145142\n",
            "epoch 33 - batch 50 - loss 1.0515886545181274\n",
            "epoch 33 - batch 60 - loss 1.0262858867645264\n",
            "epoch 33 - batch 70 - loss 1.0855735540390015\n",
            "epoch 33 - batch 80 - loss 1.0460331439971924\n",
            "epoch 33 - batch 90 - loss 1.042423963546753\n",
            "epoch 33 - batch 100 - loss 1.086756944656372\n",
            "epoch 33 - batch 110 - loss 1.043962836265564\n",
            "epoch 33 - batch 120 - loss 1.0748112201690674\n",
            "epoch 33 - batch 130 - loss 1.0439178943634033\n",
            "epoch 33 - batch 140 - loss 1.0365198850631714\n",
            "epoch 33 - batch 150 - loss 1.0509719848632812\n",
            "epoch 33 - batch 160 - loss 1.080185890197754\n",
            "epoch 33 - batch 170 - loss 1.0779439210891724\n",
            "epoch 33 - batch 180 - loss 1.046323537826538\n",
            "epoch 33 - batch 190 - loss 1.0663138628005981\n",
            "epoch 33 - batch 200 - loss 1.0294684171676636\n",
            "epoch 33 - batch 210 - loss 1.0555000305175781\n",
            "epoch 33 - batch 220 - loss 1.1193469762802124\n",
            "epoch 33 - batch 230 - loss 1.09964120388031\n",
            "epoch 33 - batch 240 - loss 1.079797387123108\n",
            "epoch 33 - batch 250 - loss 1.088360071182251\n",
            "epoch 33 - batch 260 - loss 1.0370014905929565\n",
            "epoch 33 - batch 270 - loss 1.0324839353561401\n",
            "epoch 33 - batch 280 - loss 1.0550066232681274\n",
            "epoch 33 - batch 290 - loss 1.0742835998535156\n",
            "epoch 33 - batch 300 - loss 1.0114378929138184\n",
            "epoch 33 - batch 310 - loss 1.0602495670318604\n",
            "epoch 33 - batch 320 - loss 1.0594063997268677\n",
            "epoch 33 - batch 330 - loss 1.1053745746612549\n",
            "epoch 33 - batch 340 - loss 1.0934454202651978\n",
            "epoch 33 - batch 350 - loss 1.0189863443374634\n",
            "epoch 33 - batch 360 - loss 0.9834624528884888\n",
            "epoch 33 - batch 370 - loss 0.9952929615974426\n",
            "epoch 33 - batch 380 - loss 1.0349502563476562\n",
            "epoch 33 - batch 390 - loss 1.0271899700164795\n",
            "epoch 33 - batch 400 - loss 1.052590012550354\n",
            "epoch 33 - batch 410 - loss 1.0617567300796509\n",
            "epoch 33 - batch 420 - loss 1.0617202520370483\n",
            "epoch 33 - batch 430 - loss 1.0452154874801636\n",
            "epoch 33 - batch 440 - loss 1.0355408191680908\n",
            "epoch 33 - batch 450 - loss 1.0537998676300049\n",
            "epoch 33 - batch 460 - loss 0.996898353099823\n",
            "epoch 33 - batch 470 - loss 0.9810972809791565\n",
            "epoch 33 - batch 480 - loss 1.0676429271697998\n",
            "epoch 33 - batch 490 - loss 1.022532343864441\n",
            "epoch 33 - batch 500 - loss 1.0311534404754639\n",
            "epoch 33 - batch 510 - loss 1.0434602499008179\n",
            "epoch 33 - batch 520 - loss 1.0259629487991333\n",
            "epoch 33 - batch 530 - loss 0.9833386540412903\n",
            "epoch 33 - batch 540 - loss 1.0479724407196045\n",
            "epoch 33 - batch 550 - loss 1.057196855545044\n",
            "epoch 33 - batch 560 - loss 1.0533316135406494\n",
            "epoch 33 - batch 570 - loss 1.0375875234603882\n",
            "epoch 33 - batch 580 - loss 1.0390251874923706\n",
            "epoch 33 - batch 590 - loss 1.0746409893035889\n",
            "epoch 33 - batch 600 - loss 1.0343459844589233\n",
            "epoch 33 - batch 610 - loss 1.0475800037384033\n",
            "epoch 33 training time: 219.45314741134644 sec\n",
            "evaluation of batch 0 took: 0.15629935264587402\n",
            "evaluation of batch 50 took: 0.1596219539642334\n",
            "evaluation of batch 100 took: 0.15355420112609863\n",
            "evaluation of batch 150 took: 0.15666937828063965\n",
            "evaluation of batch 200 took: 0.15722298622131348\n",
            "evaluation of batch 250 took: 0.1515028476715088\n",
            "evaluation of batch 300 took: 0.15596365928649902\n",
            "evaluation of batch 350 took: 0.16533112525939941\n",
            "evaluation of batch 400 took: 0.1565098762512207\n",
            "evaluation of batch 450 took: 0.15003395080566406\n",
            "evaluation of batch 500 took: 0.1562504768371582\n",
            "evaluation of batch 550 took: 0.16155505180358887\n",
            "evaluation of batch 600 took: 0.1551530361175537\n",
            "epoch 33 evaluation on training data time: 96.88296747207642 sec\n",
            "evaluation of batch 0 took: 0.16298770904541016\n",
            "evaluation of batch 50 took: 0.15268445014953613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 evaluation on test data time: 22.130075454711914 sec\n",
            "epoch evaluation:  {'epoch': 33, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2792995>, 'test_rouge_1_p': 0.5661116506261596, 'test_rouge_1_r': 0.522967970924165, 'test_rouge_1_f1': 0.5342234114612614, 'test_rouge_2_p': 0.29568071902056264, 'test_rouge_2_r': 0.2816486573322511, 'test_rouge_2_f1': 0.2851347508377491, 'test_rouge_3_p': 0.12927150974025975, 'test_rouge_3_r': 0.12234108664772728, 'test_rouge_3_f1': 0.12402232022070193, 'test_rouge_L_p': 0.5642499009547, 'test_rouge_L_r': 0.5215829673488712, 'test_rouge_L_f1': 0.5326784868777057}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 33 saved checkpoint: models/checkpoints/baseline/ckpt-12\n",
            "epoch 34 - batch 10 - loss 1.059340238571167\n",
            "epoch 34 - batch 20 - loss 1.008583903312683\n",
            "epoch 34 - batch 30 - loss 0.9886077046394348\n",
            "epoch 34 - batch 40 - loss 1.0056084394454956\n",
            "epoch 34 - batch 50 - loss 1.0478770732879639\n",
            "epoch 34 - batch 60 - loss 0.994658887386322\n",
            "epoch 34 - batch 70 - loss 1.050538420677185\n",
            "epoch 34 - batch 80 - loss 1.0494921207427979\n",
            "epoch 34 - batch 90 - loss 1.0313465595245361\n",
            "epoch 34 - batch 100 - loss 1.0282872915267944\n",
            "epoch 34 - batch 110 - loss 1.0902835130691528\n",
            "epoch 34 - batch 120 - loss 0.9988900423049927\n",
            "epoch 34 - batch 130 - loss 0.9954032897949219\n",
            "epoch 34 - batch 140 - loss 1.0283724069595337\n",
            "epoch 34 - batch 150 - loss 1.0844895839691162\n",
            "epoch 34 - batch 160 - loss 0.966579020023346\n",
            "epoch 34 - batch 170 - loss 1.0609763860702515\n",
            "epoch 34 - batch 180 - loss 1.0611307621002197\n",
            "epoch 34 - batch 190 - loss 1.0780768394470215\n",
            "epoch 34 - batch 200 - loss 1.0498642921447754\n",
            "epoch 34 - batch 210 - loss 1.0932128429412842\n",
            "epoch 34 - batch 220 - loss 1.0188111066818237\n",
            "epoch 34 - batch 230 - loss 1.037348985671997\n",
            "epoch 34 - batch 240 - loss 1.0279260873794556\n",
            "epoch 34 - batch 250 - loss 0.9961833953857422\n",
            "epoch 34 - batch 260 - loss 1.004464864730835\n",
            "epoch 34 - batch 270 - loss 1.0051921606063843\n",
            "epoch 34 - batch 280 - loss 1.0345150232315063\n",
            "epoch 34 - batch 290 - loss 1.0854263305664062\n",
            "epoch 34 - batch 300 - loss 1.0192182064056396\n",
            "epoch 34 - batch 310 - loss 1.0286377668380737\n",
            "epoch 34 - batch 320 - loss 1.04441237449646\n",
            "epoch 34 - batch 330 - loss 1.0622310638427734\n",
            "epoch 34 - batch 340 - loss 1.0519624948501587\n",
            "epoch 34 - batch 350 - loss 1.05381178855896\n",
            "epoch 34 - batch 360 - loss 1.0382124185562134\n",
            "epoch 34 - batch 370 - loss 1.0149511098861694\n",
            "epoch 34 - batch 380 - loss 1.0043636560440063\n",
            "epoch 34 - batch 390 - loss 1.010707139968872\n",
            "epoch 34 - batch 400 - loss 1.080941915512085\n",
            "epoch 34 - batch 410 - loss 1.0520271062850952\n",
            "epoch 34 - batch 420 - loss 1.038774847984314\n",
            "epoch 34 - batch 430 - loss 0.9903311133384705\n",
            "epoch 34 - batch 440 - loss 1.0970829725265503\n",
            "epoch 34 - batch 450 - loss 1.0850282907485962\n",
            "epoch 34 - batch 460 - loss 1.012637972831726\n",
            "epoch 34 - batch 470 - loss 1.0642684698104858\n",
            "epoch 34 - batch 480 - loss 1.0275447368621826\n",
            "epoch 34 - batch 490 - loss 0.9822143912315369\n",
            "epoch 34 - batch 500 - loss 0.9659236073493958\n",
            "epoch 34 - batch 510 - loss 1.0152102708816528\n",
            "epoch 34 - batch 520 - loss 1.0610359907150269\n",
            "epoch 34 - batch 530 - loss 0.9995120167732239\n",
            "epoch 34 - batch 540 - loss 1.0588816404342651\n",
            "epoch 34 - batch 550 - loss 1.004266619682312\n",
            "epoch 34 - batch 560 - loss 0.9746831059455872\n",
            "epoch 34 - batch 570 - loss 1.1257661581039429\n",
            "epoch 34 - batch 580 - loss 1.0881175994873047\n",
            "epoch 34 - batch 590 - loss 1.02491295337677\n",
            "epoch 34 - batch 600 - loss 1.0023037195205688\n",
            "epoch 34 - batch 610 - loss 1.101096272468567\n",
            "epoch 34 training time: 219.38763284683228 sec\n",
            "evaluation of batch 0 took: 0.15939712524414062\n",
            "evaluation of batch 50 took: 0.15395522117614746\n",
            "evaluation of batch 100 took: 0.15738606452941895\n",
            "evaluation of batch 150 took: 0.16118526458740234\n",
            "evaluation of batch 200 took: 0.1592400074005127\n",
            "evaluation of batch 250 took: 0.1529397964477539\n",
            "evaluation of batch 300 took: 0.1574723720550537\n",
            "evaluation of batch 350 took: 0.16623592376708984\n",
            "evaluation of batch 400 took: 0.15905284881591797\n",
            "evaluation of batch 450 took: 0.15200281143188477\n",
            "evaluation of batch 500 took: 0.155836820602417\n",
            "evaluation of batch 550 took: 0.15717673301696777\n",
            "evaluation of batch 600 took: 0.16418814659118652\n",
            "epoch 34 evaluation on training data time: 98.10286331176758 sec\n",
            "evaluation of batch 0 took: 0.16916751861572266\n",
            "evaluation of batch 50 took: 0.15231704711914062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 34 evaluation on test data time: 22.431221961975098 sec\n",
            "epoch evaluation:  {'epoch': 34, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2791493>, 'test_rouge_1_p': 0.5639834751372139, 'test_rouge_1_r': 0.5254560009373067, 'test_rouge_1_f1': 0.5346723936631098, 'test_rouge_2_p': 0.2951784868777057, 'test_rouge_2_r': 0.2833111387310605, 'test_rouge_2_f1': 0.2859341920846334, 'test_rouge_3_p': 0.12850780404491344, 'test_rouge_3_r': 0.12295344629329003, 'test_rouge_3_f1': 0.12414895527339723, 'test_rouge_L_p': 0.562083979543522, 'test_rouge_L_r': 0.5239672111742424, 'test_rouge_L_f1': 0.5330476617059662}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 35 - batch 10 - loss 1.027585506439209\n",
            "epoch 35 - batch 20 - loss 1.0435285568237305\n",
            "epoch 35 - batch 30 - loss 0.962447464466095\n",
            "epoch 35 - batch 40 - loss 1.0045660734176636\n",
            "epoch 35 - batch 50 - loss 1.0302951335906982\n",
            "epoch 35 - batch 60 - loss 1.022478699684143\n",
            "epoch 35 - batch 70 - loss 1.0533384084701538\n",
            "epoch 35 - batch 80 - loss 1.0963244438171387\n",
            "epoch 35 - batch 90 - loss 1.0706727504730225\n",
            "epoch 35 - batch 100 - loss 1.1229850053787231\n",
            "epoch 35 - batch 110 - loss 1.0506058931350708\n",
            "epoch 35 - batch 120 - loss 1.0327664613723755\n",
            "epoch 35 - batch 130 - loss 1.0346193313598633\n",
            "epoch 35 - batch 140 - loss 1.0300099849700928\n",
            "epoch 35 - batch 150 - loss 1.071826696395874\n",
            "epoch 35 - batch 160 - loss 1.0429831743240356\n",
            "epoch 35 - batch 170 - loss 0.9871913194656372\n",
            "epoch 35 - batch 180 - loss 1.0591145753860474\n",
            "epoch 35 - batch 190 - loss 1.0073357820510864\n",
            "epoch 35 - batch 200 - loss 0.9887953996658325\n",
            "epoch 35 - batch 210 - loss 1.0490524768829346\n",
            "epoch 35 - batch 220 - loss 1.0150275230407715\n",
            "epoch 35 - batch 230 - loss 1.0655542612075806\n",
            "epoch 35 - batch 240 - loss 1.0204823017120361\n",
            "epoch 35 - batch 250 - loss 1.0863826274871826\n",
            "epoch 35 - batch 260 - loss 1.01567804813385\n",
            "epoch 35 - batch 270 - loss 1.0518791675567627\n",
            "epoch 35 - batch 280 - loss 1.0189845561981201\n",
            "epoch 35 - batch 290 - loss 0.980224609375\n",
            "epoch 35 - batch 300 - loss 1.0651617050170898\n",
            "epoch 35 - batch 310 - loss 1.0264331102371216\n",
            "epoch 35 - batch 320 - loss 1.0633924007415771\n",
            "epoch 35 - batch 330 - loss 1.018112063407898\n",
            "epoch 35 - batch 340 - loss 1.0427039861679077\n",
            "epoch 35 - batch 350 - loss 1.0265758037567139\n",
            "epoch 35 - batch 360 - loss 1.0872291326522827\n",
            "epoch 35 - batch 370 - loss 1.0366424322128296\n",
            "epoch 35 - batch 380 - loss 1.0660752058029175\n",
            "epoch 35 - batch 390 - loss 0.992425799369812\n",
            "epoch 35 - batch 400 - loss 0.9485608339309692\n",
            "epoch 35 - batch 410 - loss 1.038028597831726\n",
            "epoch 35 - batch 420 - loss 1.0424774885177612\n",
            "epoch 35 - batch 430 - loss 1.024464726448059\n",
            "epoch 35 - batch 440 - loss 1.050361156463623\n",
            "epoch 35 - batch 450 - loss 1.024675965309143\n",
            "epoch 35 - batch 460 - loss 1.0459643602371216\n",
            "epoch 35 - batch 470 - loss 1.086614727973938\n",
            "epoch 35 - batch 480 - loss 1.0585463047027588\n",
            "epoch 35 - batch 490 - loss 0.9945395588874817\n",
            "epoch 35 - batch 500 - loss 0.9714602828025818\n",
            "epoch 35 - batch 510 - loss 1.0021976232528687\n",
            "epoch 35 - batch 520 - loss 1.0892117023468018\n",
            "epoch 35 - batch 530 - loss 1.061951994895935\n",
            "epoch 35 - batch 540 - loss 1.0277786254882812\n",
            "epoch 35 - batch 550 - loss 0.9936753511428833\n",
            "epoch 35 - batch 560 - loss 1.0379232168197632\n",
            "epoch 35 - batch 570 - loss 1.006132960319519\n",
            "epoch 35 - batch 580 - loss 1.0451867580413818\n",
            "epoch 35 - batch 590 - loss 0.9612714648246765\n",
            "epoch 35 - batch 600 - loss 1.002318024635315\n",
            "epoch 35 - batch 610 - loss 1.0064634084701538\n",
            "epoch 35 training time: 218.85864734649658 sec\n",
            "evaluation of batch 0 took: 0.1537775993347168\n",
            "evaluation of batch 50 took: 0.166151762008667\n",
            "evaluation of batch 100 took: 0.16140294075012207\n",
            "evaluation of batch 150 took: 0.16579318046569824\n",
            "evaluation of batch 200 took: 0.15282440185546875\n",
            "evaluation of batch 250 took: 0.15176868438720703\n",
            "evaluation of batch 300 took: 0.1638956069946289\n",
            "evaluation of batch 350 took: 0.15553736686706543\n",
            "evaluation of batch 400 took: 0.16250205039978027\n",
            "evaluation of batch 450 took: 0.15245342254638672\n",
            "evaluation of batch 500 took: 0.15949320793151855\n",
            "evaluation of batch 550 took: 0.15286922454833984\n",
            "evaluation of batch 600 took: 0.15240049362182617\n",
            "epoch 35 evaluation on training data time: 97.83394742012024 sec\n",
            "evaluation of batch 0 took: 0.16532635688781738\n",
            "evaluation of batch 50 took: 0.15904498100280762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 35 evaluation on test data time: 22.314900636672974 sec\n",
            "epoch evaluation:  {'epoch': 35, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2755626>, 'test_rouge_1_p': 0.5641534223774738, 'test_rouge_1_r': 0.526863803049629, 'test_rouge_1_f1': 0.5357549779653068, 'test_rouge_2_p': 0.2972417393804112, 'test_rouge_2_r': 0.28529723856872297, 'test_rouge_2_f1': 0.2880048766450452, 'test_rouge_3_p': 0.1306484205898268, 'test_rouge_3_r': 0.12467807257846321, 'test_rouge_3_f1': 0.12601531498015878, 'test_rouge_L_p': 0.5623154979321274, 'test_rouge_L_r': 0.5254478478181046, 'test_rouge_L_f1': 0.5342012651456137}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 - batch 10 - loss 1.0384998321533203\n",
            "epoch 36 - batch 20 - loss 1.0886815786361694\n",
            "epoch 36 - batch 30 - loss 1.0021839141845703\n",
            "epoch 36 - batch 40 - loss 1.0256412029266357\n",
            "epoch 36 - batch 50 - loss 1.012203574180603\n",
            "epoch 36 - batch 60 - loss 1.0043991804122925\n",
            "epoch 36 - batch 70 - loss 0.9533543586730957\n",
            "epoch 36 - batch 80 - loss 1.0090347528457642\n",
            "epoch 36 - batch 90 - loss 1.0494465827941895\n",
            "epoch 36 - batch 100 - loss 0.9940943121910095\n",
            "epoch 36 - batch 110 - loss 0.961483895778656\n",
            "epoch 36 - batch 120 - loss 1.056679606437683\n",
            "epoch 36 - batch 130 - loss 1.0417476892471313\n",
            "epoch 36 - batch 140 - loss 0.9644880294799805\n",
            "epoch 36 - batch 150 - loss 1.061436414718628\n",
            "epoch 36 - batch 160 - loss 1.1328362226486206\n",
            "epoch 36 - batch 170 - loss 1.0101313591003418\n",
            "epoch 36 - batch 180 - loss 1.0583419799804688\n",
            "epoch 36 - batch 190 - loss 0.9867820143699646\n",
            "epoch 36 - batch 200 - loss 1.0056216716766357\n",
            "epoch 36 - batch 210 - loss 1.0304815769195557\n",
            "epoch 36 - batch 220 - loss 1.0586837530136108\n",
            "epoch 36 - batch 230 - loss 1.0060789585113525\n",
            "epoch 36 - batch 240 - loss 1.0330064296722412\n",
            "epoch 36 - batch 250 - loss 1.01425302028656\n",
            "epoch 36 - batch 260 - loss 1.0528310537338257\n",
            "epoch 36 - batch 270 - loss 1.0240734815597534\n",
            "epoch 36 - batch 280 - loss 1.0615179538726807\n",
            "epoch 36 - batch 290 - loss 1.0935803651809692\n",
            "epoch 36 - batch 300 - loss 1.0158048868179321\n",
            "epoch 36 - batch 310 - loss 1.006434679031372\n",
            "epoch 36 - batch 320 - loss 1.0117191076278687\n",
            "epoch 36 - batch 330 - loss 1.0354487895965576\n",
            "epoch 36 - batch 340 - loss 1.0366185903549194\n",
            "epoch 36 - batch 350 - loss 0.9952715635299683\n",
            "epoch 36 - batch 360 - loss 1.0092318058013916\n",
            "epoch 36 - batch 370 - loss 1.013753890991211\n",
            "epoch 36 - batch 380 - loss 0.9671201705932617\n",
            "epoch 36 - batch 390 - loss 1.0221484899520874\n",
            "epoch 36 - batch 400 - loss 0.9901948571205139\n",
            "epoch 36 - batch 410 - loss 0.9337232708930969\n",
            "epoch 36 - batch 420 - loss 0.9846588969230652\n",
            "epoch 36 - batch 430 - loss 0.9943893551826477\n",
            "epoch 36 - batch 440 - loss 1.071528673171997\n",
            "epoch 36 - batch 450 - loss 0.9641095995903015\n",
            "epoch 36 - batch 460 - loss 1.0182536840438843\n",
            "epoch 36 - batch 470 - loss 0.9548920392990112\n",
            "epoch 36 - batch 480 - loss 1.0253769159317017\n",
            "epoch 36 - batch 490 - loss 1.0375579595565796\n",
            "epoch 36 - batch 500 - loss 1.0819693803787231\n",
            "epoch 36 - batch 510 - loss 1.0852097272872925\n",
            "epoch 36 - batch 520 - loss 1.1001156568527222\n",
            "epoch 36 - batch 530 - loss 1.0186887979507446\n",
            "epoch 36 - batch 540 - loss 0.9819139838218689\n",
            "epoch 36 - batch 550 - loss 0.9958719611167908\n",
            "epoch 36 - batch 560 - loss 1.037000298500061\n",
            "epoch 36 - batch 570 - loss 1.0379188060760498\n",
            "epoch 36 - batch 580 - loss 1.1056548357009888\n",
            "epoch 36 - batch 590 - loss 1.0615546703338623\n",
            "epoch 36 - batch 600 - loss 0.9826135635375977\n",
            "epoch 36 - batch 610 - loss 0.9911717176437378\n",
            "epoch 36 training time: 219.29812836647034 sec\n",
            "evaluation of batch 0 took: 0.1569972038269043\n",
            "evaluation of batch 50 took: 0.15903067588806152\n",
            "evaluation of batch 100 took: 0.16145086288452148\n",
            "evaluation of batch 150 took: 0.17203211784362793\n",
            "evaluation of batch 200 took: 0.1518392562866211\n",
            "evaluation of batch 250 took: 0.15297484397888184\n",
            "evaluation of batch 300 took: 0.15648746490478516\n",
            "evaluation of batch 350 took: 0.1557025909423828\n",
            "evaluation of batch 400 took: 0.16396141052246094\n",
            "evaluation of batch 450 took: 0.16368436813354492\n",
            "evaluation of batch 500 took: 0.1567981243133545\n",
            "evaluation of batch 550 took: 0.15972542762756348\n",
            "evaluation of batch 600 took: 0.15247559547424316\n",
            "epoch 36 evaluation on training data time: 97.33432626724243 sec\n",
            "evaluation of batch 0 took: 0.1707000732421875\n",
            "evaluation of batch 50 took: 0.15022683143615723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 evaluation on test data time: 22.32497262954712 sec\n",
            "epoch evaluation:  {'epoch': 36, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2729633>, 'test_rouge_1_p': 0.5672085471262368, 'test_rouge_1_r': 0.527661570665198, 'test_rouge_1_f1': 0.5373718606711196, 'test_rouge_2_p': 0.29755500033820337, 'test_rouge_2_r': 0.2851735829274892, 'test_rouge_2_f1': 0.28798675585721556, 'test_rouge_3_p': 0.13094667376893943, 'test_rouge_3_r': 0.12474930668290048, 'test_rouge_3_f1': 0.12616004794436717, 'test_rouge_L_p': 0.5652773751546075, 'test_rouge_L_r': 0.5261464795435217, 'test_rouge_L_f1': 0.5357213156415419}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 36 saved checkpoint: models/checkpoints/baseline/ckpt-13\n",
            "epoch 37 - batch 10 - loss 1.0504944324493408\n",
            "epoch 37 - batch 20 - loss 1.0120830535888672\n",
            "epoch 37 - batch 30 - loss 0.9988843202590942\n",
            "epoch 37 - batch 40 - loss 1.0444642305374146\n",
            "epoch 37 - batch 50 - loss 0.9992551803588867\n",
            "epoch 37 - batch 60 - loss 1.0226452350616455\n",
            "epoch 37 - batch 70 - loss 1.0275651216506958\n",
            "epoch 37 - batch 80 - loss 1.0088989734649658\n",
            "epoch 37 - batch 90 - loss 1.04780113697052\n",
            "epoch 37 - batch 100 - loss 1.0047143697738647\n",
            "epoch 37 - batch 110 - loss 1.033920407295227\n",
            "epoch 37 - batch 120 - loss 1.0483627319335938\n",
            "epoch 37 - batch 130 - loss 1.0254485607147217\n",
            "epoch 37 - batch 140 - loss 1.055915117263794\n",
            "epoch 37 - batch 150 - loss 1.0896995067596436\n",
            "epoch 37 - batch 160 - loss 1.0702658891677856\n",
            "epoch 37 - batch 170 - loss 1.0263086557388306\n",
            "epoch 37 - batch 180 - loss 0.9614143371582031\n",
            "epoch 37 - batch 190 - loss 1.0024020671844482\n",
            "epoch 37 - batch 200 - loss 1.016628384590149\n",
            "epoch 37 - batch 210 - loss 1.034576654434204\n",
            "epoch 37 - batch 220 - loss 1.0120337009429932\n",
            "epoch 37 - batch 230 - loss 1.032758355140686\n",
            "epoch 37 - batch 240 - loss 1.0164564847946167\n",
            "epoch 37 - batch 250 - loss 1.0726486444473267\n",
            "epoch 37 - batch 260 - loss 1.0227938890457153\n",
            "epoch 37 - batch 270 - loss 1.0467129945755005\n",
            "epoch 37 - batch 280 - loss 0.9951644539833069\n",
            "epoch 37 - batch 290 - loss 1.005718469619751\n",
            "epoch 37 - batch 300 - loss 1.0312098264694214\n",
            "epoch 37 - batch 310 - loss 0.998902440071106\n",
            "epoch 37 - batch 320 - loss 0.9933911561965942\n",
            "epoch 37 - batch 330 - loss 1.0039113759994507\n",
            "epoch 37 - batch 340 - loss 1.0815166234970093\n",
            "epoch 37 - batch 350 - loss 1.0529978275299072\n",
            "epoch 37 - batch 360 - loss 0.9469693303108215\n",
            "epoch 37 - batch 370 - loss 1.038333535194397\n",
            "epoch 37 - batch 380 - loss 0.9500381350517273\n",
            "epoch 37 - batch 390 - loss 1.044062852859497\n",
            "epoch 37 - batch 400 - loss 1.0111571550369263\n",
            "epoch 37 - batch 410 - loss 1.0427772998809814\n",
            "epoch 37 - batch 420 - loss 1.00770902633667\n",
            "epoch 37 - batch 430 - loss 0.918371856212616\n",
            "epoch 37 - batch 440 - loss 1.0504918098449707\n",
            "epoch 37 - batch 450 - loss 1.0424286127090454\n",
            "epoch 37 - batch 460 - loss 0.9693938493728638\n",
            "epoch 37 - batch 470 - loss 0.9893509745597839\n",
            "epoch 37 - batch 480 - loss 1.0013870000839233\n",
            "epoch 37 - batch 490 - loss 0.9346302151679993\n",
            "epoch 37 - batch 500 - loss 1.0743523836135864\n",
            "epoch 37 - batch 510 - loss 1.031118631362915\n",
            "epoch 37 - batch 520 - loss 1.0021910667419434\n",
            "epoch 37 - batch 530 - loss 1.004071593284607\n",
            "epoch 37 - batch 540 - loss 1.019856572151184\n",
            "epoch 37 - batch 550 - loss 0.9888995289802551\n",
            "epoch 37 - batch 560 - loss 1.012237548828125\n",
            "epoch 37 - batch 570 - loss 0.9870820641517639\n",
            "epoch 37 - batch 580 - loss 0.9932361245155334\n",
            "epoch 37 - batch 590 - loss 1.0098830461502075\n",
            "epoch 37 - batch 600 - loss 0.9762665629386902\n",
            "epoch 37 - batch 610 - loss 1.0437597036361694\n",
            "epoch 37 training time: 218.61160945892334 sec\n",
            "evaluation of batch 0 took: 0.15666556358337402\n",
            "evaluation of batch 50 took: 0.15835165977478027\n",
            "evaluation of batch 100 took: 0.15144753456115723\n",
            "evaluation of batch 150 took: 0.15567827224731445\n",
            "evaluation of batch 200 took: 0.15205597877502441\n",
            "evaluation of batch 250 took: 0.15262389183044434\n",
            "evaluation of batch 300 took: 0.15723085403442383\n",
            "evaluation of batch 350 took: 0.1685647964477539\n",
            "evaluation of batch 400 took: 0.156965970993042\n",
            "evaluation of batch 450 took: 0.1528322696685791\n",
            "evaluation of batch 500 took: 0.15398454666137695\n",
            "evaluation of batch 550 took: 0.15741944313049316\n",
            "evaluation of batch 600 took: 0.15464282035827637\n",
            "epoch 37 evaluation on training data time: 97.69094824790955 sec\n",
            "evaluation of batch 0 took: 0.16353654861450195\n",
            "evaluation of batch 50 took: 0.15897822380065918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 37 evaluation on test data time: 22.328513145446777 sec\n",
            "epoch evaluation:  {'epoch': 37, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2745851>, 'test_rouge_1_p': 0.5695616883116883, 'test_rouge_1_r': 0.5292309857181509, 'test_rouge_1_f1': 0.5393506137621396, 'test_rouge_2_p': 0.29955864448051944, 'test_rouge_2_r': 0.28610871550324674, 'test_rouge_2_f1': 0.2893956157704129, 'test_rouge_3_p': 0.13168501420454543, 'test_rouge_3_r': 0.12477678571428573, 'test_rouge_3_f1': 0.12647399336154916, 'test_rouge_L_p': 0.5676820923681973, 'test_rouge_L_r': 0.5277782810567411, 'test_rouge_L_f1': 0.5377601944496473}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 38 - batch 10 - loss 1.0372470617294312\n",
            "epoch 38 - batch 20 - loss 1.033203125\n",
            "epoch 38 - batch 30 - loss 0.983280599117279\n",
            "epoch 38 - batch 40 - loss 1.0099979639053345\n",
            "epoch 38 - batch 50 - loss 1.0501673221588135\n",
            "epoch 38 - batch 60 - loss 1.0680288076400757\n",
            "epoch 38 - batch 70 - loss 0.9971346259117126\n",
            "epoch 38 - batch 80 - loss 1.0680210590362549\n",
            "epoch 38 - batch 90 - loss 1.0255954265594482\n",
            "epoch 38 - batch 100 - loss 1.026690125465393\n",
            "epoch 38 - batch 110 - loss 0.9706239700317383\n",
            "epoch 38 - batch 120 - loss 1.0394481420516968\n",
            "epoch 38 - batch 130 - loss 0.9744085669517517\n",
            "epoch 38 - batch 140 - loss 1.018574595451355\n",
            "epoch 38 - batch 150 - loss 1.0585997104644775\n",
            "epoch 38 - batch 160 - loss 1.0317336320877075\n",
            "epoch 38 - batch 170 - loss 1.0248668193817139\n",
            "epoch 38 - batch 180 - loss 1.0334386825561523\n",
            "epoch 38 - batch 190 - loss 1.0188521146774292\n",
            "epoch 38 - batch 200 - loss 0.9705837965011597\n",
            "epoch 38 - batch 210 - loss 1.0248209238052368\n",
            "epoch 38 - batch 220 - loss 1.043456792831421\n",
            "epoch 38 - batch 230 - loss 0.9710522294044495\n",
            "epoch 38 - batch 240 - loss 1.0655659437179565\n",
            "epoch 38 - batch 250 - loss 1.0214109420776367\n",
            "epoch 38 - batch 260 - loss 1.0299166440963745\n",
            "epoch 38 - batch 270 - loss 0.9842429757118225\n",
            "epoch 38 - batch 280 - loss 1.02426016330719\n",
            "epoch 38 - batch 290 - loss 1.037388563156128\n",
            "epoch 38 - batch 300 - loss 1.0161283016204834\n",
            "epoch 38 - batch 310 - loss 0.9941628575325012\n",
            "epoch 38 - batch 320 - loss 0.9640598297119141\n",
            "epoch 38 - batch 330 - loss 0.9952266812324524\n",
            "epoch 38 - batch 340 - loss 1.0153234004974365\n",
            "epoch 38 - batch 350 - loss 1.0367544889450073\n",
            "epoch 38 - batch 360 - loss 1.0379823446273804\n",
            "epoch 38 - batch 370 - loss 1.0141023397445679\n",
            "epoch 38 - batch 380 - loss 0.9822884798049927\n",
            "epoch 38 - batch 390 - loss 1.0228393077850342\n",
            "epoch 38 - batch 400 - loss 1.044640064239502\n",
            "epoch 38 - batch 410 - loss 1.0411614179611206\n",
            "epoch 38 - batch 420 - loss 1.0289522409439087\n",
            "epoch 38 - batch 430 - loss 1.012198805809021\n",
            "epoch 38 - batch 440 - loss 1.042359471321106\n",
            "epoch 38 - batch 450 - loss 0.9902342557907104\n",
            "epoch 38 - batch 460 - loss 1.052573800086975\n",
            "epoch 38 - batch 470 - loss 1.0172947645187378\n",
            "epoch 38 - batch 480 - loss 1.013140320777893\n",
            "epoch 38 - batch 490 - loss 0.9567745923995972\n",
            "epoch 38 - batch 500 - loss 1.0121592283248901\n",
            "epoch 38 - batch 510 - loss 1.0383967161178589\n",
            "epoch 38 - batch 520 - loss 1.0737018585205078\n",
            "epoch 38 - batch 530 - loss 0.9973779320716858\n",
            "epoch 38 - batch 540 - loss 1.017960786819458\n",
            "epoch 38 - batch 550 - loss 0.9687219262123108\n",
            "epoch 38 - batch 560 - loss 1.092623233795166\n",
            "epoch 38 - batch 570 - loss 0.9765042066574097\n",
            "epoch 38 - batch 580 - loss 1.0173625946044922\n",
            "epoch 38 - batch 590 - loss 0.9747126698493958\n",
            "epoch 38 - batch 600 - loss 0.9881277084350586\n",
            "epoch 38 - batch 610 - loss 1.0085806846618652\n",
            "epoch 38 training time: 221.37946701049805 sec\n",
            "evaluation of batch 0 took: 0.16002440452575684\n",
            "evaluation of batch 50 took: 0.15511393547058105\n",
            "evaluation of batch 100 took: 0.15681767463684082\n",
            "evaluation of batch 150 took: 0.15427017211914062\n",
            "evaluation of batch 200 took: 0.15360498428344727\n",
            "evaluation of batch 250 took: 0.16016674041748047\n",
            "evaluation of batch 300 took: 0.1745755672454834\n",
            "evaluation of batch 350 took: 0.15337085723876953\n",
            "evaluation of batch 400 took: 0.15078401565551758\n",
            "evaluation of batch 450 took: 0.15679717063903809\n",
            "evaluation of batch 500 took: 0.16310572624206543\n",
            "evaluation of batch 550 took: 0.16377997398376465\n",
            "evaluation of batch 600 took: 0.16409778594970703\n",
            "epoch 38 evaluation on training data time: 98.45742011070251 sec\n",
            "evaluation of batch 0 took: 0.1662743091583252\n",
            "evaluation of batch 50 took: 0.15768074989318848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 38 evaluation on test data time: 22.82277750968933 sec\n",
            "epoch evaluation:  {'epoch': 38, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2709237>, 'test_rouge_1_p': 0.5657829893320968, 'test_rouge_1_r': 0.5313748031172695, 'test_rouge_1_f1': 0.5389810965634065, 'test_rouge_2_p': 0.29905937161796536, 'test_rouge_2_r': 0.2882136093073593, 'test_rouge_2_f1': 0.2903537289786783, 'test_rouge_3_p': 0.13142142688041128, 'test_rouge_3_r': 0.12616616781655846, 'test_rouge_3_f1': 0.12721008917297977, 'test_rouge_L_p': 0.5638421846011132, 'test_rouge_L_r': 0.5298493847112709, 'test_rouge_L_f1': 0.5373256111645163}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 - batch 10 - loss 1.0034995079040527\n",
            "epoch 39 - batch 20 - loss 0.9869289398193359\n",
            "epoch 39 - batch 30 - loss 0.9867118000984192\n",
            "epoch 39 - batch 40 - loss 0.9737949371337891\n",
            "epoch 39 - batch 50 - loss 1.0549262762069702\n",
            "epoch 39 - batch 60 - loss 1.0315423011779785\n",
            "epoch 39 - batch 70 - loss 0.981514036655426\n",
            "epoch 39 - batch 80 - loss 1.0184143781661987\n",
            "epoch 39 - batch 90 - loss 0.9992674589157104\n",
            "epoch 39 - batch 100 - loss 0.9815158843994141\n",
            "epoch 39 - batch 110 - loss 1.0744529962539673\n",
            "epoch 39 - batch 120 - loss 1.0572584867477417\n",
            "epoch 39 - batch 130 - loss 1.022201418876648\n",
            "epoch 39 - batch 140 - loss 1.0311814546585083\n",
            "epoch 39 - batch 150 - loss 1.0834009647369385\n",
            "epoch 39 - batch 160 - loss 1.0093772411346436\n",
            "epoch 39 - batch 170 - loss 1.016506552696228\n",
            "epoch 39 - batch 180 - loss 1.0058516263961792\n",
            "epoch 39 - batch 190 - loss 1.0290712118148804\n",
            "epoch 39 - batch 200 - loss 1.0566632747650146\n",
            "epoch 39 - batch 210 - loss 1.00283682346344\n",
            "epoch 39 - batch 220 - loss 1.0098456144332886\n",
            "epoch 39 - batch 230 - loss 1.0727161169052124\n",
            "epoch 39 - batch 240 - loss 0.9906612038612366\n",
            "epoch 39 - batch 250 - loss 1.043486475944519\n",
            "epoch 39 - batch 260 - loss 1.0077011585235596\n",
            "epoch 39 - batch 270 - loss 1.0088979005813599\n",
            "epoch 39 - batch 280 - loss 0.9968443512916565\n",
            "epoch 39 - batch 290 - loss 0.9864767789840698\n",
            "epoch 39 - batch 300 - loss 0.9621729850769043\n",
            "epoch 39 - batch 310 - loss 1.005989670753479\n",
            "epoch 39 - batch 320 - loss 0.9944493174552917\n",
            "epoch 39 - batch 330 - loss 0.970865786075592\n",
            "epoch 39 - batch 340 - loss 1.041273593902588\n",
            "epoch 39 - batch 350 - loss 1.026354193687439\n",
            "epoch 39 - batch 360 - loss 1.0313979387283325\n",
            "epoch 39 - batch 370 - loss 1.0375365018844604\n",
            "epoch 39 - batch 380 - loss 0.9609381556510925\n",
            "epoch 39 - batch 390 - loss 1.0147532224655151\n",
            "epoch 39 - batch 400 - loss 1.0037964582443237\n",
            "epoch 39 - batch 410 - loss 0.9754975438117981\n",
            "epoch 39 - batch 420 - loss 0.9331303834915161\n",
            "epoch 39 - batch 430 - loss 0.9432579278945923\n",
            "epoch 39 - batch 440 - loss 1.0455950498580933\n",
            "epoch 39 - batch 450 - loss 1.0492801666259766\n",
            "epoch 39 - batch 460 - loss 0.966278076171875\n",
            "epoch 39 - batch 470 - loss 1.0550297498703003\n",
            "epoch 39 - batch 480 - loss 1.030403971672058\n",
            "epoch 39 - batch 490 - loss 0.9721353650093079\n",
            "epoch 39 - batch 500 - loss 0.9958571791648865\n",
            "epoch 39 - batch 510 - loss 1.052458643913269\n",
            "epoch 39 - batch 520 - loss 0.982925295829773\n",
            "epoch 39 - batch 530 - loss 1.0323354005813599\n",
            "epoch 39 - batch 540 - loss 0.9515534043312073\n",
            "epoch 39 - batch 550 - loss 0.9581122398376465\n",
            "epoch 39 - batch 560 - loss 1.023551344871521\n",
            "epoch 39 - batch 570 - loss 1.0082271099090576\n",
            "epoch 39 - batch 580 - loss 1.008697748184204\n",
            "epoch 39 - batch 590 - loss 0.9586731791496277\n",
            "epoch 39 - batch 600 - loss 0.9893515706062317\n",
            "epoch 39 - batch 610 - loss 1.0410714149475098\n",
            "epoch 39 training time: 219.70994901657104 sec\n",
            "evaluation of batch 0 took: 0.1578226089477539\n",
            "evaluation of batch 50 took: 0.15384316444396973\n",
            "evaluation of batch 100 took: 0.15820026397705078\n",
            "evaluation of batch 150 took: 0.15881586074829102\n",
            "evaluation of batch 200 took: 0.15872406959533691\n",
            "evaluation of batch 250 took: 0.1656327247619629\n",
            "evaluation of batch 300 took: 0.1662755012512207\n",
            "evaluation of batch 350 took: 0.16155648231506348\n",
            "evaluation of batch 400 took: 0.16291356086730957\n",
            "evaluation of batch 450 took: 0.15967774391174316\n",
            "evaluation of batch 500 took: 0.15850400924682617\n",
            "evaluation of batch 550 took: 0.16177892684936523\n",
            "evaluation of batch 600 took: 0.15378499031066895\n",
            "epoch 39 evaluation on training data time: 98.51809334754944 sec\n",
            "evaluation of batch 0 took: 0.16798782348632812\n",
            "evaluation of batch 50 took: 0.15514016151428223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 evaluation on test data time: 22.668214797973633 sec\n",
            "epoch evaluation:  {'epoch': 39, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.271811>, 'test_rouge_1_p': 0.5656124381570812, 'test_rouge_1_r': 0.5316863730577457, 'test_rouge_1_f1': 0.5391314347882682, 'test_rouge_2_p': 0.2993007643398268, 'test_rouge_2_r': 0.2882825182629869, 'test_rouge_2_f1': 0.2905289231308915, 'test_rouge_3_p': 0.1316930465367965, 'test_rouge_3_r': 0.12631793662067098, 'test_rouge_3_f1': 0.1274023014745671, 'test_rouge_L_p': 0.5637043968865955, 'test_rouge_L_r': 0.5301856253865181, 'test_rouge_L_f1': 0.5375009307493328}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 39 saved checkpoint: models/checkpoints/baseline/ckpt-14\n",
            "epoch 40 - batch 10 - loss 0.9663603901863098\n",
            "epoch 40 - batch 20 - loss 1.0085994005203247\n",
            "epoch 40 - batch 30 - loss 0.986186146736145\n",
            "epoch 40 - batch 40 - loss 0.9758250117301941\n",
            "epoch 40 - batch 50 - loss 0.9798806309700012\n",
            "epoch 40 - batch 60 - loss 1.0256606340408325\n",
            "epoch 40 - batch 70 - loss 0.9381056427955627\n",
            "epoch 40 - batch 80 - loss 0.9994156956672668\n",
            "epoch 40 - batch 90 - loss 0.9830611348152161\n",
            "epoch 40 - batch 100 - loss 0.9828591346740723\n",
            "epoch 40 - batch 110 - loss 1.0123119354248047\n",
            "epoch 40 - batch 120 - loss 0.9974437355995178\n",
            "epoch 40 - batch 130 - loss 1.0078057050704956\n",
            "epoch 40 - batch 140 - loss 0.9808177351951599\n",
            "epoch 40 - batch 150 - loss 0.9912225008010864\n",
            "epoch 40 - batch 160 - loss 0.9452053904533386\n",
            "epoch 40 - batch 170 - loss 1.0237656831741333\n",
            "epoch 40 - batch 180 - loss 0.9959706664085388\n",
            "epoch 40 - batch 190 - loss 0.976775586605072\n",
            "epoch 40 - batch 200 - loss 1.0300753116607666\n",
            "epoch 40 - batch 210 - loss 0.9703603386878967\n",
            "epoch 40 - batch 220 - loss 1.0218751430511475\n",
            "epoch 40 - batch 230 - loss 0.9667184948921204\n",
            "epoch 40 - batch 240 - loss 0.9876019358634949\n",
            "epoch 40 - batch 250 - loss 1.0584205389022827\n",
            "epoch 40 - batch 260 - loss 0.9826670289039612\n",
            "epoch 40 - batch 270 - loss 1.0054214000701904\n",
            "epoch 40 - batch 280 - loss 0.9685344696044922\n",
            "epoch 40 - batch 290 - loss 1.0542681217193604\n",
            "epoch 40 - batch 300 - loss 1.0160335302352905\n",
            "epoch 40 - batch 310 - loss 0.9998614192008972\n",
            "epoch 40 - batch 320 - loss 1.028577446937561\n",
            "epoch 40 - batch 330 - loss 1.0639336109161377\n",
            "epoch 40 - batch 340 - loss 0.9857217073440552\n",
            "epoch 40 - batch 350 - loss 1.0392541885375977\n",
            "epoch 40 - batch 360 - loss 1.0008348226547241\n",
            "epoch 40 - batch 370 - loss 1.015380620956421\n",
            "epoch 40 - batch 380 - loss 0.9870070219039917\n",
            "epoch 40 - batch 390 - loss 0.9513083696365356\n",
            "epoch 40 - batch 400 - loss 0.9249142408370972\n",
            "epoch 40 - batch 410 - loss 1.018388032913208\n",
            "epoch 40 - batch 420 - loss 1.0604716539382935\n",
            "epoch 40 - batch 430 - loss 0.9602679014205933\n",
            "epoch 40 - batch 440 - loss 0.9812055230140686\n",
            "epoch 40 - batch 450 - loss 0.9595522284507751\n",
            "epoch 40 - batch 460 - loss 0.9577532410621643\n",
            "epoch 40 - batch 470 - loss 1.0048998594284058\n",
            "epoch 40 - batch 480 - loss 0.9721036553382874\n",
            "epoch 40 - batch 490 - loss 0.9920573830604553\n",
            "epoch 40 - batch 500 - loss 0.9895766377449036\n",
            "epoch 40 - batch 510 - loss 0.9878362417221069\n",
            "epoch 40 - batch 520 - loss 0.9899107813835144\n",
            "epoch 40 - batch 530 - loss 1.0170263051986694\n",
            "epoch 40 - batch 540 - loss 0.9656332731246948\n",
            "epoch 40 - batch 550 - loss 0.9469202756881714\n",
            "epoch 40 - batch 560 - loss 1.002508282661438\n",
            "epoch 40 - batch 570 - loss 1.0256210565567017\n",
            "epoch 40 - batch 580 - loss 1.019027590751648\n",
            "epoch 40 - batch 590 - loss 1.0421802997589111\n",
            "epoch 40 - batch 600 - loss 0.9504268765449524\n",
            "epoch 40 - batch 610 - loss 1.0014092922210693\n",
            "epoch 40 training time: 220.5915310382843 sec\n",
            "evaluation of batch 0 took: 0.15979433059692383\n",
            "evaluation of batch 50 took: 0.16256093978881836\n",
            "evaluation of batch 100 took: 0.1554863452911377\n",
            "evaluation of batch 150 took: 0.15579795837402344\n",
            "evaluation of batch 200 took: 0.15207982063293457\n",
            "evaluation of batch 250 took: 0.15177226066589355\n",
            "evaluation of batch 300 took: 0.15850281715393066\n",
            "evaluation of batch 350 took: 0.29816293716430664\n",
            "evaluation of batch 400 took: 0.15936660766601562\n",
            "evaluation of batch 450 took: 0.15521025657653809\n",
            "evaluation of batch 500 took: 0.16131377220153809\n",
            "evaluation of batch 550 took: 0.16574454307556152\n",
            "evaluation of batch 600 took: 0.1615309715270996\n",
            "epoch 40 evaluation on training data time: 100.3619954586029 sec\n",
            "evaluation of batch 0 took: 0.1799910068511963\n",
            "evaluation of batch 50 took: 0.1622602939605713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 40 evaluation on test data time: 22.955618143081665 sec\n",
            "epoch evaluation:  {'epoch': 40, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2692076>, 'test_rouge_1_p': 0.5660569039405537, 'test_rouge_1_r': 0.5330358652694032, 'test_rouge_1_f1': 0.5402132484049589, 'test_rouge_2_p': 0.2991595643939393, 'test_rouge_2_r': 0.289316786728896, 'test_rouge_2_f1': 0.29114738160243425, 'test_rouge_3_p': 0.13165394176136364, 'test_rouge_3_r': 0.12684870468073592, 'test_rouge_3_f1': 0.12777005747848375, 'test_rouge_L_p': 0.5641200549822203, 'test_rouge_L_r': 0.5314890675730523, 'test_rouge_L_f1': 0.5385401953323777}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 41 - batch 10 - loss 0.9880309700965881\n",
            "epoch 41 - batch 20 - loss 0.9796074628829956\n",
            "epoch 41 - batch 30 - loss 0.9885392785072327\n",
            "epoch 41 - batch 40 - loss 0.9987257719039917\n",
            "epoch 41 - batch 50 - loss 1.0267492532730103\n",
            "epoch 41 - batch 60 - loss 0.9350056648254395\n",
            "epoch 41 - batch 70 - loss 0.9774805903434753\n",
            "epoch 41 - batch 80 - loss 1.0410206317901611\n",
            "epoch 41 - batch 90 - loss 0.9863763451576233\n",
            "epoch 41 - batch 100 - loss 0.9859482049942017\n",
            "epoch 41 - batch 110 - loss 1.0408135652542114\n",
            "epoch 41 - batch 120 - loss 1.035581350326538\n",
            "epoch 41 - batch 130 - loss 0.959889829158783\n",
            "epoch 41 - batch 140 - loss 0.9221119284629822\n",
            "epoch 41 - batch 150 - loss 1.0478532314300537\n",
            "epoch 41 - batch 160 - loss 1.0041182041168213\n",
            "epoch 41 - batch 170 - loss 1.0167973041534424\n",
            "epoch 41 - batch 180 - loss 0.9921624064445496\n",
            "epoch 41 - batch 190 - loss 1.0040843486785889\n",
            "epoch 41 - batch 200 - loss 0.9792922139167786\n",
            "epoch 41 - batch 210 - loss 1.0103689432144165\n",
            "epoch 41 - batch 220 - loss 1.0087834596633911\n",
            "epoch 41 - batch 230 - loss 1.031556487083435\n",
            "epoch 41 - batch 240 - loss 1.0021194219589233\n",
            "epoch 41 - batch 250 - loss 1.0373834371566772\n",
            "epoch 41 - batch 260 - loss 1.0080379247665405\n",
            "epoch 41 - batch 270 - loss 0.998066782951355\n",
            "epoch 41 - batch 280 - loss 0.9957143068313599\n",
            "epoch 41 - batch 290 - loss 0.9747986197471619\n",
            "epoch 41 - batch 300 - loss 0.9502009153366089\n",
            "epoch 41 - batch 310 - loss 0.9777550101280212\n",
            "epoch 41 - batch 320 - loss 1.0355490446090698\n",
            "epoch 41 - batch 330 - loss 1.0229499340057373\n",
            "epoch 41 - batch 340 - loss 1.0014468431472778\n",
            "epoch 41 - batch 350 - loss 0.9632354378700256\n",
            "epoch 41 - batch 360 - loss 1.0622559785842896\n",
            "epoch 41 - batch 370 - loss 1.0577276945114136\n",
            "epoch 41 - batch 380 - loss 0.993864893913269\n",
            "epoch 41 - batch 390 - loss 0.9762958884239197\n",
            "epoch 41 - batch 400 - loss 0.9721919298171997\n",
            "epoch 41 - batch 410 - loss 1.0112719535827637\n",
            "epoch 41 - batch 420 - loss 1.0276459455490112\n",
            "epoch 41 - batch 430 - loss 0.9984452128410339\n",
            "epoch 41 - batch 440 - loss 1.0260443687438965\n",
            "epoch 41 - batch 450 - loss 1.0099655389785767\n",
            "epoch 41 - batch 460 - loss 1.0300959348678589\n",
            "epoch 41 - batch 470 - loss 0.9385923147201538\n",
            "epoch 41 - batch 480 - loss 0.9791334867477417\n",
            "epoch 41 - batch 490 - loss 0.9389511942863464\n",
            "epoch 41 - batch 500 - loss 1.026061773300171\n",
            "epoch 41 - batch 510 - loss 1.025272011756897\n",
            "epoch 41 - batch 520 - loss 0.9855056405067444\n",
            "epoch 41 - batch 530 - loss 0.9478141069412231\n",
            "epoch 41 - batch 540 - loss 0.9544805884361267\n",
            "epoch 41 - batch 550 - loss 0.9872865676879883\n",
            "epoch 41 - batch 560 - loss 0.9957987666130066\n",
            "epoch 41 - batch 570 - loss 1.013175129890442\n",
            "epoch 41 - batch 580 - loss 1.0292056798934937\n",
            "epoch 41 - batch 590 - loss 0.9724920988082886\n",
            "epoch 41 - batch 600 - loss 0.9252421259880066\n",
            "epoch 41 - batch 610 - loss 1.05910325050354\n",
            "epoch 41 training time: 222.07359862327576 sec\n",
            "evaluation of batch 0 took: 0.16851425170898438\n",
            "evaluation of batch 50 took: 0.16807293891906738\n",
            "evaluation of batch 100 took: 0.17582440376281738\n",
            "evaluation of batch 150 took: 0.1667630672454834\n",
            "evaluation of batch 200 took: 0.1615769863128662\n",
            "evaluation of batch 250 took: 0.16449952125549316\n",
            "evaluation of batch 300 took: 0.16424179077148438\n",
            "evaluation of batch 350 took: 0.16087603569030762\n",
            "evaluation of batch 400 took: 0.17465662956237793\n",
            "evaluation of batch 450 took: 0.15912485122680664\n",
            "evaluation of batch 500 took: 0.16838431358337402\n",
            "evaluation of batch 550 took: 0.16969561576843262\n",
            "evaluation of batch 600 took: 0.17135143280029297\n",
            "epoch 41 evaluation on training data time: 101.09025454521179 sec\n",
            "evaluation of batch 0 took: 0.17488503456115723\n",
            "evaluation of batch 50 took: 0.16287636756896973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 41 evaluation on test data time: 22.787538766860962 sec\n",
            "epoch evaluation:  {'epoch': 41, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2689683>, 'test_rouge_1_p': 0.5698140424493663, 'test_rouge_1_r': 0.5330711048623998, 'test_rouge_1_f1': 0.541843908143624, 'test_rouge_2_p': 0.3027863737824676, 'test_rouge_2_r': 0.28992111404220783, 'test_rouge_2_f1': 0.2929133151947936, 'test_rouge_3_p': 0.13266812939664502, 'test_rouge_3_r': 0.12607062533820346, 'test_rouge_3_f1': 0.12766179210794165, 'test_rouge_L_p': 0.5678345255005411, 'test_rouge_L_r': 0.5315527826897803, 'test_rouge_L_f1': 0.5401736690691387}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 - batch 10 - loss 0.9908854365348816\n",
            "epoch 42 - batch 20 - loss 1.0431724786758423\n",
            "epoch 42 - batch 30 - loss 0.9990759491920471\n",
            "epoch 42 - batch 40 - loss 1.0232751369476318\n",
            "epoch 42 - batch 50 - loss 0.9540567398071289\n",
            "epoch 42 - batch 60 - loss 1.0056848526000977\n",
            "epoch 42 - batch 70 - loss 0.9903929829597473\n",
            "epoch 42 - batch 80 - loss 0.9905018210411072\n",
            "epoch 42 - batch 90 - loss 1.0391792058944702\n",
            "epoch 42 - batch 100 - loss 0.9763911366462708\n",
            "epoch 42 - batch 110 - loss 0.9866078495979309\n",
            "epoch 42 - batch 120 - loss 1.0470249652862549\n",
            "epoch 42 - batch 130 - loss 0.9490280151367188\n",
            "epoch 42 - batch 140 - loss 0.9480823278427124\n",
            "epoch 42 - batch 150 - loss 1.0278615951538086\n",
            "epoch 42 - batch 160 - loss 1.0038050413131714\n",
            "epoch 42 - batch 170 - loss 0.9588915109634399\n",
            "epoch 42 - batch 180 - loss 1.0217840671539307\n",
            "epoch 42 - batch 190 - loss 0.9942452311515808\n",
            "epoch 42 - batch 200 - loss 0.9284428358078003\n",
            "epoch 42 - batch 210 - loss 1.0297328233718872\n",
            "epoch 42 - batch 220 - loss 1.0243682861328125\n",
            "epoch 42 - batch 230 - loss 0.9764123558998108\n",
            "epoch 42 - batch 240 - loss 0.9947282671928406\n",
            "epoch 42 - batch 250 - loss 1.0137004852294922\n",
            "epoch 42 - batch 260 - loss 1.0087432861328125\n",
            "epoch 42 - batch 270 - loss 0.985980212688446\n",
            "epoch 42 - batch 280 - loss 1.034818172454834\n",
            "epoch 42 - batch 290 - loss 1.0041916370391846\n",
            "epoch 42 - batch 300 - loss 0.9764168858528137\n",
            "epoch 42 - batch 310 - loss 0.9418169856071472\n",
            "epoch 42 - batch 320 - loss 0.974506676197052\n",
            "epoch 42 - batch 330 - loss 0.99903404712677\n",
            "epoch 42 - batch 340 - loss 0.9770762324333191\n",
            "epoch 42 - batch 350 - loss 0.9983092546463013\n",
            "epoch 42 - batch 360 - loss 0.9388955235481262\n",
            "epoch 42 - batch 370 - loss 0.9824870228767395\n",
            "epoch 42 - batch 380 - loss 1.0089813470840454\n",
            "epoch 42 - batch 390 - loss 0.9769102334976196\n",
            "epoch 42 - batch 400 - loss 0.9767430424690247\n",
            "epoch 42 - batch 410 - loss 0.9421029686927795\n",
            "epoch 42 - batch 420 - loss 0.9538164138793945\n",
            "epoch 42 - batch 430 - loss 0.9805091023445129\n",
            "epoch 42 - batch 440 - loss 1.0503426790237427\n",
            "epoch 42 - batch 450 - loss 1.0041760206222534\n",
            "epoch 42 - batch 460 - loss 0.990746796131134\n",
            "epoch 42 - batch 470 - loss 0.9896618723869324\n",
            "epoch 42 - batch 480 - loss 0.9999512434005737\n",
            "epoch 42 - batch 490 - loss 0.9410322904586792\n",
            "epoch 42 - batch 500 - loss 0.9404370188713074\n",
            "epoch 42 - batch 510 - loss 0.9372811317443848\n",
            "epoch 42 - batch 520 - loss 0.9351587295532227\n",
            "epoch 42 - batch 530 - loss 1.0300779342651367\n",
            "epoch 42 - batch 540 - loss 0.9904700517654419\n",
            "epoch 42 - batch 550 - loss 0.9391878247261047\n",
            "epoch 42 - batch 560 - loss 0.9824501872062683\n",
            "epoch 42 - batch 570 - loss 0.9928109049797058\n",
            "epoch 42 - batch 580 - loss 0.9319227933883667\n",
            "epoch 42 - batch 590 - loss 0.9896013140678406\n",
            "epoch 42 - batch 600 - loss 1.0137830972671509\n",
            "epoch 42 - batch 610 - loss 0.9965468645095825\n",
            "epoch 42 training time: 221.41910219192505 sec\n",
            "evaluation of batch 0 took: 0.15805792808532715\n",
            "evaluation of batch 50 took: 0.15567398071289062\n",
            "evaluation of batch 100 took: 0.15635371208190918\n",
            "evaluation of batch 150 took: 0.16000580787658691\n",
            "evaluation of batch 200 took: 0.15449094772338867\n",
            "evaluation of batch 250 took: 0.15560603141784668\n",
            "evaluation of batch 300 took: 0.1541750431060791\n",
            "evaluation of batch 350 took: 0.1521751880645752\n",
            "evaluation of batch 400 took: 0.15706872940063477\n",
            "evaluation of batch 450 took: 0.1543445587158203\n",
            "evaluation of batch 500 took: 0.16997528076171875\n",
            "evaluation of batch 550 took: 0.15324687957763672\n",
            "evaluation of batch 600 took: 0.15273308753967285\n",
            "epoch 42 evaluation on training data time: 98.73199200630188 sec\n",
            "evaluation of batch 0 took: 0.1650676727294922\n",
            "evaluation of batch 50 took: 0.16502118110656738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 evaluation on test data time: 22.346075296401978 sec\n",
            "epoch evaluation:  {'epoch': 42, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2687055>, 'test_rouge_1_p': 0.5717155914212273, 'test_rouge_1_r': 0.5314855949482067, 'test_rouge_1_f1': 0.5416375465235295, 'test_rouge_2_p': 0.302252012310606, 'test_rouge_2_r': 0.28892869825487016, 'test_rouge_2_f1': 0.2921808612929503, 'test_rouge_3_p': 0.133401819534632, 'test_rouge_3_r': 0.12672885382846324, 'test_rouge_3_f1': 0.1283420541512059, 'test_rouge_L_p': 0.5697935388644094, 'test_rouge_L_r': 0.5299962012503865, 'test_rouge_L_f1': 0.5400034088104808}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 42 saved checkpoint: models/checkpoints/baseline/ckpt-15\n",
            "epoch 43 - batch 10 - loss 1.0374175310134888\n",
            "epoch 43 - batch 20 - loss 1.0027391910552979\n",
            "epoch 43 - batch 30 - loss 0.9698172807693481\n",
            "epoch 43 - batch 40 - loss 0.9594807624816895\n",
            "epoch 43 - batch 50 - loss 1.0152385234832764\n",
            "epoch 43 - batch 60 - loss 0.9582932591438293\n",
            "epoch 43 - batch 70 - loss 0.9730368852615356\n",
            "epoch 43 - batch 80 - loss 0.9547268152236938\n",
            "epoch 43 - batch 90 - loss 0.9446926116943359\n",
            "epoch 43 - batch 100 - loss 1.0189964771270752\n",
            "epoch 43 - batch 110 - loss 0.9850949048995972\n",
            "epoch 43 - batch 120 - loss 1.0686882734298706\n",
            "epoch 43 - batch 130 - loss 0.960048258304596\n",
            "epoch 43 - batch 140 - loss 0.986831545829773\n",
            "epoch 43 - batch 150 - loss 1.0613733530044556\n",
            "epoch 43 - batch 160 - loss 0.934293270111084\n",
            "epoch 43 - batch 170 - loss 0.9944156408309937\n",
            "epoch 43 - batch 180 - loss 0.9631638526916504\n",
            "epoch 43 - batch 190 - loss 0.9719746708869934\n",
            "epoch 43 - batch 200 - loss 0.96178138256073\n",
            "epoch 43 - batch 210 - loss 1.013628363609314\n",
            "epoch 43 - batch 220 - loss 0.988957405090332\n",
            "epoch 43 - batch 230 - loss 0.9321260452270508\n",
            "epoch 43 - batch 240 - loss 1.00576651096344\n",
            "epoch 43 - batch 250 - loss 0.9554916024208069\n",
            "epoch 43 - batch 260 - loss 0.9153925180435181\n",
            "epoch 43 - batch 270 - loss 0.9454702734947205\n",
            "epoch 43 - batch 280 - loss 1.0096431970596313\n",
            "epoch 43 - batch 290 - loss 0.93498295545578\n",
            "epoch 43 - batch 300 - loss 1.0156267881393433\n",
            "epoch 43 - batch 310 - loss 0.923050582408905\n",
            "epoch 43 - batch 320 - loss 0.9914999604225159\n",
            "epoch 43 - batch 330 - loss 1.0232003927230835\n",
            "epoch 43 - batch 340 - loss 0.961572527885437\n",
            "epoch 43 - batch 350 - loss 0.9945506453514099\n",
            "epoch 43 - batch 360 - loss 1.018152117729187\n",
            "epoch 43 - batch 370 - loss 0.9586012959480286\n",
            "epoch 43 - batch 380 - loss 0.9558338522911072\n",
            "epoch 43 - batch 390 - loss 0.94013512134552\n",
            "epoch 43 - batch 400 - loss 0.9694862365722656\n",
            "epoch 43 - batch 410 - loss 1.0018432140350342\n",
            "epoch 43 - batch 420 - loss 0.9752645492553711\n",
            "epoch 43 - batch 430 - loss 0.9672592878341675\n",
            "epoch 43 - batch 440 - loss 1.0446265935897827\n",
            "epoch 43 - batch 450 - loss 1.0414022207260132\n",
            "epoch 43 - batch 460 - loss 0.9305434823036194\n",
            "epoch 43 - batch 470 - loss 1.0378504991531372\n",
            "epoch 43 - batch 480 - loss 0.9899304509162903\n",
            "epoch 43 - batch 490 - loss 0.937886655330658\n",
            "epoch 43 - batch 500 - loss 0.9498988389968872\n",
            "epoch 43 - batch 510 - loss 0.9978455305099487\n",
            "epoch 43 - batch 520 - loss 0.9532197117805481\n",
            "epoch 43 - batch 530 - loss 1.000096082687378\n",
            "epoch 43 - batch 540 - loss 0.9157649278640747\n",
            "epoch 43 - batch 550 - loss 0.9653035402297974\n",
            "epoch 43 - batch 560 - loss 0.9720970988273621\n",
            "epoch 43 - batch 570 - loss 1.0613898038864136\n",
            "epoch 43 - batch 580 - loss 0.990273654460907\n",
            "epoch 43 - batch 590 - loss 0.9816799163818359\n",
            "epoch 43 - batch 600 - loss 0.9744940996170044\n",
            "epoch 43 - batch 610 - loss 1.083755612373352\n",
            "epoch 43 training time: 219.30242085456848 sec\n",
            "evaluation of batch 0 took: 0.15541362762451172\n",
            "evaluation of batch 50 took: 0.15724396705627441\n",
            "evaluation of batch 100 took: 0.19040584564208984\n",
            "evaluation of batch 150 took: 0.1747734546661377\n",
            "evaluation of batch 200 took: 0.15976715087890625\n",
            "evaluation of batch 250 took: 0.16465091705322266\n",
            "evaluation of batch 300 took: 0.16075778007507324\n",
            "evaluation of batch 350 took: 0.16663479804992676\n",
            "evaluation of batch 400 took: 0.17989444732666016\n",
            "evaluation of batch 450 took: 0.15308260917663574\n",
            "evaluation of batch 500 took: 0.15921258926391602\n",
            "evaluation of batch 550 took: 0.1630568504333496\n",
            "evaluation of batch 600 took: 0.16389989852905273\n",
            "epoch 43 evaluation on training data time: 101.92125535011292 sec\n",
            "evaluation of batch 0 took: 0.19142556190490723\n",
            "evaluation of batch 50 took: 0.1616368293762207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 43 evaluation on test data time: 22.982276439666748 sec\n",
            "epoch evaluation:  {'epoch': 43, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2682048>, 'test_rouge_1_p': 0.5696146533897652, 'test_rouge_1_r': 0.5336944863172542, 'test_rouge_1_f1': 0.5420561099113506, 'test_rouge_2_p': 0.3026075487012987, 'test_rouge_2_r': 0.2906554383116882, 'test_rouge_2_f1': 0.2932616007082068, 'test_rouge_3_p': 0.13359375000000007, 'test_rouge_3_r': 0.127685546875, 'test_rouge_3_f1': 0.1289721191506906, 'test_rouge_L_p': 0.5677735582869512, 'test_rouge_L_r': 0.5322423251971241, 'test_rouge_L_f1': 0.5404771267158393}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 44 - batch 10 - loss 1.0286606550216675\n",
            "epoch 44 - batch 20 - loss 1.0128223896026611\n",
            "epoch 44 - batch 30 - loss 0.9788488745689392\n",
            "epoch 44 - batch 40 - loss 0.9674969911575317\n",
            "epoch 44 - batch 50 - loss 0.9530134201049805\n",
            "epoch 44 - batch 60 - loss 0.9717759490013123\n",
            "epoch 44 - batch 70 - loss 1.035414695739746\n",
            "epoch 44 - batch 80 - loss 0.9476943612098694\n",
            "epoch 44 - batch 90 - loss 0.9690905809402466\n",
            "epoch 44 - batch 100 - loss 1.0148675441741943\n",
            "epoch 44 - batch 110 - loss 0.9585353136062622\n",
            "epoch 44 - batch 120 - loss 1.0419597625732422\n",
            "epoch 44 - batch 130 - loss 0.9303600192070007\n",
            "epoch 44 - batch 140 - loss 0.9789015650749207\n",
            "epoch 44 - batch 150 - loss 1.0397846698760986\n",
            "epoch 44 - batch 160 - loss 0.9523399472236633\n",
            "epoch 44 - batch 170 - loss 0.9429406523704529\n",
            "epoch 44 - batch 180 - loss 0.9472756385803223\n",
            "epoch 44 - batch 190 - loss 0.9158840775489807\n",
            "epoch 44 - batch 200 - loss 0.9965687394142151\n",
            "epoch 44 - batch 210 - loss 1.013828992843628\n",
            "epoch 44 - batch 220 - loss 1.0336310863494873\n",
            "epoch 44 - batch 230 - loss 0.9757749438285828\n",
            "epoch 44 - batch 240 - loss 0.9937944412231445\n",
            "epoch 44 - batch 250 - loss 1.0065537691116333\n",
            "epoch 44 - batch 260 - loss 0.9548020958900452\n",
            "epoch 44 - batch 270 - loss 0.9981827735900879\n",
            "epoch 44 - batch 280 - loss 0.9874754548072815\n",
            "epoch 44 - batch 290 - loss 0.9477360844612122\n",
            "epoch 44 - batch 300 - loss 0.950579822063446\n",
            "epoch 44 - batch 310 - loss 1.033132553100586\n",
            "epoch 44 - batch 320 - loss 1.0276978015899658\n",
            "epoch 44 - batch 330 - loss 0.9766892194747925\n",
            "epoch 44 - batch 340 - loss 1.0344064235687256\n",
            "epoch 44 - batch 350 - loss 0.9582332968711853\n",
            "epoch 44 - batch 360 - loss 0.967427134513855\n",
            "epoch 44 - batch 370 - loss 0.9308095574378967\n",
            "epoch 44 - batch 380 - loss 0.9568091630935669\n",
            "epoch 44 - batch 390 - loss 1.0180033445358276\n",
            "epoch 44 - batch 400 - loss 0.9885311126708984\n",
            "epoch 44 - batch 410 - loss 0.9610744118690491\n",
            "epoch 44 - batch 420 - loss 1.0206327438354492\n",
            "epoch 44 - batch 430 - loss 0.9981546998023987\n",
            "epoch 44 - batch 440 - loss 0.9720249176025391\n",
            "epoch 44 - batch 450 - loss 0.9623960852622986\n",
            "epoch 44 - batch 460 - loss 0.9524456858634949\n",
            "epoch 44 - batch 470 - loss 0.9728646278381348\n",
            "epoch 44 - batch 480 - loss 0.9789184927940369\n",
            "epoch 44 - batch 490 - loss 0.9043785929679871\n",
            "epoch 44 - batch 500 - loss 1.004954218864441\n",
            "epoch 44 - batch 510 - loss 0.9616827964782715\n",
            "epoch 44 - batch 520 - loss 0.9456062912940979\n",
            "epoch 44 - batch 530 - loss 0.9830598831176758\n",
            "epoch 44 - batch 540 - loss 0.9911162257194519\n",
            "epoch 44 - batch 550 - loss 0.952255129814148\n",
            "epoch 44 - batch 560 - loss 0.9705796241760254\n",
            "epoch 44 - batch 570 - loss 1.0006576776504517\n",
            "epoch 44 - batch 580 - loss 0.9423650503158569\n",
            "epoch 44 - batch 590 - loss 0.953273355960846\n",
            "epoch 44 - batch 600 - loss 0.9647331237792969\n",
            "epoch 44 - batch 610 - loss 1.0118744373321533\n",
            "epoch 44 training time: 220.89448428153992 sec\n",
            "evaluation of batch 0 took: 0.1637887954711914\n",
            "evaluation of batch 50 took: 0.1543285846710205\n",
            "evaluation of batch 100 took: 0.1542208194732666\n",
            "evaluation of batch 150 took: 0.15737223625183105\n",
            "evaluation of batch 200 took: 0.16308212280273438\n",
            "evaluation of batch 250 took: 0.15581679344177246\n",
            "evaluation of batch 300 took: 0.16199064254760742\n",
            "evaluation of batch 350 took: 0.16325116157531738\n",
            "evaluation of batch 400 took: 0.17693257331848145\n",
            "evaluation of batch 450 took: 0.1531834602355957\n",
            "evaluation of batch 500 took: 0.1608414649963379\n",
            "evaluation of batch 550 took: 0.15621495246887207\n",
            "evaluation of batch 600 took: 0.16128063201904297\n",
            "epoch 44 evaluation on training data time: 100.60571503639221 sec\n",
            "evaluation of batch 0 took: 0.17604398727416992\n",
            "evaluation of batch 50 took: 0.16042470932006836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 44 evaluation on test data time: 22.690736532211304 sec\n",
            "epoch evaluation:  {'epoch': 44, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2658404>, 'test_rouge_1_p': 0.5709801498724487, 'test_rouge_1_r': 0.5341493095817873, 'test_rouge_1_f1': 0.5428962662151834, 'test_rouge_2_p': 0.30301001082251083, 'test_rouge_2_r': 0.2904013629599567, 'test_rouge_2_f1': 0.29331065302110876, 'test_rouge_3_p': 0.13321158008658004, 'test_rouge_3_r': 0.12719176981872296, 'test_rouge_3_f1': 0.1285553739725057, 'test_rouge_L_p': 0.569072048208488, 'test_rouge_L_r': 0.5326633885088125, 'test_rouge_L_f1': 0.5412720620322586}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 - batch 10 - loss 0.931524395942688\n",
            "epoch 45 - batch 20 - loss 1.0174188613891602\n",
            "epoch 45 - batch 30 - loss 0.9899865388870239\n",
            "epoch 45 - batch 40 - loss 0.9320268034934998\n",
            "epoch 45 - batch 50 - loss 0.8788550496101379\n",
            "epoch 45 - batch 60 - loss 1.0127846002578735\n",
            "epoch 45 - batch 70 - loss 0.9828928709030151\n",
            "epoch 45 - batch 80 - loss 1.0048578977584839\n",
            "epoch 45 - batch 90 - loss 0.9962790608406067\n",
            "epoch 45 - batch 100 - loss 1.0481005907058716\n",
            "epoch 45 - batch 110 - loss 0.8843392729759216\n",
            "epoch 45 - batch 120 - loss 1.0002926588058472\n",
            "epoch 45 - batch 130 - loss 0.8989294767379761\n",
            "epoch 45 - batch 140 - loss 0.9292303919792175\n",
            "epoch 45 - batch 150 - loss 0.9691885709762573\n",
            "epoch 45 - batch 160 - loss 0.945987343788147\n",
            "epoch 45 - batch 170 - loss 0.9679589867591858\n",
            "epoch 45 - batch 180 - loss 1.0409175157546997\n",
            "epoch 45 - batch 190 - loss 0.9445309042930603\n",
            "epoch 45 - batch 200 - loss 1.027344822883606\n",
            "epoch 45 - batch 210 - loss 1.0033679008483887\n",
            "epoch 45 - batch 220 - loss 1.0205446481704712\n",
            "epoch 45 - batch 230 - loss 0.9179626107215881\n",
            "epoch 45 - batch 240 - loss 0.9921581149101257\n",
            "epoch 45 - batch 250 - loss 0.9815454483032227\n",
            "epoch 45 - batch 260 - loss 0.9106935858726501\n",
            "epoch 45 - batch 270 - loss 1.0684196949005127\n",
            "epoch 45 - batch 280 - loss 0.9764022827148438\n",
            "epoch 45 - batch 290 - loss 0.9645509123802185\n",
            "epoch 45 - batch 300 - loss 0.9571153521537781\n",
            "epoch 45 - batch 310 - loss 0.9221067428588867\n",
            "epoch 45 - batch 320 - loss 0.9021620154380798\n",
            "epoch 45 - batch 330 - loss 0.9718592762947083\n",
            "epoch 45 - batch 340 - loss 0.9640079140663147\n",
            "epoch 45 - batch 350 - loss 1.0030912160873413\n",
            "epoch 45 - batch 360 - loss 0.9444180130958557\n",
            "epoch 45 - batch 370 - loss 0.9564794301986694\n",
            "epoch 45 - batch 380 - loss 0.9433258771896362\n",
            "epoch 45 - batch 390 - loss 0.9666531682014465\n",
            "epoch 45 - batch 400 - loss 0.9441943168640137\n",
            "epoch 45 - batch 410 - loss 0.9381968379020691\n",
            "epoch 45 - batch 420 - loss 0.9705468416213989\n",
            "epoch 45 - batch 430 - loss 0.8765122294425964\n",
            "epoch 45 - batch 440 - loss 1.014398217201233\n",
            "epoch 45 - batch 450 - loss 0.9908071160316467\n",
            "epoch 45 - batch 460 - loss 0.9471855163574219\n",
            "epoch 45 - batch 470 - loss 1.0292078256607056\n",
            "epoch 45 - batch 480 - loss 0.983372688293457\n",
            "epoch 45 - batch 490 - loss 0.98987877368927\n",
            "epoch 45 - batch 500 - loss 0.9690548777580261\n",
            "epoch 45 - batch 510 - loss 0.9668805003166199\n",
            "epoch 45 - batch 520 - loss 0.9629444479942322\n",
            "epoch 45 - batch 530 - loss 1.011710524559021\n",
            "epoch 45 - batch 540 - loss 0.9630448222160339\n",
            "epoch 45 - batch 550 - loss 0.9266991019248962\n",
            "epoch 45 - batch 560 - loss 1.0069983005523682\n",
            "epoch 45 - batch 570 - loss 1.0222418308258057\n",
            "epoch 45 - batch 580 - loss 0.9177295565605164\n",
            "epoch 45 - batch 590 - loss 0.9663294553756714\n",
            "epoch 45 - batch 600 - loss 0.969459056854248\n",
            "epoch 45 - batch 610 - loss 1.0183144807815552\n",
            "epoch 45 training time: 219.5784945487976 sec\n",
            "evaluation of batch 0 took: 0.1623516082763672\n",
            "evaluation of batch 50 took: 0.1790919303894043\n",
            "evaluation of batch 100 took: 0.16499090194702148\n",
            "evaluation of batch 150 took: 0.1603710651397705\n",
            "evaluation of batch 200 took: 0.15962719917297363\n",
            "evaluation of batch 250 took: 0.166611909866333\n",
            "evaluation of batch 300 took: 0.16589665412902832\n",
            "evaluation of batch 350 took: 0.16425275802612305\n",
            "evaluation of batch 400 took: 0.16337847709655762\n",
            "evaluation of batch 450 took: 0.16777849197387695\n",
            "evaluation of batch 500 took: 0.16671514511108398\n",
            "evaluation of batch 550 took: 0.17375636100769043\n",
            "evaluation of batch 600 took: 0.16340923309326172\n",
            "epoch 45 evaluation on training data time: 101.9187068939209 sec\n",
            "evaluation of batch 0 took: 0.17235970497131348\n",
            "evaluation of batch 50 took: 0.1645064353942871\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 evaluation on test data time: 22.997745752334595 sec\n",
            "epoch evaluation:  {'epoch': 45, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2650678>, 'test_rouge_1_p': 0.5710026464420993, 'test_rouge_1_r': 0.5343445918850495, 'test_rouge_1_f1': 0.5430379423403975, 'test_rouge_2_p': 0.3046052742830086, 'test_rouge_2_r': 0.292037210836039, 'test_rouge_2_f1': 0.2948968281785855, 'test_rouge_3_p': 0.1346664891098484, 'test_rouge_3_r': 0.12853760822510826, 'test_rouge_3_f1': 0.1299681786777211, 'test_rouge_L_p': 0.5690696928629407, 'test_rouge_L_r': 0.5328379256532161, 'test_rouge_L_f1': 0.5413898488776735}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 45 saved checkpoint: models/checkpoints/baseline/ckpt-16\n",
            "epoch 46 - batch 10 - loss 0.9285939931869507\n",
            "epoch 46 - batch 20 - loss 0.9816206693649292\n",
            "epoch 46 - batch 30 - loss 0.9610574841499329\n",
            "epoch 46 - batch 40 - loss 0.9744483828544617\n",
            "epoch 46 - batch 50 - loss 0.9041189551353455\n",
            "epoch 46 - batch 60 - loss 0.9424633383750916\n",
            "epoch 46 - batch 70 - loss 0.9547563791275024\n",
            "epoch 46 - batch 80 - loss 0.9485529065132141\n",
            "epoch 46 - batch 90 - loss 0.9425891041755676\n",
            "epoch 46 - batch 100 - loss 0.9986170530319214\n",
            "epoch 46 - batch 110 - loss 0.9365893602371216\n",
            "epoch 46 - batch 120 - loss 0.9238850474357605\n",
            "epoch 46 - batch 130 - loss 0.9276658892631531\n",
            "epoch 46 - batch 140 - loss 1.075683832168579\n",
            "epoch 46 - batch 150 - loss 1.0059057474136353\n",
            "epoch 46 - batch 160 - loss 1.0590503215789795\n",
            "epoch 46 - batch 170 - loss 0.9751158356666565\n",
            "epoch 46 - batch 180 - loss 0.9452705979347229\n",
            "epoch 46 - batch 190 - loss 0.975346028804779\n",
            "epoch 46 - batch 200 - loss 0.9846078753471375\n",
            "epoch 46 - batch 210 - loss 0.9665972590446472\n",
            "epoch 46 - batch 220 - loss 1.0132803916931152\n",
            "epoch 46 - batch 230 - loss 1.0018764734268188\n",
            "epoch 46 - batch 240 - loss 0.9857450723648071\n",
            "epoch 46 - batch 250 - loss 0.9780397415161133\n",
            "epoch 46 - batch 260 - loss 0.9315761923789978\n",
            "epoch 46 - batch 270 - loss 1.025516390800476\n",
            "epoch 46 - batch 280 - loss 0.9436163306236267\n",
            "epoch 46 - batch 290 - loss 0.9355095624923706\n",
            "epoch 46 - batch 300 - loss 0.9375659227371216\n",
            "epoch 46 - batch 310 - loss 0.9827789664268494\n",
            "epoch 46 - batch 320 - loss 0.9672263264656067\n",
            "epoch 46 - batch 330 - loss 1.0042190551757812\n",
            "epoch 46 - batch 340 - loss 0.9662516713142395\n",
            "epoch 46 - batch 350 - loss 0.9353247880935669\n",
            "epoch 46 - batch 360 - loss 0.928926408290863\n",
            "epoch 46 - batch 370 - loss 0.9769777655601501\n",
            "epoch 46 - batch 380 - loss 1.0021699666976929\n",
            "epoch 46 - batch 390 - loss 0.9221720099449158\n",
            "epoch 46 - batch 400 - loss 0.9467032551765442\n",
            "epoch 46 - batch 410 - loss 0.989316463470459\n",
            "epoch 46 - batch 420 - loss 0.9293848872184753\n",
            "epoch 46 - batch 430 - loss 0.9207631349563599\n",
            "epoch 46 - batch 440 - loss 0.8998484015464783\n",
            "epoch 46 - batch 450 - loss 0.9883166551589966\n",
            "epoch 46 - batch 460 - loss 0.9734274744987488\n",
            "epoch 46 - batch 470 - loss 0.9470078349113464\n",
            "epoch 46 - batch 480 - loss 0.9743745923042297\n",
            "epoch 46 - batch 490 - loss 0.9560437798500061\n",
            "epoch 46 - batch 500 - loss 0.9272720217704773\n",
            "epoch 46 - batch 510 - loss 0.9623547792434692\n",
            "epoch 46 - batch 520 - loss 0.9870601892471313\n",
            "epoch 46 - batch 530 - loss 0.9472402930259705\n",
            "epoch 46 - batch 540 - loss 0.9550873041152954\n",
            "epoch 46 - batch 550 - loss 0.9659823179244995\n",
            "epoch 46 - batch 560 - loss 1.0176918506622314\n",
            "epoch 46 - batch 570 - loss 1.0082628726959229\n",
            "epoch 46 - batch 580 - loss 0.9599607586860657\n",
            "epoch 46 - batch 590 - loss 0.9703575968742371\n",
            "epoch 46 - batch 600 - loss 0.962433397769928\n",
            "epoch 46 - batch 610 - loss 1.0116544961929321\n",
            "epoch 46 training time: 224.26296257972717 sec\n",
            "evaluation of batch 0 took: 0.16922330856323242\n",
            "evaluation of batch 50 took: 0.15805840492248535\n",
            "evaluation of batch 100 took: 0.1611158847808838\n",
            "evaluation of batch 150 took: 0.16730833053588867\n",
            "evaluation of batch 200 took: 0.1682124137878418\n",
            "evaluation of batch 250 took: 0.16624736785888672\n",
            "evaluation of batch 300 took: 0.1773085594177246\n",
            "evaluation of batch 350 took: 0.1647341251373291\n",
            "evaluation of batch 400 took: 0.1706678867340088\n",
            "evaluation of batch 450 took: 0.16894793510437012\n",
            "evaluation of batch 500 took: 0.1598982810974121\n",
            "evaluation of batch 550 took: 0.16605877876281738\n",
            "evaluation of batch 600 took: 0.17740797996520996\n",
            "epoch 46 evaluation on training data time: 103.05266261100769 sec\n",
            "evaluation of batch 0 took: 0.17247843742370605\n",
            "evaluation of batch 50 took: 0.16606569290161133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 46 evaluation on test data time: 23.4702467918396 sec\n",
            "epoch evaluation:  {'epoch': 46, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2645553>, 'test_rouge_1_p': 0.5706613931083794, 'test_rouge_1_r': 0.5356559758136208, 'test_rouge_1_f1': 0.5437070978968046, 'test_rouge_2_p': 0.3037918949540043, 'test_rouge_2_r': 0.29251915077110374, 'test_rouge_2_f1': 0.29492563586643344, 'test_rouge_3_p': 0.13470876454274888, 'test_rouge_3_r': 0.1293399959415585, 'test_rouge_3_f1': 0.13046009360183472, 'test_rouge_L_p': 0.5688398050981757, 'test_rouge_L_r': 0.5342148666995207, 'test_rouge_L_f1': 0.5421435746567644}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 47 - batch 10 - loss 0.970469057559967\n",
            "epoch 47 - batch 20 - loss 0.983113169670105\n",
            "epoch 47 - batch 30 - loss 0.9630421996116638\n",
            "epoch 47 - batch 40 - loss 0.9855199456214905\n",
            "epoch 47 - batch 50 - loss 0.9204539060592651\n",
            "epoch 47 - batch 60 - loss 0.9543933272361755\n",
            "epoch 47 - batch 70 - loss 0.9815830588340759\n",
            "epoch 47 - batch 80 - loss 0.9924159049987793\n",
            "epoch 47 - batch 90 - loss 0.9535061717033386\n",
            "epoch 47 - batch 100 - loss 0.9605928063392639\n",
            "epoch 47 - batch 110 - loss 0.9549840092658997\n",
            "epoch 47 - batch 120 - loss 0.9827050566673279\n",
            "epoch 47 - batch 130 - loss 0.9705778956413269\n",
            "epoch 47 - batch 140 - loss 0.9503196477890015\n",
            "epoch 47 - batch 150 - loss 0.9565411806106567\n",
            "epoch 47 - batch 160 - loss 1.030594825744629\n",
            "epoch 47 - batch 170 - loss 0.9558741450309753\n",
            "epoch 47 - batch 180 - loss 0.9446324110031128\n",
            "epoch 47 - batch 190 - loss 0.9734689593315125\n",
            "epoch 47 - batch 200 - loss 0.9723494648933411\n",
            "epoch 47 - batch 210 - loss 0.9311580657958984\n",
            "epoch 47 - batch 220 - loss 1.061408281326294\n",
            "epoch 47 - batch 230 - loss 0.9836164116859436\n",
            "epoch 47 - batch 240 - loss 0.9932384490966797\n",
            "epoch 47 - batch 250 - loss 0.9629347920417786\n",
            "epoch 47 - batch 260 - loss 0.938186526298523\n",
            "epoch 47 - batch 270 - loss 0.956119954586029\n",
            "epoch 47 - batch 280 - loss 0.9583929181098938\n",
            "epoch 47 - batch 290 - loss 0.9375075101852417\n",
            "epoch 47 - batch 300 - loss 0.9785199761390686\n",
            "epoch 47 - batch 310 - loss 0.9925358891487122\n",
            "epoch 47 - batch 320 - loss 0.9572331309318542\n",
            "epoch 47 - batch 330 - loss 0.9269222617149353\n",
            "epoch 47 - batch 340 - loss 1.0029423236846924\n",
            "epoch 47 - batch 350 - loss 0.9678309559822083\n",
            "epoch 47 - batch 360 - loss 0.9284659028053284\n",
            "epoch 47 - batch 370 - loss 0.937808096408844\n",
            "epoch 47 - batch 380 - loss 0.9816909432411194\n",
            "epoch 47 - batch 390 - loss 0.9804449081420898\n",
            "epoch 47 - batch 400 - loss 0.9572924375534058\n",
            "epoch 47 - batch 410 - loss 0.9590557813644409\n",
            "epoch 47 - batch 420 - loss 0.9354615211486816\n",
            "epoch 47 - batch 430 - loss 0.9855448603630066\n",
            "epoch 47 - batch 440 - loss 0.9864460229873657\n",
            "epoch 47 - batch 450 - loss 1.0302259922027588\n",
            "epoch 47 - batch 460 - loss 0.9220157265663147\n",
            "epoch 47 - batch 470 - loss 0.9267417192459106\n",
            "epoch 47 - batch 480 - loss 0.9692238569259644\n",
            "epoch 47 - batch 490 - loss 1.0054099559783936\n",
            "epoch 47 - batch 500 - loss 0.9544164538383484\n",
            "epoch 47 - batch 510 - loss 0.9664987921714783\n",
            "epoch 47 - batch 520 - loss 0.9971106648445129\n",
            "epoch 47 - batch 530 - loss 0.9660671353340149\n",
            "epoch 47 - batch 540 - loss 0.9583669900894165\n",
            "epoch 47 - batch 550 - loss 1.017924189567566\n",
            "epoch 47 - batch 560 - loss 0.9834578633308411\n",
            "epoch 47 - batch 570 - loss 1.0173958539962769\n",
            "epoch 47 - batch 580 - loss 0.927224338054657\n",
            "epoch 47 - batch 590 - loss 1.0338541269302368\n",
            "epoch 47 - batch 600 - loss 0.972574770450592\n",
            "epoch 47 - batch 610 - loss 0.9537637829780579\n",
            "epoch 47 training time: 222.0935924053192 sec\n",
            "evaluation of batch 0 took: 0.16234970092773438\n",
            "evaluation of batch 50 took: 0.16904211044311523\n",
            "evaluation of batch 100 took: 0.1624741554260254\n",
            "evaluation of batch 150 took: 0.17348217964172363\n",
            "evaluation of batch 200 took: 0.16758179664611816\n",
            "evaluation of batch 250 took: 0.16051316261291504\n",
            "evaluation of batch 300 took: 0.16546082496643066\n",
            "evaluation of batch 350 took: 0.16825294494628906\n",
            "evaluation of batch 400 took: 0.16701626777648926\n",
            "evaluation of batch 450 took: 0.16637778282165527\n",
            "evaluation of batch 500 took: 0.15869903564453125\n",
            "evaluation of batch 550 took: 0.16402697563171387\n",
            "evaluation of batch 600 took: 0.16854333877563477\n",
            "epoch 47 evaluation on training data time: 102.46520948410034 sec\n",
            "evaluation of batch 0 took: 0.17629122734069824\n",
            "evaluation of batch 50 took: 0.15894246101379395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 47 evaluation on test data time: 23.07061767578125 sec\n",
            "epoch evaluation:  {'epoch': 47, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2664636>, 'test_rouge_1_p': 0.5729610558712122, 'test_rouge_1_r': 0.5342848627377086, 'test_rouge_1_f1': 0.5438676352282275, 'test_rouge_2_p': 0.3036139153814935, 'test_rouge_2_r': 0.29063683712121213, 'test_rouge_2_f1': 0.29376460698561463, 'test_rouge_3_p': 0.13373748647186143, 'test_rouge_3_r': 0.12735347334956715, 'test_rouge_3_f1': 0.1288470643939394, 'test_rouge_L_p': 0.5709898430252781, 'test_rouge_L_r': 0.532768593943259, 'test_rouge_L_f1': 0.5421989287261708}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 - batch 10 - loss 0.96798175573349\n",
            "epoch 48 - batch 20 - loss 0.951029896736145\n",
            "epoch 48 - batch 30 - loss 0.9493598341941833\n",
            "epoch 48 - batch 40 - loss 0.9322354197502136\n",
            "epoch 48 - batch 50 - loss 0.9507901072502136\n",
            "epoch 48 - batch 60 - loss 0.9860549569129944\n",
            "epoch 48 - batch 70 - loss 0.9631204605102539\n",
            "epoch 48 - batch 80 - loss 0.9500627517700195\n",
            "epoch 48 - batch 90 - loss 0.9430164098739624\n",
            "epoch 48 - batch 100 - loss 1.0159251689910889\n",
            "epoch 48 - batch 110 - loss 0.9539744257926941\n",
            "epoch 48 - batch 120 - loss 1.0217711925506592\n",
            "epoch 48 - batch 130 - loss 0.9759988784790039\n",
            "epoch 48 - batch 140 - loss 1.0171815156936646\n",
            "epoch 48 - batch 150 - loss 0.9857237935066223\n",
            "epoch 48 - batch 160 - loss 0.9666718244552612\n",
            "epoch 48 - batch 170 - loss 0.9148843884468079\n",
            "epoch 48 - batch 180 - loss 0.9529692530632019\n",
            "epoch 48 - batch 190 - loss 0.9071879386901855\n",
            "epoch 48 - batch 200 - loss 0.9207273721694946\n",
            "epoch 48 - batch 210 - loss 0.9826770424842834\n",
            "epoch 48 - batch 220 - loss 1.0580202341079712\n",
            "epoch 48 - batch 230 - loss 0.9303496479988098\n",
            "epoch 48 - batch 240 - loss 0.9707926511764526\n",
            "epoch 48 - batch 250 - loss 1.0218755006790161\n",
            "epoch 48 - batch 260 - loss 0.9601699113845825\n",
            "epoch 48 - batch 270 - loss 0.9680002927780151\n",
            "epoch 48 - batch 280 - loss 0.9603630900382996\n",
            "epoch 48 - batch 290 - loss 0.9489850997924805\n",
            "epoch 48 - batch 300 - loss 0.9140337109565735\n",
            "epoch 48 - batch 310 - loss 0.9762310981750488\n",
            "epoch 48 - batch 320 - loss 0.9867647886276245\n",
            "epoch 48 - batch 330 - loss 0.9446150660514832\n",
            "epoch 48 - batch 340 - loss 1.0181033611297607\n",
            "epoch 48 - batch 350 - loss 0.9280279874801636\n",
            "epoch 48 - batch 360 - loss 0.9329285025596619\n",
            "epoch 48 - batch 370 - loss 0.9403100609779358\n",
            "epoch 48 - batch 380 - loss 0.9025106430053711\n",
            "epoch 48 - batch 390 - loss 0.9460748434066772\n",
            "epoch 48 - batch 400 - loss 0.9523068070411682\n",
            "epoch 48 - batch 410 - loss 0.9332531690597534\n",
            "epoch 48 - batch 420 - loss 0.9810087084770203\n",
            "epoch 48 - batch 430 - loss 0.9985979795455933\n",
            "epoch 48 - batch 440 - loss 0.9617230296134949\n",
            "epoch 48 - batch 450 - loss 0.948804497718811\n",
            "epoch 48 - batch 460 - loss 0.9318119883537292\n",
            "epoch 48 - batch 470 - loss 0.9447602033615112\n",
            "epoch 48 - batch 480 - loss 0.979756772518158\n",
            "epoch 48 - batch 490 - loss 0.9483613967895508\n",
            "epoch 48 - batch 500 - loss 0.8918498754501343\n",
            "epoch 48 - batch 510 - loss 0.89876788854599\n",
            "epoch 48 - batch 520 - loss 1.008887529373169\n",
            "epoch 48 - batch 530 - loss 0.9512841105461121\n",
            "epoch 48 - batch 540 - loss 0.925420343875885\n",
            "epoch 48 - batch 550 - loss 0.9846686124801636\n",
            "epoch 48 - batch 560 - loss 0.9929441809654236\n",
            "epoch 48 - batch 570 - loss 0.9294257760047913\n",
            "epoch 48 - batch 580 - loss 1.012929081916809\n",
            "epoch 48 - batch 590 - loss 0.9498168230056763\n",
            "epoch 48 - batch 600 - loss 0.9133491516113281\n",
            "epoch 48 - batch 610 - loss 0.9888013005256653\n",
            "epoch 48 training time: 219.8075771331787 sec\n",
            "evaluation of batch 0 took: 0.15999412536621094\n",
            "evaluation of batch 50 took: 0.1568455696105957\n",
            "evaluation of batch 100 took: 0.15365266799926758\n",
            "evaluation of batch 150 took: 0.16312694549560547\n",
            "evaluation of batch 200 took: 0.15700125694274902\n",
            "evaluation of batch 250 took: 0.16508126258850098\n",
            "evaluation of batch 300 took: 0.15921783447265625\n",
            "evaluation of batch 350 took: 0.16129589080810547\n",
            "evaluation of batch 400 took: 0.1553809642791748\n",
            "evaluation of batch 450 took: 0.16762924194335938\n",
            "evaluation of batch 500 took: 0.16329717636108398\n",
            "evaluation of batch 550 took: 0.16684293746948242\n",
            "evaluation of batch 600 took: 0.16069269180297852\n",
            "epoch 48 evaluation on training data time: 100.31115746498108 sec\n",
            "evaluation of batch 0 took: 0.17270588874816895\n",
            "evaluation of batch 50 took: 0.16108298301696777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 evaluation on test data time: 22.76160478591919 sec\n",
            "epoch evaluation:  {'epoch': 48, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2650311>, 'test_rouge_1_p': 0.5704740827438931, 'test_rouge_1_r': 0.5371425008213515, 'test_rouge_1_f1': 0.544569554104355, 'test_rouge_2_p': 0.30550383860930747, 'test_rouge_2_r': 0.29399266098484855, 'test_rouge_2_f1': 0.29641353253810127, 'test_rouge_3_p': 0.13528899485930734, 'test_rouge_3_r': 0.1297134993912338, 'test_rouge_3_f1': 0.1309163059163059, 'test_rouge_L_p': 0.5686238984230055, 'test_rouge_L_r': 0.5356670580163884, 'test_rouge_L_f1': 0.5429745790262225}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 48 saved checkpoint: models/checkpoints/baseline/ckpt-17\n",
            "epoch 49 - batch 10 - loss 0.9620097279548645\n",
            "epoch 49 - batch 20 - loss 1.0030436515808105\n",
            "epoch 49 - batch 30 - loss 0.9543736577033997\n",
            "epoch 49 - batch 40 - loss 0.9848076701164246\n",
            "epoch 49 - batch 50 - loss 0.9341984987258911\n",
            "epoch 49 - batch 60 - loss 0.9404576420783997\n",
            "epoch 49 - batch 70 - loss 0.9462589621543884\n",
            "epoch 49 - batch 80 - loss 0.946780264377594\n",
            "epoch 49 - batch 90 - loss 0.9466019868850708\n",
            "epoch 49 - batch 100 - loss 0.9932678937911987\n",
            "epoch 49 - batch 110 - loss 0.9792372584342957\n",
            "epoch 49 - batch 120 - loss 0.9597407579421997\n",
            "epoch 49 - batch 130 - loss 0.8874680399894714\n",
            "epoch 49 - batch 140 - loss 0.9669962525367737\n",
            "epoch 49 - batch 150 - loss 1.0148593187332153\n",
            "epoch 49 - batch 160 - loss 0.9414123892784119\n",
            "epoch 49 - batch 170 - loss 0.9662327170372009\n",
            "epoch 49 - batch 180 - loss 0.9685665965080261\n",
            "epoch 49 - batch 190 - loss 0.9968535304069519\n",
            "epoch 49 - batch 200 - loss 0.9767688512802124\n",
            "epoch 49 - batch 210 - loss 0.9764028787612915\n",
            "epoch 49 - batch 220 - loss 0.9596922993659973\n",
            "epoch 49 - batch 230 - loss 0.8954758644104004\n",
            "epoch 49 - batch 240 - loss 0.9109182357788086\n",
            "epoch 49 - batch 250 - loss 0.9604939222335815\n",
            "epoch 49 - batch 260 - loss 1.0126291513442993\n",
            "epoch 49 - batch 270 - loss 0.9047747850418091\n",
            "epoch 49 - batch 280 - loss 0.9356443285942078\n",
            "epoch 49 - batch 290 - loss 0.9468819499015808\n",
            "epoch 49 - batch 300 - loss 0.9341241121292114\n",
            "epoch 49 - batch 310 - loss 0.9676328301429749\n",
            "epoch 49 - batch 320 - loss 0.9808184504508972\n",
            "epoch 49 - batch 330 - loss 0.9400385022163391\n",
            "epoch 49 - batch 340 - loss 0.9727568626403809\n",
            "epoch 49 - batch 350 - loss 0.9029091596603394\n",
            "epoch 49 - batch 360 - loss 0.9224199652671814\n",
            "epoch 49 - batch 370 - loss 0.9738803505897522\n",
            "epoch 49 - batch 380 - loss 0.9450415372848511\n",
            "epoch 49 - batch 390 - loss 0.9188865423202515\n",
            "epoch 49 - batch 400 - loss 0.9308390617370605\n",
            "epoch 49 - batch 410 - loss 0.991485595703125\n",
            "epoch 49 - batch 420 - loss 1.0261657238006592\n",
            "epoch 49 - batch 430 - loss 0.981770396232605\n",
            "epoch 49 - batch 440 - loss 0.9076595306396484\n",
            "epoch 49 - batch 450 - loss 0.9967976808547974\n",
            "epoch 49 - batch 460 - loss 0.8952969312667847\n",
            "epoch 49 - batch 470 - loss 0.9916462302207947\n",
            "epoch 49 - batch 480 - loss 0.9821823835372925\n",
            "epoch 49 - batch 490 - loss 0.9468351602554321\n",
            "epoch 49 - batch 500 - loss 0.9062355756759644\n",
            "epoch 49 - batch 510 - loss 0.9266749620437622\n",
            "epoch 49 - batch 520 - loss 0.9288263320922852\n",
            "epoch 49 - batch 530 - loss 0.9481867551803589\n",
            "epoch 49 - batch 540 - loss 0.9185584187507629\n",
            "epoch 49 - batch 550 - loss 0.952451765537262\n",
            "epoch 49 - batch 560 - loss 0.9365260004997253\n",
            "epoch 49 - batch 570 - loss 0.9691809415817261\n",
            "epoch 49 - batch 580 - loss 0.9999145865440369\n",
            "epoch 49 - batch 590 - loss 1.0207569599151611\n",
            "epoch 49 - batch 600 - loss 0.8888211846351624\n",
            "epoch 49 - batch 610 - loss 1.0236178636550903\n",
            "epoch 49 training time: 220.88983583450317 sec\n",
            "evaluation of batch 0 took: 0.16409993171691895\n",
            "evaluation of batch 50 took: 0.16241192817687988\n",
            "evaluation of batch 100 took: 0.16251325607299805\n",
            "evaluation of batch 150 took: 0.15941786766052246\n",
            "evaluation of batch 200 took: 0.15714716911315918\n",
            "evaluation of batch 250 took: 0.15804123878479004\n",
            "evaluation of batch 300 took: 0.16031336784362793\n",
            "evaluation of batch 350 took: 0.15766191482543945\n",
            "evaluation of batch 400 took: 0.16093206405639648\n",
            "evaluation of batch 450 took: 0.16493701934814453\n",
            "evaluation of batch 500 took: 0.15290331840515137\n",
            "evaluation of batch 550 took: 0.16008591651916504\n",
            "evaluation of batch 600 took: 0.1623532772064209\n",
            "epoch 49 evaluation on training data time: 100.31471705436707 sec\n",
            "evaluation of batch 0 took: 0.1656501293182373\n",
            "evaluation of batch 50 took: 0.16423821449279785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 49 evaluation on test data time: 22.487233877182007 sec\n",
            "epoch evaluation:  {'epoch': 49, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.2651151>, 'test_rouge_1_p': 0.5723172010281387, 'test_rouge_1_r': 0.5352513697240263, 'test_rouge_1_f1': 0.5443317566947842, 'test_rouge_2_p': 0.30584035105519475, 'test_rouge_2_r': 0.2927647710362554, 'test_rouge_2_f1': 0.29594097829817184, 'test_rouge_3_p': 0.1351146086985931, 'test_rouge_3_r': 0.128620890827922, 'test_rouge_3_f1': 0.13018476981228094, 'test_rouge_L_p': 0.5705356538922387, 'test_rouge_L_r': 0.5338425711193568, 'test_rouge_L_f1': 0.5427999429857433}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 50 - batch 10 - loss 0.950724720954895\n",
            "epoch 50 - batch 20 - loss 0.9580774307250977\n",
            "epoch 50 - batch 30 - loss 0.9733214378356934\n",
            "epoch 50 - batch 40 - loss 0.9872172474861145\n",
            "epoch 50 - batch 50 - loss 0.9304069876670837\n",
            "epoch 50 - batch 60 - loss 0.951403796672821\n",
            "epoch 50 - batch 70 - loss 1.0181962251663208\n",
            "epoch 50 - batch 80 - loss 0.9546391367912292\n",
            "epoch 50 - batch 90 - loss 0.9173582196235657\n",
            "epoch 50 - batch 100 - loss 0.9559993743896484\n",
            "epoch 50 - batch 110 - loss 0.9671520590782166\n",
            "epoch 50 - batch 120 - loss 0.9621896147727966\n",
            "epoch 50 - batch 130 - loss 0.901062548160553\n",
            "epoch 50 - batch 140 - loss 0.9035749435424805\n",
            "epoch 50 - batch 150 - loss 0.947558581829071\n",
            "epoch 50 - batch 160 - loss 0.9556016325950623\n",
            "epoch 50 - batch 170 - loss 0.9278303980827332\n",
            "epoch 50 - batch 180 - loss 0.9166904091835022\n",
            "epoch 50 - batch 190 - loss 0.9502372741699219\n",
            "epoch 50 - batch 200 - loss 0.9440442323684692\n",
            "epoch 50 - batch 210 - loss 0.9642869234085083\n",
            "epoch 50 - batch 220 - loss 0.9516928791999817\n",
            "epoch 50 - batch 230 - loss 0.879956841468811\n",
            "epoch 50 - batch 240 - loss 0.9319602847099304\n",
            "epoch 50 - batch 250 - loss 1.0089434385299683\n",
            "epoch 50 - batch 260 - loss 0.9175083041191101\n",
            "epoch 50 - batch 270 - loss 0.9417707324028015\n",
            "epoch 50 - batch 280 - loss 0.9837698340415955\n",
            "epoch 50 - batch 290 - loss 0.9775065779685974\n",
            "epoch 50 - batch 300 - loss 1.0028682947158813\n",
            "epoch 50 - batch 310 - loss 0.8864200711250305\n",
            "epoch 50 - batch 320 - loss 0.9500375986099243\n",
            "epoch 50 - batch 330 - loss 0.927690327167511\n",
            "epoch 50 - batch 340 - loss 0.9598673582077026\n",
            "epoch 50 - batch 350 - loss 0.9537218809127808\n",
            "epoch 50 - batch 360 - loss 0.9528139233589172\n",
            "epoch 50 - batch 370 - loss 0.9795676469802856\n",
            "epoch 50 - batch 380 - loss 0.9269469976425171\n",
            "epoch 50 - batch 390 - loss 0.937346875667572\n",
            "epoch 50 - batch 400 - loss 0.9479238390922546\n",
            "epoch 50 - batch 410 - loss 0.935668408870697\n",
            "epoch 50 - batch 420 - loss 0.8934023976325989\n",
            "epoch 50 - batch 430 - loss 0.9935530424118042\n",
            "epoch 50 - batch 440 - loss 1.0440155267715454\n",
            "epoch 50 - batch 450 - loss 0.9791640043258667\n",
            "epoch 50 - batch 460 - loss 0.926697850227356\n",
            "epoch 50 - batch 470 - loss 0.9241803288459778\n",
            "epoch 50 - batch 480 - loss 0.9556711316108704\n",
            "epoch 50 - batch 490 - loss 0.9065626263618469\n",
            "epoch 50 - batch 500 - loss 0.9032368063926697\n",
            "epoch 50 - batch 510 - loss 1.0122038125991821\n",
            "epoch 50 - batch 520 - loss 0.9251143336296082\n",
            "epoch 50 - batch 530 - loss 0.9653794169425964\n",
            "epoch 50 - batch 540 - loss 0.9787346720695496\n",
            "epoch 50 - batch 550 - loss 0.9115657806396484\n",
            "epoch 50 - batch 560 - loss 0.9723485112190247\n",
            "epoch 50 - batch 570 - loss 0.9824748039245605\n",
            "epoch 50 - batch 580 - loss 0.9368831515312195\n",
            "epoch 50 - batch 590 - loss 0.9175930619239807\n",
            "epoch 50 - batch 600 - loss 0.9307878613471985\n",
            "epoch 50 - batch 610 - loss 0.9271859526634216\n",
            "epoch 50 training time: 220.60182332992554 sec\n",
            "evaluation of batch 0 took: 0.157789945602417\n",
            "evaluation of batch 50 took: 0.17722320556640625\n",
            "evaluation of batch 100 took: 0.16913318634033203\n",
            "evaluation of batch 150 took: 0.1744678020477295\n",
            "evaluation of batch 200 took: 0.15451669692993164\n",
            "evaluation of batch 250 took: 0.15714502334594727\n",
            "evaluation of batch 300 took: 0.16257023811340332\n",
            "evaluation of batch 350 took: 0.1560378074645996\n",
            "evaluation of batch 400 took: 0.1554725170135498\n",
            "evaluation of batch 450 took: 0.16483116149902344\n",
            "evaluation of batch 500 took: 0.16663384437561035\n",
            "evaluation of batch 550 took: 0.16577863693237305\n",
            "evaluation of batch 600 took: 0.1608870029449463\n",
            "epoch 50 evaluation on training data time: 100.34300804138184 sec\n",
            "evaluation of batch 0 took: 0.1744997501373291\n",
            "evaluation of batch 50 took: 0.16724610328674316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 50 evaluation on test data time: 22.752963304519653 sec\n",
            "epoch evaluation:  {'epoch': 50, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=1.265295>, 'test_rouge_1_p': 0.5710893714730211, 'test_rouge_1_r': 0.5373749251120902, 'test_rouge_1_f1': 0.5448867129753139, 'test_rouge_2_p': 0.30549390388257586, 'test_rouge_2_r': 0.29448453564664495, 'test_rouge_2_f1': 0.29676273493428024, 'test_rouge_3_p': 0.13609814664502168, 'test_rouge_3_r': 0.13055541463744588, 'test_rouge_3_f1': 0.1317300174094259, 'test_rouge_L_p': 0.5691989952941404, 'test_rouge_L_r': 0.5358522546092299, 'test_rouge_L_f1': 0.5432466466360792}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-18\n",
            "Done saving model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "00-colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpetkoff/identifier-suggestion/blob/master/notebooks/10-colab-random-search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeJd9knlVGS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import utilities\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from subprocess import check_output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntLDgQhocMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "2f07da3e-873f-4c68-d762-92586d476ad8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul  3 16:20:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_PqX9NDdEJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "cfc37831-26eb-4664-a25a-08cdbf1dbb38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvcl4iohdWEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97510474-6526-476d-da56-c86c41330cc0"
      },
      "source": [
        "%env WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
        "\n",
        "# TODO: how can one read an environment variable?!?\n",
        "%cd '/content/gdrive/My Drive/src'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WORKSPACE_DIR=/content/gdrive/My Drive/src\n",
            "/content/gdrive/My Drive/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNR4ZqljgeyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timestamp = check_output(['date', '-u', \"+%Y-%m-%dT%H-%M-%S\"]).decode('utf-8').strip()\n",
        "\n",
        "os.environ['PROJECT_DIR'] = os.path.join(\n",
        "    os.environ['WORKSPACE_DIR'],\n",
        "    f'identifier-suggestion-{timestamp}',\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PoI39h1dEJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "b785440a-fd01-4828-c78a-64e4d0c0fbd0"
      },
      "source": [
        "!git clone https://github.com/antonpetkoff/identifier-suggestion.git --depth 1 \"${PROJECT_DIR}\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/gdrive/My Drive/src/identifier-suggestion-2020-07-03T16-30-04'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 117 (delta 2), reused 68 (delta 1), pack-reused 0\n",
            "Receiving objects: 100% (117/117), 937.84 KiB | 9.67 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mki1yqMgixt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(os.environ['PROJECT_DIR'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjKIGx4i2dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c4bd2bcf-ee92-41dd-ff42-a961c8483846"
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/src/identifier-suggestion-2020-07-03T16-30-04\n",
            "total 73\n",
            "drwx------  4 root root  4096 Jul  3 16:30 data\n",
            "drwx------  2 root root  4096 Jul  3 16:30 experiments\n",
            "-rw-------  1 root root 35149 Jul  3 16:30 LICENSE\n",
            "drwx------  2 root root  4096 Jul  3 16:30 notebooks\n",
            "-rw-------  1 root root 10478 Jul  3 16:30 README.md\n",
            "drwx------  3 root root  4096 Jul  3 16:30 reports\n",
            "drwx------  2 root root  4096 Jul  3 16:30 requirements\n",
            "drwx------ 13 root root  4096 Jul  3 16:30 src\n",
            "drwx------  3 root root  4096 Jul  3 16:30 vscode-extension\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCr-lrn4SRhW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bb10bf11-ef2f-4a49-85f2-bb58e051fe82"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M7BkvE4dEJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "c98f7b7d-7577-476d-97f6-ba94999c07a2"
      },
      "source": [
        "# Google Colab has standard libraries like numpy, pandas, matplotlib and TF (of course) pre-installed\n",
        "!pip install -r requirements/colab.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: javalang==0.13.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 1)) (0.13.0)\n",
            "Requirement already satisfied: pydash==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: python-dotenv==0.13.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 3)) (0.13.0)\n",
            "Requirement already satisfied: wandb==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 4)) (0.9.1)\n",
            "Requirement already satisfied: tables==3.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: rouge-score==0.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements/colab.txt (line 6)) (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from javalang==0.13.0->-r requirements/colab.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.5.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (5.0.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (5.4.8)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (0.10.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (7.352.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (0.15.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables==3.6.1->-r requirements/colab.txt (line 5)) (1.18.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score==0.0.4->-r requirements/colab.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (1.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (4.0.5)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb==0.9.1->-r requirements/colab.txt (line 4)) (0.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (2.9)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.9.1->-r requirements/colab.txt (line 4)) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASrqt3o4TP5E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea6f3b50-f759-42b5-a271-d785cfc612e7"
      },
      "source": [
        "# provide secrets to the project, e.g. access to wandb\n",
        "shutil.copy(\n",
        "    os.path.join(os.environ['WORKSPACE_DIR'], 'secrets/.env'),\n",
        "    os.environ['PROJECT_DIR']\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/src/identifier-suggestion-2020-07-02T18-36-13/.env'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBhbZ4muuveC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# monkey-patch (mock) os.symlink to be a noop, because wandb.save() uses it, but it is not supported by Google Colab Notebooks\n",
        "os.symlink = lambda *x: print('Executing mocked noop symlink with arguments', x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHkwwGMPts7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0546f165-edec-43d2-ebb3-9817ae4efbf4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcI5mkG8fRXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RANDOM_SEARCH_ID = 1\n",
        "EXPERIMENT_ID = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RjqyY0ifdYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_random_search_experiment_params(random_search_id, experiment_id):\n",
        "  with open(f'../data/experiments/random_search_{random_search_id}.json', 'r') as f:\n",
        "    experiments = json.load(f)\n",
        "    loaded_params = experiments[f'experiment_{experiment_id}']\n",
        "    id = f'0{experiment_id}' if experiment_id < 10 else experiment_id\n",
        "    \n",
        "    return {\n",
        "      'dir_data': '../data/processed/subtoken/', # get the input data from the shared directory\n",
        "      'file_checkpoint_dir': 'models/checkpoints/baseline/',\n",
        "      'dir_preprocessed_data': 'data/processed/seq2seq/',\n",
        "\n",
        "      'experiment_name': f'rs_{random_search_id}_{id}',\n",
        "      'max_input_length': loaded_params['max_input_seq_length'],\n",
        "      'max_output_length': loaded_params['max_output_seq_length'],\n",
        "      'input_vocab_size': loaded_params['vocabulary_size'],\n",
        "      'output_vocab_size': loaded_params['vocabulary_size'],\n",
        "      'input_embedding_dim': loaded_params['embedding_dim'],\n",
        "      'output_embedding_dim': loaded_params['embedding_dim'],\n",
        "      'latent_dim': loaded_params['latent_dim'],\n",
        "      'learning_rate': loaded_params['learning_rate'],\n",
        "      'dropout_rate': loaded_params['dropout_rate'],\n",
        "      'batch_size': loaded_params['batch_size'],\n",
        "\n",
        "      'epochs': 200,\n",
        "      'early_stopping_patience': 3,\n",
        "      'early_stopping_min_delta': 0.0,\n",
        "      'evaluation_dataset': 'validation',\n",
        "      'random_seed': 1,\n",
        "    }"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY3fkLJIm7ja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8616ef86-2fd2-4788-c3c9-f23de599a5f3"
      },
      "source": [
        "from argparse import Namespace\n",
        "from src.pipelines.baseline import run\n",
        "\n",
        "params = load_random_search_experiment_params(\n",
        "    random_search_id=1,\n",
        "    experiment_id=0\n",
        ")\n",
        "\n",
        "print('params', params)\n",
        "\n",
        "run(Namespace(**params))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing logger\n",
            "Current working directory: /content/gdrive/My Drive/src/identifier-suggestion-2020-07-02T18-36-13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/31wky7nm\" target=\"_blank\">https://app.wandb.ai/antonpetkoff/identifier-suggestion/runs/31wky7nm</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.2 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Experiment parameters:  Namespace(batch_size=512, dir_data='../data/processed/subtoken/', dir_preprocessed_data='data/processed/seq2seq/', dropout_rate=0.130622, early_stopping_min_delta=0.0, early_stopping_patience=4, epochs=200, evaluation_dataset='validation', experiment_name='random_search_0_10', file_checkpoint_dir='models/checkpoints/baseline/', input_embedding_dim=96, input_vocab_size=6600, latent_dim=1024, learning_rate=0.046641, max_input_length=80, max_output_length=7, output_embedding_dim=96, output_vocab_size=6600, random_seed=1)\n",
            "Preprocessed files not found. Preprocessing...\n",
            "Reading input files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 45267/633714 [00:00<00:01, 452659.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding <start> and <end> markers to output sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 633714/633714 [00:02<00:00, 286926.15it/s]\n",
            "  0%|          | 0/79267 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding <start> and <end> markers to output sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79267/79267 [00:01<00:00, 57240.49it/s]\n",
            "100%|██████████| 78990/78990 [00:00<00:00, 540228.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding <start> and <end> markers to output sequences\n",
            "Building input vocabulary\n",
            "Building output vocabulary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 468/633714 [00:00<02:15, 4660.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Encoding and padding training data...\n",
            "Encoding input sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 633714/633714 [01:42<00:00, 6173.64it/s]\n",
            "  1%|          | 4374/633714 [00:00<00:14, 43735.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inputs after tokenizer 0                                       [7]\n",
            "1                      [7, 7, 13, 42, 5, 8]\n",
            "2    [7, 13, 1292, 601, 4, 162, 3, 2, 5, 8]\n",
            "Name: inputs, dtype: object\n",
            "Encoding output sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 633714/633714 [00:12<00:00, 50254.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "outputs after tokenizer 0    [2, 5, 9, 3705, 2021, 1096, 5, 630, 3]\n",
            "1              [2, 10, 469, 12, 50, 995, 3]\n",
            "2                     [2, 4, 2364, 1714, 3]\n",
            "Name: outputs, dtype: object\n",
            "Padding and aligning input sequences\n",
            "inputs after padding 0    [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1    [7, 7, 13, 42, 5, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "2    [7, 13, 1292, 601, 4, 162, 3, 2, 5, 8, 0, 0, 0...\n",
            "Name: inputs, dtype: object\n",
            "Padding and aligning output sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 675/79267 [00:00<00:11, 6713.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Outputs after padding:  0    [2, 5, 9, 3705, 2021, 1096, 5]\n",
            "1      [2, 10, 469, 12, 50, 995, 3]\n",
            "2       [2, 4, 2364, 1714, 3, 0, 0]\n",
            "Name: outputs, dtype: object\n",
            "Encoding and padding validation data...\n",
            "Encoding input sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79267/79267 [00:14<00:00, 5469.40it/s]\n",
            "  6%|▌         | 4840/79267 [00:00<00:01, 48399.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inputs after tokenizer 0    [7, 15, 32, 3, 3, 82, 2, 12, 6, 384, 3, 10, 2,...\n",
            "1                                                  [7]\n",
            "2                                    [7, 13, 86, 5, 8]\n",
            "Name: inputs, dtype: object\n",
            "Encoding output sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79267/79267 [00:01<00:00, 53222.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "outputs after tokenizer 0            [2, 5, 409, 3520, 3]\n",
            "1    [2, 14, 4761, 2442, 1743, 3]\n",
            "2                    [2, 1538, 3]\n",
            "Name: outputs, dtype: object\n",
            "Padding and aligning input sequences\n",
            "inputs after padding 0    [7, 15, 32, 3, 3, 82, 2, 12, 6, 384, 3, 10, 2,...\n",
            "1    [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "2    [7, 13, 86, 5, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
            "Name: inputs, dtype: object\n",
            "Padding and aligning output sequences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 650/78990 [00:00<00:12, 6458.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Outputs after padding:  0         [2, 5, 409, 3520, 3, 0, 0]\n",
            "1    [2, 14, 4761, 2442, 1743, 3, 0]\n",
            "2           [2, 1538, 3, 0, 0, 0, 0]\n",
            "Name: outputs, dtype: object\n",
            "Encoding and padding testing data...\n",
            "Encoding input sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78990/78990 [00:12<00:00, 6505.26it/s]\n",
            "  7%|▋         | 5288/78990 [00:00<00:01, 52876.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "inputs after tokenizer 0    [7, 52, 303, 52, 9, 45, 154, 4, 11, 52, 3, 27,...\n",
            "1    [7, 422, 4, 153, 217, 2604, 3, 852, 6, 221, 2,...\n",
            "2    [7, 19, 3, 1296, 1872, 130, 126, 1872, 2, 7, 1...\n",
            "Name: inputs, dtype: object\n",
            "Encoding output sequences into numbers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78990/78990 [00:01<00:00, 51203.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "outputs after tokenizer 0          [2, 102, 73, 3]\n",
            "1    [2, 32, 246, 2189, 3]\n",
            "2             [2, 6227, 3]\n",
            "Name: outputs, dtype: object\n",
            "Padding and aligning input sequences\n",
            "inputs after padding 0    [7, 52, 303, 52, 9, 45, 154, 4, 11, 52, 3, 27,...\n",
            "1    [7, 422, 4, 153, 217, 2604, 3, 852, 6, 221, 2,...\n",
            "2    [7, 19, 3, 1296, 1872, 130, 126, 1872, 2, 7, 1...\n",
            "Name: inputs, dtype: object\n",
            "Padding and aligning output sequences\n",
            "Outputs after padding:  0       [2, 102, 73, 3, 0, 0, 0]\n",
            "1    [2, 32, 246, 2189, 3, 0, 0]\n",
            "2       [2, 6227, 3, 0, 0, 0, 0]\n",
            "Name: outputs, dtype: object\n",
            "Done preprocessing\n",
            "Done preprocessing. Saving...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:2505: PerformanceWarning: \n",
            "your performance may suffer as PyTables will pickle object types that it cannot\n",
            "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['inputs', 'outputs'], dtype='object')]\n",
            "\n",
            "  encoding=encoding,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading preprocessed files...\n",
            "Loaded input vocabulary.\n",
            "Loaded output vocabulary.\n",
            "Loaded preprocessed files.\n",
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "EncoderEmbedding (Embedding) multiple                  633600    \n",
            "_________________________________________________________________\n",
            "EncoderLSTM (LSTM)           multiple                  4591616   \n",
            "=================================================================\n",
            "Total params: 5,225,216\n",
            "Trainable params: 5,225,216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "DecoderEmbedding (Embedding) multiple                  633600    \n",
            "_________________________________________________________________\n",
            "DecoderLSTM (LSTM)           multiple                  8785920   \n",
            "_________________________________________________________________\n",
            "DenseOutput (Dense)          multiple                  6765000   \n",
            "_________________________________________________________________\n",
            "bahdanau_attention (Bahdanau multiple                  2100225   \n",
            "=================================================================\n",
            "Total params: 18,284,745\n",
            "Trainable params: 18,284,745\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-1\n",
            "Executing mocked noop symlink with arguments ('/content/gdrive/My Drive/src/identifier-suggestion-2020-07-02T18-36-13/models/checkpoints/baseline/config.json', './reports/wandb/run-20200702_183625-31wky7nm/config.json')\n",
            "Done saving model\n",
            "Restored from models/checkpoints/baseline/ckpt-1\n",
            "epoch 1 - batch 10 - loss 35.4049186706543\n",
            "epoch 1 - batch 20 - loss 53.62553024291992\n",
            "epoch 1 - batch 30 - loss 57.17080307006836\n",
            "epoch 1 - batch 40 - loss 67.08588409423828\n",
            "epoch 1 - batch 50 - loss 78.29825592041016\n",
            "epoch 1 - batch 60 - loss 81.23928833007812\n",
            "epoch 1 - batch 70 - loss 92.82015228271484\n",
            "epoch 1 - batch 80 - loss 100.0181884765625\n",
            "epoch 1 - batch 90 - loss 101.5063705444336\n",
            "epoch 1 - batch 100 - loss 110.7083969116211\n",
            "epoch 1 - batch 110 - loss 122.42423248291016\n",
            "epoch 1 - batch 120 - loss 125.89956665039062\n",
            "epoch 1 - batch 130 - loss 128.6324005126953\n",
            "epoch 1 - batch 140 - loss 150.25990295410156\n",
            "epoch 1 - batch 150 - loss 138.43331909179688\n",
            "epoch 1 - batch 160 - loss 145.65347290039062\n",
            "epoch 1 - batch 170 - loss 149.97023010253906\n",
            "epoch 1 - batch 180 - loss 167.94406127929688\n",
            "epoch 1 - batch 190 - loss 163.0581817626953\n",
            "epoch 1 - batch 200 - loss 172.8438720703125\n",
            "epoch 1 - batch 210 - loss 157.36082458496094\n",
            "epoch 1 - batch 220 - loss 165.6527557373047\n",
            "epoch 1 - batch 230 - loss 163.06884765625\n",
            "epoch 1 - batch 240 - loss 171.3830108642578\n",
            "epoch 1 - batch 250 - loss 165.79200744628906\n",
            "epoch 1 - batch 260 - loss 188.79246520996094\n",
            "epoch 1 - batch 270 - loss 172.36753845214844\n",
            "epoch 1 - batch 280 - loss 175.26190185546875\n",
            "epoch 1 - batch 290 - loss 176.4477081298828\n",
            "epoch 1 - batch 300 - loss 188.34373474121094\n",
            "epoch 1 - batch 310 - loss 185.6024627685547\n",
            "epoch 1 - batch 320 - loss 182.06146240234375\n",
            "epoch 1 - batch 330 - loss 194.57421875\n",
            "epoch 1 - batch 340 - loss 184.87294006347656\n",
            "epoch 1 - batch 350 - loss 200.8309783935547\n",
            "epoch 1 - batch 360 - loss 198.72251892089844\n",
            "epoch 1 - batch 370 - loss 197.9992218017578\n",
            "epoch 1 - batch 380 - loss 205.37640380859375\n",
            "epoch 1 - batch 390 - loss 207.41175842285156\n",
            "epoch 1 - batch 400 - loss 198.0900421142578\n",
            "epoch 1 - batch 410 - loss 202.0621337890625\n",
            "epoch 1 - batch 420 - loss 209.17535400390625\n",
            "epoch 1 - batch 430 - loss 205.6812744140625\n",
            "epoch 1 - batch 440 - loss 198.4298858642578\n",
            "epoch 1 - batch 450 - loss 192.4879913330078\n",
            "epoch 1 - batch 460 - loss 198.96299743652344\n",
            "epoch 1 - batch 470 - loss 202.73533630371094\n",
            "epoch 1 - batch 480 - loss 197.2840118408203\n",
            "epoch 1 - batch 490 - loss 191.355224609375\n",
            "epoch 1 - batch 500 - loss 199.2111053466797\n",
            "epoch 1 - batch 510 - loss 196.09637451171875\n",
            "epoch 1 - batch 520 - loss 196.84107971191406\n",
            "epoch 1 - batch 530 - loss 205.6785125732422\n",
            "epoch 1 - batch 540 - loss 193.67674255371094\n",
            "epoch 1 - batch 550 - loss 204.67767333984375\n",
            "epoch 1 - batch 560 - loss 201.6717071533203\n",
            "epoch 1 - batch 570 - loss 205.9155731201172\n",
            "epoch 1 - batch 580 - loss 210.78858947753906\n",
            "epoch 1 - batch 590 - loss 203.9191436767578\n",
            "epoch 1 - batch 600 - loss 208.916259765625\n",
            "epoch 1 - batch 610 - loss 198.0814208984375\n",
            "epoch 1 - batch 620 - loss 192.6916046142578\n",
            "epoch 1 - batch 630 - loss 199.1697235107422\n",
            "epoch 1 - batch 640 - loss 205.73536682128906\n",
            "epoch 1 - batch 650 - loss 196.6314697265625\n",
            "epoch 1 - batch 660 - loss 204.0599365234375\n",
            "epoch 1 - batch 670 - loss 210.96498107910156\n",
            "epoch 1 - batch 680 - loss 185.59776306152344\n",
            "epoch 1 - batch 690 - loss 203.9365234375\n",
            "epoch 1 - batch 700 - loss 217.87596130371094\n",
            "epoch 1 - batch 710 - loss 192.7379608154297\n",
            "epoch 1 - batch 720 - loss 212.02232360839844\n",
            "epoch 1 - batch 730 - loss 196.7976531982422\n",
            "epoch 1 - batch 740 - loss 207.2462158203125\n",
            "epoch 1 - batch 750 - loss 217.6873321533203\n",
            "epoch 1 - batch 760 - loss 197.1271209716797\n",
            "epoch 1 - batch 770 - loss 191.4540557861328\n",
            "epoch 1 - batch 780 - loss 209.1171417236328\n",
            "epoch 1 - batch 790 - loss 216.52635192871094\n",
            "epoch 1 - batch 800 - loss 199.2664031982422\n",
            "epoch 1 - batch 810 - loss 204.040283203125\n",
            "epoch 1 - batch 820 - loss 194.6258087158203\n",
            "epoch 1 - batch 830 - loss 202.3541259765625\n",
            "epoch 1 - batch 840 - loss 184.9403533935547\n",
            "epoch 1 - batch 850 - loss 189.0726318359375\n",
            "epoch 1 - batch 860 - loss 180.2261962890625\n",
            "epoch 1 - batch 870 - loss 207.8475799560547\n",
            "epoch 1 - batch 880 - loss 184.96600341796875\n",
            "epoch 1 - batch 890 - loss 202.19720458984375\n",
            "epoch 1 - batch 900 - loss 209.1162567138672\n",
            "epoch 1 - batch 910 - loss 209.07395935058594\n",
            "epoch 1 - batch 920 - loss 190.6430206298828\n",
            "epoch 1 - batch 930 - loss 205.1825408935547\n",
            "epoch 1 - batch 940 - loss 203.0810546875\n",
            "epoch 1 - batch 950 - loss 189.44432067871094\n",
            "epoch 1 - batch 960 - loss 208.25250244140625\n",
            "epoch 1 - batch 970 - loss 193.14076232910156\n",
            "epoch 1 - batch 980 - loss 206.53392028808594\n",
            "epoch 1 - batch 990 - loss 204.2202911376953\n",
            "epoch 1 - batch 1000 - loss 195.90802001953125\n",
            "epoch 1 - batch 1010 - loss 202.0497589111328\n",
            "epoch 1 - batch 1020 - loss 179.6067352294922\n",
            "epoch 1 - batch 1030 - loss 205.03224182128906\n",
            "epoch 1 - batch 1040 - loss 189.9212646484375\n",
            "epoch 1 - batch 1050 - loss 196.92962646484375\n",
            "epoch 1 - batch 1060 - loss 209.73521423339844\n",
            "epoch 1 - batch 1070 - loss 183.8557891845703\n",
            "epoch 1 - batch 1080 - loss 203.31947326660156\n",
            "epoch 1 - batch 1090 - loss 196.1144561767578\n",
            "epoch 1 - batch 1100 - loss 193.556640625\n",
            "epoch 1 - batch 1110 - loss 191.889404296875\n",
            "epoch 1 - batch 1120 - loss 190.6894989013672\n",
            "epoch 1 - batch 1130 - loss 198.193115234375\n",
            "epoch 1 - batch 1140 - loss 185.64849853515625\n",
            "epoch 1 - batch 1150 - loss 177.6798858642578\n",
            "epoch 1 - batch 1160 - loss 180.3693389892578\n",
            "epoch 1 - batch 1170 - loss 183.76609802246094\n",
            "epoch 1 - batch 1180 - loss 183.58270263671875\n",
            "epoch 1 - batch 1190 - loss 179.2990264892578\n",
            "epoch 1 - batch 1200 - loss 180.19273376464844\n",
            "epoch 1 - batch 1210 - loss 180.2344207763672\n",
            "epoch 1 - batch 1220 - loss 182.36534118652344\n",
            "epoch 1 - batch 1230 - loss 174.7804412841797\n",
            "epoch 1 training time: 677.4388749599457 sec\n",
            "evaluation of batch 0 took: 0.1135854721069336\n",
            "evaluation of batch 50 took: 0.12686824798583984\n",
            "evaluation of batch 100 took: 0.12101054191589355\n",
            "evaluation of batch 150 took: 0.12114500999450684\n",
            "evaluation of batch 200 took: 0.053991079330444336\n",
            "evaluation of batch 250 took: 0.05125856399536133\n",
            "evaluation of batch 300 took: 0.05170440673828125\n",
            "evaluation of batch 350 took: 0.0719907283782959\n",
            "evaluation of batch 400 took: 0.12670564651489258\n",
            "evaluation of batch 450 took: 0.13669633865356445\n",
            "evaluation of batch 500 took: 0.12531208992004395\n",
            "evaluation of batch 550 took: 0.11895608901977539\n",
            "evaluation of batch 600 took: 0.052130937576293945\n",
            "evaluation of batch 650 took: 0.1324782371520996\n",
            "evaluation of batch 700 took: 0.11701560020446777\n",
            "evaluation of batch 750 took: 0.052030086517333984\n",
            "evaluation of batch 800 took: 0.13690686225891113\n",
            "evaluation of batch 850 took: 0.1138916015625\n",
            "evaluation of batch 900 took: 0.12946772575378418\n",
            "evaluation of batch 950 took: 0.11776542663574219\n",
            "evaluation of batch 1000 took: 0.052825212478637695\n",
            "evaluation of batch 1050 took: 0.1202847957611084\n",
            "evaluation of batch 1100 took: 0.1251821517944336\n",
            "evaluation of batch 1150 took: 0.1331651210784912\n",
            "evaluation of batch 1200 took: 0.13764262199401855\n",
            "epoch 1 evaluation on training data time: 121.78072953224182 sec\n",
            "evaluation of batch 0 took: 0.06028485298156738\n",
            "evaluation of batch 50 took: 0.0504460334777832\n",
            "evaluation of batch 100 took: 0.05022263526916504\n",
            "evaluation of batch 150 took: 0.049935340881347656\n",
            "epoch 1 evaluation on test data time: 36.95553493499756 sec\n",
            "epoch evaluation:  {'epoch': 1, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=174.961>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "Increasing epochs without improvement to: 1\n",
            "epoch 2 - batch 10 - loss 166.2401123046875\n",
            "epoch 2 - batch 20 - loss 167.7519073486328\n",
            "epoch 2 - batch 30 - loss 186.4810028076172\n",
            "epoch 2 - batch 40 - loss 172.3791961669922\n",
            "epoch 2 - batch 50 - loss 174.0639190673828\n",
            "epoch 2 - batch 60 - loss 175.35643005371094\n",
            "epoch 2 - batch 70 - loss 162.26158142089844\n",
            "epoch 2 - batch 80 - loss 184.7918701171875\n",
            "epoch 2 - batch 90 - loss 168.76112365722656\n",
            "epoch 2 - batch 100 - loss 177.04833984375\n",
            "epoch 2 - batch 110 - loss 171.7364501953125\n",
            "epoch 2 - batch 120 - loss 171.98797607421875\n",
            "epoch 2 - batch 130 - loss 172.6795654296875\n",
            "epoch 2 - batch 140 - loss 160.65966796875\n",
            "epoch 2 - batch 150 - loss 185.6064453125\n",
            "epoch 2 - batch 160 - loss 175.3385772705078\n",
            "epoch 2 - batch 170 - loss 174.8080596923828\n",
            "epoch 2 - batch 180 - loss 196.8108367919922\n",
            "epoch 2 - batch 190 - loss 172.42755126953125\n",
            "epoch 2 - batch 200 - loss 175.0019989013672\n",
            "epoch 2 - batch 210 - loss 177.96360778808594\n",
            "epoch 2 - batch 220 - loss 185.1326904296875\n",
            "epoch 2 - batch 230 - loss 179.80284118652344\n",
            "epoch 2 - batch 240 - loss 180.3688507080078\n",
            "epoch 2 - batch 250 - loss 176.02972412109375\n",
            "epoch 2 - batch 260 - loss 176.36830139160156\n",
            "epoch 2 - batch 270 - loss 174.26686096191406\n",
            "epoch 2 - batch 280 - loss 178.5330352783203\n",
            "epoch 2 - batch 290 - loss 162.3755645751953\n",
            "epoch 2 - batch 300 - loss 171.42572021484375\n",
            "epoch 2 - batch 310 - loss 176.10687255859375\n",
            "epoch 2 - batch 320 - loss 171.3948974609375\n",
            "epoch 2 - batch 330 - loss 198.8511199951172\n",
            "epoch 2 - batch 340 - loss 176.066162109375\n",
            "epoch 2 - batch 350 - loss 188.76597595214844\n",
            "epoch 2 - batch 360 - loss 176.1657257080078\n",
            "epoch 2 - batch 370 - loss 169.5943603515625\n",
            "epoch 2 - batch 380 - loss 174.7965087890625\n",
            "epoch 2 - batch 390 - loss 177.81434631347656\n",
            "epoch 2 - batch 400 - loss 180.83485412597656\n",
            "epoch 2 - batch 410 - loss 159.08184814453125\n",
            "epoch 2 - batch 420 - loss 186.20794677734375\n",
            "epoch 2 - batch 430 - loss 179.8695831298828\n",
            "epoch 2 - batch 440 - loss 176.8328857421875\n",
            "epoch 2 - batch 450 - loss 155.78152465820312\n",
            "epoch 2 - batch 460 - loss 169.02130126953125\n",
            "epoch 2 - batch 470 - loss 171.78797912597656\n",
            "epoch 2 - batch 480 - loss 171.3000030517578\n",
            "epoch 2 - batch 490 - loss 179.11907958984375\n",
            "epoch 2 - batch 500 - loss 172.1885528564453\n",
            "epoch 2 - batch 510 - loss 178.6360321044922\n",
            "epoch 2 - batch 520 - loss 164.7469940185547\n",
            "epoch 2 - batch 530 - loss 176.6367950439453\n",
            "epoch 2 - batch 540 - loss 176.3334503173828\n",
            "epoch 2 - batch 550 - loss 172.89642333984375\n",
            "epoch 2 - batch 560 - loss 180.4625244140625\n",
            "epoch 2 - batch 570 - loss 190.0964813232422\n",
            "epoch 2 - batch 580 - loss 169.66258239746094\n",
            "epoch 2 - batch 590 - loss 161.51707458496094\n",
            "epoch 2 - batch 600 - loss 172.5351104736328\n",
            "epoch 2 - batch 610 - loss 167.6061248779297\n",
            "epoch 2 - batch 620 - loss 157.50013732910156\n",
            "epoch 2 - batch 630 - loss 190.55137634277344\n",
            "epoch 2 - batch 640 - loss 176.92640686035156\n",
            "epoch 2 - batch 650 - loss 177.17628479003906\n",
            "epoch 2 - batch 660 - loss 172.3708038330078\n",
            "epoch 2 - batch 670 - loss 175.69871520996094\n",
            "epoch 2 - batch 680 - loss 171.0767822265625\n",
            "epoch 2 - batch 690 - loss 152.53172302246094\n",
            "epoch 2 - batch 700 - loss 153.4682159423828\n",
            "epoch 2 - batch 710 - loss 175.9691619873047\n",
            "epoch 2 - batch 720 - loss 175.74725341796875\n",
            "epoch 2 - batch 730 - loss 154.5965576171875\n",
            "epoch 2 - batch 740 - loss 166.39755249023438\n",
            "epoch 2 - batch 750 - loss 172.1256103515625\n",
            "epoch 2 - batch 760 - loss 155.52081298828125\n",
            "epoch 2 - batch 770 - loss 187.25477600097656\n",
            "epoch 2 - batch 780 - loss 176.5368194580078\n",
            "epoch 2 - batch 790 - loss 167.47842407226562\n",
            "epoch 2 - batch 800 - loss 160.8447723388672\n",
            "epoch 2 - batch 810 - loss 167.461181640625\n",
            "epoch 2 - batch 820 - loss 178.05914306640625\n",
            "epoch 2 - batch 830 - loss 182.61546325683594\n",
            "epoch 2 - batch 840 - loss 174.28428649902344\n",
            "epoch 2 - batch 850 - loss 167.4096221923828\n",
            "epoch 2 - batch 860 - loss 186.9661865234375\n",
            "epoch 2 - batch 870 - loss 173.6085662841797\n",
            "epoch 2 - batch 880 - loss 161.91981506347656\n",
            "epoch 2 - batch 890 - loss 165.5172119140625\n",
            "epoch 2 - batch 900 - loss 175.60284423828125\n",
            "epoch 2 - batch 910 - loss 173.0980987548828\n",
            "epoch 2 - batch 920 - loss 165.4685821533203\n",
            "epoch 2 - batch 930 - loss 166.14048767089844\n",
            "epoch 2 - batch 940 - loss 166.73252868652344\n",
            "epoch 2 - batch 950 - loss 157.14462280273438\n",
            "epoch 2 - batch 960 - loss 161.6400604248047\n",
            "epoch 2 - batch 970 - loss 171.22203063964844\n",
            "epoch 2 - batch 980 - loss 175.5038299560547\n",
            "epoch 2 - batch 990 - loss 175.62103271484375\n",
            "epoch 2 - batch 1000 - loss 160.33103942871094\n",
            "epoch 2 - batch 1010 - loss 158.03855895996094\n",
            "epoch 2 - batch 1020 - loss 176.3794403076172\n",
            "epoch 2 - batch 1030 - loss 181.3524169921875\n",
            "epoch 2 - batch 1040 - loss 166.9856719970703\n",
            "epoch 2 - batch 1050 - loss 163.57000732421875\n",
            "epoch 2 - batch 1060 - loss 163.93846130371094\n",
            "epoch 2 - batch 1070 - loss 171.86451721191406\n",
            "epoch 2 - batch 1080 - loss 171.98590087890625\n",
            "epoch 2 - batch 1090 - loss 176.03440856933594\n",
            "epoch 2 - batch 1100 - loss 166.91943359375\n",
            "epoch 2 - batch 1110 - loss 161.1732635498047\n",
            "epoch 2 - batch 1120 - loss 168.32984924316406\n",
            "epoch 2 - batch 1130 - loss 176.55628967285156\n",
            "epoch 2 - batch 1140 - loss 188.1070556640625\n",
            "epoch 2 - batch 1150 - loss 157.4911346435547\n",
            "epoch 2 - batch 1160 - loss 172.74363708496094\n",
            "epoch 2 - batch 1170 - loss 152.36460876464844\n",
            "epoch 2 - batch 1180 - loss 154.3839874267578\n",
            "epoch 2 - batch 1190 - loss 170.93019104003906\n",
            "epoch 2 - batch 1200 - loss 168.7991180419922\n",
            "epoch 2 - batch 1210 - loss 170.41883850097656\n",
            "epoch 2 - batch 1220 - loss 183.90625\n",
            "epoch 2 - batch 1230 - loss 174.4374237060547\n",
            "epoch 2 training time: 677.137610912323 sec\n",
            "evaluation of batch 0 took: 0.05205821990966797\n",
            "evaluation of batch 50 took: 0.049237966537475586\n",
            "evaluation of batch 100 took: 0.12996864318847656\n",
            "evaluation of batch 150 took: 0.0711815357208252\n",
            "evaluation of batch 200 took: 0.13326334953308105\n",
            "evaluation of batch 250 took: 0.12037491798400879\n",
            "evaluation of batch 300 took: 0.050527095794677734\n",
            "evaluation of batch 350 took: 0.11783957481384277\n",
            "evaluation of batch 400 took: 0.12733054161071777\n",
            "evaluation of batch 450 took: 0.05062603950500488\n",
            "evaluation of batch 500 took: 0.05317831039428711\n",
            "evaluation of batch 550 took: 0.052983999252319336\n",
            "evaluation of batch 600 took: 0.12030911445617676\n",
            "evaluation of batch 650 took: 0.12315869331359863\n",
            "evaluation of batch 700 took: 0.05152606964111328\n",
            "evaluation of batch 750 took: 0.07354521751403809\n",
            "evaluation of batch 800 took: 0.05307340621948242\n",
            "evaluation of batch 850 took: 0.051061153411865234\n",
            "evaluation of batch 900 took: 0.05035686492919922\n",
            "evaluation of batch 950 took: 0.05096101760864258\n",
            "evaluation of batch 1000 took: 0.07294392585754395\n",
            "evaluation of batch 1050 took: 0.0535740852355957\n",
            "evaluation of batch 1100 took: 0.051558494567871094\n",
            "evaluation of batch 1150 took: 0.1261749267578125\n",
            "evaluation of batch 1200 took: 0.12718534469604492\n",
            "epoch 2 evaluation on training data time: 113.59058332443237 sec\n",
            "evaluation of batch 0 took: 0.06098580360412598\n",
            "evaluation of batch 50 took: 0.053249359130859375\n",
            "evaluation of batch 100 took: 0.05179166793823242\n",
            "evaluation of batch 150 took: 0.04979538917541504\n",
            "epoch 2 evaluation on test data time: 37.435311794281006 sec\n",
            "epoch evaluation:  {'epoch': 2, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=171.59297>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "Increasing epochs without improvement to: 2\n",
            "epoch 3 - batch 10 - loss 163.57154846191406\n",
            "epoch 3 - batch 20 - loss 156.83111572265625\n",
            "epoch 3 - batch 30 - loss 189.3573760986328\n",
            "epoch 3 - batch 40 - loss 162.79014587402344\n",
            "epoch 3 - batch 50 - loss 166.7897491455078\n",
            "epoch 3 - batch 60 - loss 165.56263732910156\n",
            "epoch 3 - batch 70 - loss 190.47265625\n",
            "epoch 3 - batch 80 - loss 174.9716339111328\n",
            "epoch 3 - batch 90 - loss 157.07177734375\n",
            "epoch 3 - batch 100 - loss 171.8737030029297\n",
            "epoch 3 - batch 110 - loss 162.05917358398438\n",
            "epoch 3 - batch 120 - loss 172.21612548828125\n",
            "epoch 3 - batch 130 - loss 171.85960388183594\n",
            "epoch 3 - batch 140 - loss 162.0675506591797\n",
            "epoch 3 - batch 150 - loss 193.66815185546875\n",
            "epoch 3 - batch 160 - loss 162.3435821533203\n",
            "epoch 3 - batch 170 - loss 148.1751251220703\n",
            "epoch 3 - batch 180 - loss 168.92320251464844\n",
            "epoch 3 - batch 190 - loss 180.73748779296875\n",
            "epoch 3 - batch 200 - loss 179.5972137451172\n",
            "epoch 3 - batch 210 - loss 167.53587341308594\n",
            "epoch 3 - batch 220 - loss 164.32403564453125\n",
            "epoch 3 - batch 230 - loss 159.24388122558594\n",
            "epoch 3 - batch 240 - loss 174.0289306640625\n",
            "epoch 3 - batch 250 - loss 161.22198486328125\n",
            "epoch 3 - batch 260 - loss 148.38514709472656\n",
            "epoch 3 - batch 270 - loss 177.3221893310547\n",
            "epoch 3 - batch 280 - loss 166.90623474121094\n",
            "epoch 3 - batch 290 - loss 167.60731506347656\n",
            "epoch 3 - batch 300 - loss 150.74285888671875\n",
            "epoch 3 - batch 310 - loss 146.17430114746094\n",
            "epoch 3 - batch 320 - loss 158.45452880859375\n",
            "epoch 3 - batch 330 - loss 179.7985382080078\n",
            "epoch 3 - batch 340 - loss 167.14141845703125\n",
            "epoch 3 - batch 350 - loss 165.36302185058594\n",
            "epoch 3 - batch 360 - loss 152.18397521972656\n",
            "epoch 3 - batch 370 - loss 164.3748321533203\n",
            "epoch 3 - batch 380 - loss 164.44078063964844\n",
            "epoch 3 - batch 390 - loss 177.7721710205078\n",
            "epoch 3 - batch 400 - loss 193.17884826660156\n",
            "epoch 3 - batch 410 - loss 180.4237060546875\n",
            "epoch 3 - batch 420 - loss 181.09449768066406\n",
            "epoch 3 - batch 430 - loss 175.8919677734375\n",
            "epoch 3 - batch 440 - loss 158.89419555664062\n",
            "epoch 3 - batch 450 - loss 165.49029541015625\n",
            "epoch 3 - batch 460 - loss 174.1960906982422\n",
            "epoch 3 - batch 470 - loss 177.2178192138672\n",
            "epoch 3 - batch 480 - loss 167.2564697265625\n",
            "epoch 3 - batch 490 - loss 171.99205017089844\n",
            "epoch 3 - batch 500 - loss 173.699951171875\n",
            "epoch 3 - batch 510 - loss 179.29063415527344\n",
            "epoch 3 - batch 520 - loss 158.86529541015625\n",
            "epoch 3 - batch 530 - loss 178.9575958251953\n",
            "epoch 3 - batch 540 - loss 167.0839080810547\n",
            "epoch 3 - batch 550 - loss 159.18797302246094\n",
            "epoch 3 - batch 560 - loss 178.3223114013672\n",
            "epoch 3 - batch 570 - loss 151.6389923095703\n",
            "epoch 3 - batch 580 - loss 161.9779510498047\n",
            "epoch 3 - batch 590 - loss 148.68997192382812\n",
            "epoch 3 - batch 600 - loss 160.3212127685547\n",
            "epoch 3 - batch 610 - loss 160.02281188964844\n",
            "epoch 3 - batch 620 - loss 152.2477264404297\n",
            "epoch 3 - batch 630 - loss 156.985107421875\n",
            "epoch 3 - batch 640 - loss 160.6428680419922\n",
            "epoch 3 - batch 650 - loss 168.3955078125\n",
            "epoch 3 - batch 660 - loss 166.79759216308594\n",
            "epoch 3 - batch 670 - loss 164.90830993652344\n",
            "epoch 3 - batch 680 - loss 172.25929260253906\n",
            "epoch 3 - batch 690 - loss 157.27745056152344\n",
            "epoch 3 - batch 700 - loss 167.88543701171875\n",
            "epoch 3 - batch 710 - loss 171.92442321777344\n",
            "epoch 3 - batch 720 - loss 183.4054718017578\n",
            "epoch 3 - batch 730 - loss 150.66981506347656\n",
            "epoch 3 - batch 740 - loss 169.58558654785156\n",
            "epoch 3 - batch 750 - loss 165.66197204589844\n",
            "epoch 3 - batch 760 - loss 154.34039306640625\n",
            "epoch 3 - batch 770 - loss 175.80291748046875\n",
            "epoch 3 - batch 780 - loss 159.40689086914062\n",
            "epoch 3 - batch 790 - loss 164.59971618652344\n",
            "epoch 3 - batch 800 - loss 135.5386505126953\n",
            "epoch 3 - batch 810 - loss 154.21697998046875\n",
            "epoch 3 - batch 820 - loss 163.1988983154297\n",
            "epoch 3 - batch 830 - loss 155.8503875732422\n",
            "epoch 3 - batch 840 - loss 166.3347930908203\n",
            "epoch 3 - batch 850 - loss 134.58245849609375\n",
            "epoch 3 - batch 860 - loss 148.72930908203125\n",
            "epoch 3 - batch 870 - loss 147.35154724121094\n",
            "epoch 3 - batch 880 - loss 146.5803985595703\n",
            "epoch 3 - batch 890 - loss 146.30540466308594\n",
            "epoch 3 - batch 900 - loss 165.7268524169922\n",
            "epoch 3 - batch 910 - loss 154.72434997558594\n",
            "epoch 3 - batch 920 - loss 162.2738494873047\n",
            "epoch 3 - batch 930 - loss 177.22828674316406\n",
            "epoch 3 - batch 940 - loss 159.2738800048828\n",
            "epoch 3 - batch 950 - loss 142.31549072265625\n",
            "epoch 3 - batch 960 - loss 143.68556213378906\n",
            "epoch 3 - batch 970 - loss 167.57102966308594\n",
            "epoch 3 - batch 980 - loss 154.13914489746094\n",
            "epoch 3 - batch 990 - loss 152.25929260253906\n",
            "epoch 3 - batch 1000 - loss 145.0006866455078\n",
            "epoch 3 - batch 1010 - loss 166.57106018066406\n",
            "epoch 3 - batch 1020 - loss 157.96249389648438\n",
            "epoch 3 - batch 1030 - loss 164.92129516601562\n",
            "epoch 3 - batch 1040 - loss 150.16162109375\n",
            "epoch 3 - batch 1050 - loss 170.25537109375\n",
            "epoch 3 - batch 1060 - loss 152.938720703125\n",
            "epoch 3 - batch 1070 - loss 154.22146606445312\n",
            "epoch 3 - batch 1080 - loss 164.3165740966797\n",
            "epoch 3 - batch 1090 - loss 164.86439514160156\n",
            "epoch 3 - batch 1100 - loss 146.2262725830078\n",
            "epoch 3 - batch 1110 - loss 170.9624786376953\n",
            "epoch 3 - batch 1120 - loss 164.56849670410156\n",
            "epoch 3 - batch 1130 - loss 153.1637725830078\n",
            "epoch 3 - batch 1140 - loss 157.2008819580078\n",
            "epoch 3 - batch 1150 - loss 142.68592834472656\n",
            "epoch 3 - batch 1160 - loss 175.2863006591797\n",
            "epoch 3 - batch 1170 - loss 161.08033752441406\n",
            "epoch 3 - batch 1180 - loss 168.40248107910156\n",
            "epoch 3 - batch 1190 - loss 145.84251403808594\n",
            "epoch 3 - batch 1200 - loss 169.24044799804688\n",
            "epoch 3 - batch 1210 - loss 157.91127014160156\n",
            "epoch 3 - batch 1220 - loss 180.30023193359375\n",
            "epoch 3 - batch 1230 - loss 157.7979278564453\n",
            "epoch 3 training time: 678.9582357406616 sec\n",
            "evaluation of batch 0 took: 0.05231165885925293\n",
            "evaluation of batch 50 took: 0.12604236602783203\n",
            "evaluation of batch 100 took: 0.05417919158935547\n",
            "evaluation of batch 150 took: 0.1183936595916748\n",
            "evaluation of batch 200 took: 0.050139427185058594\n",
            "evaluation of batch 250 took: 0.05003094673156738\n",
            "evaluation of batch 300 took: 0.12106823921203613\n",
            "evaluation of batch 350 took: 0.07320070266723633\n",
            "evaluation of batch 400 took: 0.11636805534362793\n",
            "evaluation of batch 450 took: 0.12747788429260254\n",
            "evaluation of batch 500 took: 0.1340482234954834\n",
            "evaluation of batch 550 took: 0.05952024459838867\n",
            "evaluation of batch 600 took: 0.14571261405944824\n",
            "evaluation of batch 650 took: 0.06008195877075195\n",
            "evaluation of batch 700 took: 0.13155484199523926\n",
            "evaluation of batch 750 took: 0.1306302547454834\n",
            "evaluation of batch 800 took: 0.05087876319885254\n",
            "evaluation of batch 850 took: 0.07070374488830566\n",
            "evaluation of batch 900 took: 0.07423210144042969\n",
            "evaluation of batch 950 took: 0.08539891242980957\n",
            "evaluation of batch 1000 took: 0.05221819877624512\n",
            "evaluation of batch 1050 took: 0.05174589157104492\n",
            "evaluation of batch 1100 took: 0.05173158645629883\n",
            "evaluation of batch 1150 took: 0.12096166610717773\n",
            "evaluation of batch 1200 took: 0.05258440971374512\n",
            "epoch 3 evaluation on training data time: 107.59349346160889 sec\n",
            "evaluation of batch 0 took: 0.06194257736206055\n",
            "evaluation of batch 50 took: 0.050994873046875\n",
            "evaluation of batch 100 took: 0.05138564109802246\n",
            "evaluation of batch 150 took: 0.05192232131958008\n",
            "epoch 3 evaluation on test data time: 37.13463020324707 sec\n",
            "epoch evaluation:  {'epoch': 3, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=164.89409>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "epoch 3 saved checkpoint: models/checkpoints/baseline/ckpt-2\n",
            "Increasing epochs without improvement to: 3\n",
            "epoch 4 - batch 10 - loss 158.41249084472656\n",
            "epoch 4 - batch 20 - loss 170.9067840576172\n",
            "epoch 4 - batch 30 - loss 163.90684509277344\n",
            "epoch 4 - batch 40 - loss 163.22637939453125\n",
            "epoch 4 - batch 50 - loss 160.73023986816406\n",
            "epoch 4 - batch 60 - loss 165.75283813476562\n",
            "epoch 4 - batch 70 - loss 149.71786499023438\n",
            "epoch 4 - batch 80 - loss 162.4475860595703\n",
            "epoch 4 - batch 90 - loss 158.62974548339844\n",
            "epoch 4 - batch 100 - loss 161.752197265625\n",
            "epoch 4 - batch 110 - loss 158.01820373535156\n",
            "epoch 4 - batch 120 - loss 168.76231384277344\n",
            "epoch 4 - batch 130 - loss 176.8240966796875\n",
            "epoch 4 - batch 140 - loss 145.26287841796875\n",
            "epoch 4 - batch 150 - loss 145.2915802001953\n",
            "epoch 4 - batch 160 - loss 166.8972930908203\n",
            "epoch 4 - batch 170 - loss 168.4076385498047\n",
            "epoch 4 - batch 180 - loss 154.84861755371094\n",
            "epoch 4 - batch 190 - loss 162.4142303466797\n",
            "epoch 4 - batch 200 - loss 161.80650329589844\n",
            "epoch 4 - batch 210 - loss 155.4542236328125\n",
            "epoch 4 - batch 220 - loss 165.80345153808594\n",
            "epoch 4 - batch 230 - loss 160.88758850097656\n",
            "epoch 4 - batch 240 - loss 153.6269989013672\n",
            "epoch 4 - batch 250 - loss 155.59478759765625\n",
            "epoch 4 - batch 260 - loss 152.585693359375\n",
            "epoch 4 - batch 270 - loss 170.1475372314453\n",
            "epoch 4 - batch 280 - loss 163.98524475097656\n",
            "epoch 4 - batch 290 - loss 176.51824951171875\n",
            "epoch 4 - batch 300 - loss 144.1905059814453\n",
            "epoch 4 - batch 310 - loss 157.63490295410156\n",
            "epoch 4 - batch 320 - loss 152.44935607910156\n",
            "epoch 4 - batch 330 - loss 141.53485107421875\n",
            "epoch 4 - batch 340 - loss 142.4352264404297\n",
            "epoch 4 - batch 350 - loss 154.2045440673828\n",
            "epoch 4 - batch 360 - loss 164.56265258789062\n",
            "epoch 4 - batch 370 - loss 149.22142028808594\n",
            "epoch 4 - batch 380 - loss 161.48439025878906\n",
            "epoch 4 - batch 390 - loss 135.9127960205078\n",
            "epoch 4 - batch 400 - loss 165.5034637451172\n",
            "epoch 4 - batch 410 - loss 157.7200469970703\n",
            "epoch 4 - batch 420 - loss 153.01766967773438\n",
            "epoch 4 - batch 430 - loss 149.07456970214844\n",
            "epoch 4 - batch 440 - loss 157.6985321044922\n",
            "epoch 4 - batch 450 - loss 177.3603973388672\n",
            "epoch 4 - batch 460 - loss 143.87501525878906\n",
            "epoch 4 - batch 470 - loss 144.4287109375\n",
            "epoch 4 - batch 480 - loss 156.78929138183594\n",
            "epoch 4 - batch 490 - loss 165.75038146972656\n",
            "epoch 4 - batch 500 - loss 155.630126953125\n",
            "epoch 4 - batch 510 - loss 127.08440399169922\n",
            "epoch 4 - batch 520 - loss 150.64877319335938\n",
            "epoch 4 - batch 530 - loss 161.50543212890625\n",
            "epoch 4 - batch 540 - loss 178.7100372314453\n",
            "epoch 4 - batch 550 - loss 168.38352966308594\n",
            "epoch 4 - batch 560 - loss 136.95497131347656\n",
            "epoch 4 - batch 570 - loss 139.7418975830078\n",
            "epoch 4 - batch 580 - loss 160.46531677246094\n",
            "epoch 4 - batch 590 - loss 159.247802734375\n",
            "epoch 4 - batch 600 - loss 154.01315307617188\n",
            "epoch 4 - batch 610 - loss 136.84539794921875\n",
            "epoch 4 - batch 620 - loss 137.7340850830078\n",
            "epoch 4 - batch 630 - loss 167.08433532714844\n",
            "epoch 4 - batch 640 - loss 141.28213500976562\n",
            "epoch 4 - batch 650 - loss 148.68479919433594\n",
            "epoch 4 - batch 660 - loss 148.9557647705078\n",
            "epoch 4 - batch 670 - loss 140.98939514160156\n",
            "epoch 4 - batch 680 - loss 149.13755798339844\n",
            "epoch 4 - batch 690 - loss 160.9611053466797\n",
            "epoch 4 - batch 700 - loss 157.93722534179688\n",
            "epoch 4 - batch 710 - loss 139.5035858154297\n",
            "epoch 4 - batch 720 - loss 162.15542602539062\n",
            "epoch 4 - batch 730 - loss 159.43922424316406\n",
            "epoch 4 - batch 740 - loss 165.86741638183594\n",
            "epoch 4 - batch 750 - loss 142.41127014160156\n",
            "epoch 4 - batch 760 - loss 137.84506225585938\n",
            "epoch 4 - batch 770 - loss 158.1277313232422\n",
            "epoch 4 - batch 780 - loss 148.7771453857422\n",
            "epoch 4 - batch 790 - loss 158.13233947753906\n",
            "epoch 4 - batch 800 - loss 159.9657440185547\n",
            "epoch 4 - batch 810 - loss 146.1497344970703\n",
            "epoch 4 - batch 820 - loss 143.6816864013672\n",
            "epoch 4 - batch 830 - loss 141.86541748046875\n",
            "epoch 4 - batch 840 - loss 154.6282501220703\n",
            "epoch 4 - batch 850 - loss 141.265380859375\n",
            "epoch 4 - batch 860 - loss 146.4109344482422\n",
            "epoch 4 - batch 870 - loss 147.63760375976562\n",
            "epoch 4 - batch 880 - loss 146.85133361816406\n",
            "epoch 4 - batch 890 - loss 145.3329315185547\n",
            "epoch 4 - batch 900 - loss 127.48944854736328\n",
            "epoch 4 - batch 910 - loss 126.64281463623047\n",
            "epoch 4 - batch 920 - loss 138.41026306152344\n",
            "epoch 4 - batch 930 - loss 137.81761169433594\n",
            "epoch 4 - batch 940 - loss 130.78982543945312\n",
            "epoch 4 - batch 950 - loss 159.0273895263672\n",
            "epoch 4 - batch 960 - loss 149.81460571289062\n",
            "epoch 4 - batch 970 - loss 124.95864868164062\n",
            "epoch 4 - batch 980 - loss 140.19439697265625\n",
            "epoch 4 - batch 990 - loss 154.4801483154297\n",
            "epoch 4 - batch 1000 - loss 145.28550720214844\n",
            "epoch 4 - batch 1010 - loss 155.85853576660156\n",
            "epoch 4 - batch 1020 - loss 145.67410278320312\n",
            "epoch 4 - batch 1030 - loss 145.2866668701172\n",
            "epoch 4 - batch 1040 - loss 134.88047790527344\n",
            "epoch 4 - batch 1050 - loss 166.62355041503906\n",
            "epoch 4 - batch 1060 - loss 143.44764709472656\n",
            "epoch 4 - batch 1070 - loss 142.14039611816406\n",
            "epoch 4 - batch 1080 - loss 158.576904296875\n",
            "epoch 4 - batch 1090 - loss 138.4927215576172\n",
            "epoch 4 - batch 1100 - loss 135.66029357910156\n",
            "epoch 4 - batch 1110 - loss 142.96372985839844\n",
            "epoch 4 - batch 1120 - loss 154.126953125\n",
            "epoch 4 - batch 1130 - loss 139.24884033203125\n",
            "epoch 4 - batch 1140 - loss 146.0874481201172\n",
            "epoch 4 - batch 1150 - loss 133.09295654296875\n",
            "epoch 4 - batch 1160 - loss 140.92347717285156\n",
            "epoch 4 - batch 1170 - loss 138.63414001464844\n",
            "epoch 4 - batch 1180 - loss 113.66724395751953\n",
            "epoch 4 - batch 1190 - loss 142.2069091796875\n",
            "epoch 4 - batch 1200 - loss 146.4167938232422\n",
            "epoch 4 - batch 1210 - loss 127.7672348022461\n",
            "epoch 4 - batch 1220 - loss 135.38418579101562\n",
            "epoch 4 - batch 1230 - loss 147.88055419921875\n",
            "epoch 4 training time: 676.5944082736969 sec\n",
            "evaluation of batch 0 took: 0.0516507625579834\n",
            "evaluation of batch 50 took: 0.06255888938903809\n",
            "evaluation of batch 100 took: 0.05037426948547363\n",
            "evaluation of batch 150 took: 0.07190680503845215\n",
            "evaluation of batch 200 took: 0.1330552101135254\n",
            "evaluation of batch 250 took: 0.14654254913330078\n",
            "evaluation of batch 300 took: 0.1179661750793457\n",
            "evaluation of batch 350 took: 0.05404305458068848\n",
            "evaluation of batch 400 took: 0.12193989753723145\n",
            "evaluation of batch 450 took: 0.0508723258972168\n",
            "evaluation of batch 500 took: 0.12314152717590332\n",
            "evaluation of batch 550 took: 0.0542912483215332\n",
            "evaluation of batch 600 took: 0.07269072532653809\n",
            "evaluation of batch 650 took: 0.05046439170837402\n",
            "evaluation of batch 700 took: 0.0710597038269043\n",
            "evaluation of batch 750 took: 0.12798404693603516\n",
            "evaluation of batch 800 took: 0.0707864761352539\n",
            "evaluation of batch 850 took: 0.054247140884399414\n",
            "evaluation of batch 900 took: 0.04929232597351074\n",
            "evaluation of batch 950 took: 0.1253964900970459\n",
            "evaluation of batch 1000 took: 0.07407855987548828\n",
            "evaluation of batch 1050 took: 0.11758232116699219\n",
            "evaluation of batch 1100 took: 0.12006425857543945\n",
            "evaluation of batch 1150 took: 0.05193924903869629\n",
            "evaluation of batch 1200 took: 0.051175832748413086\n",
            "epoch 4 evaluation on training data time: 103.59316873550415 sec\n",
            "evaluation of batch 0 took: 0.06052398681640625\n",
            "evaluation of batch 50 took: 0.05760908126831055\n",
            "evaluation of batch 100 took: 0.050676584243774414\n",
            "evaluation of batch 150 took: 0.050174713134765625\n",
            "epoch 4 evaluation on test data time: 37.3969247341156 sec\n",
            "epoch evaluation:  {'epoch': 4, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=133.4707>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "Increasing epochs without improvement to: 4\n",
            "epoch 5 - batch 10 - loss 120.02813720703125\n",
            "epoch 5 - batch 20 - loss 122.4570083618164\n",
            "epoch 5 - batch 30 - loss 135.95094299316406\n",
            "epoch 5 - batch 40 - loss 136.2355194091797\n",
            "epoch 5 - batch 50 - loss 129.39794921875\n",
            "epoch 5 - batch 60 - loss 145.81813049316406\n",
            "epoch 5 - batch 70 - loss 126.9718246459961\n",
            "epoch 5 - batch 80 - loss 137.83563232421875\n",
            "epoch 5 - batch 90 - loss 132.80104064941406\n",
            "epoch 5 - batch 100 - loss 135.69277954101562\n",
            "epoch 5 - batch 110 - loss 132.6580047607422\n",
            "epoch 5 - batch 120 - loss 124.78378295898438\n",
            "epoch 5 - batch 130 - loss 145.01651000976562\n",
            "epoch 5 - batch 140 - loss 124.57369995117188\n",
            "epoch 5 - batch 150 - loss 117.02286529541016\n",
            "epoch 5 - batch 160 - loss 141.57872009277344\n",
            "epoch 5 - batch 170 - loss 136.1699981689453\n",
            "epoch 5 - batch 180 - loss 133.31700134277344\n",
            "epoch 5 - batch 190 - loss 120.0279541015625\n",
            "epoch 5 - batch 200 - loss 127.30313873291016\n",
            "epoch 5 - batch 210 - loss 128.2440185546875\n",
            "epoch 5 - batch 220 - loss 128.74473571777344\n",
            "epoch 5 - batch 230 - loss 143.9434051513672\n",
            "epoch 5 - batch 240 - loss 132.16310119628906\n",
            "epoch 5 - batch 250 - loss 141.34222412109375\n",
            "epoch 5 - batch 260 - loss 128.6217803955078\n",
            "epoch 5 - batch 270 - loss 143.5260467529297\n",
            "epoch 5 - batch 280 - loss 127.70667266845703\n",
            "epoch 5 - batch 290 - loss 114.93486785888672\n",
            "epoch 5 - batch 300 - loss 127.68197631835938\n",
            "epoch 5 - batch 310 - loss 131.25819396972656\n",
            "epoch 5 - batch 320 - loss 125.78219604492188\n",
            "epoch 5 - batch 330 - loss 108.7099380493164\n",
            "epoch 5 - batch 340 - loss 131.18359375\n",
            "epoch 5 - batch 350 - loss 113.52857208251953\n",
            "epoch 5 - batch 360 - loss 124.68142700195312\n",
            "epoch 5 - batch 370 - loss 149.08555603027344\n",
            "epoch 5 - batch 380 - loss 131.3860626220703\n",
            "epoch 5 - batch 390 - loss 125.0807876586914\n",
            "epoch 5 - batch 400 - loss 113.52899932861328\n",
            "epoch 5 - batch 410 - loss 126.46429443359375\n",
            "epoch 5 - batch 420 - loss 125.12928009033203\n",
            "epoch 5 - batch 430 - loss 109.8125228881836\n",
            "epoch 5 - batch 440 - loss 128.2784881591797\n",
            "epoch 5 - batch 450 - loss 119.40430450439453\n",
            "epoch 5 - batch 460 - loss 136.8697967529297\n",
            "epoch 5 - batch 470 - loss 119.2252197265625\n",
            "epoch 5 - batch 480 - loss 120.3834228515625\n",
            "epoch 5 - batch 490 - loss 134.8145294189453\n",
            "epoch 5 - batch 500 - loss 136.32485961914062\n",
            "epoch 5 - batch 510 - loss 111.61166381835938\n",
            "epoch 5 - batch 520 - loss 122.8263931274414\n",
            "epoch 5 - batch 530 - loss 134.2857666015625\n",
            "epoch 5 - batch 540 - loss 140.63902282714844\n",
            "epoch 5 - batch 550 - loss 130.91551208496094\n",
            "epoch 5 - batch 560 - loss 112.59070587158203\n",
            "epoch 5 - batch 570 - loss 108.79632568359375\n",
            "epoch 5 - batch 580 - loss 125.91007232666016\n",
            "epoch 5 - batch 590 - loss 114.66927337646484\n",
            "epoch 5 - batch 600 - loss 130.50706481933594\n",
            "epoch 5 - batch 610 - loss 134.1996307373047\n",
            "epoch 5 - batch 620 - loss 147.0806121826172\n",
            "epoch 5 - batch 630 - loss 127.63445281982422\n",
            "epoch 5 - batch 640 - loss 127.52288055419922\n",
            "epoch 5 - batch 650 - loss 112.1366958618164\n",
            "epoch 5 - batch 660 - loss 111.76212310791016\n",
            "epoch 5 - batch 670 - loss 123.13427734375\n",
            "epoch 5 - batch 680 - loss 105.29987335205078\n",
            "epoch 5 - batch 690 - loss 130.6741943359375\n",
            "epoch 5 - batch 700 - loss 111.98779296875\n",
            "epoch 5 - batch 710 - loss 119.1784896850586\n",
            "epoch 5 - batch 720 - loss 126.72785186767578\n",
            "epoch 5 - batch 730 - loss 108.1924057006836\n",
            "epoch 5 - batch 740 - loss 119.3174057006836\n",
            "epoch 5 - batch 750 - loss 119.47552490234375\n",
            "epoch 5 - batch 760 - loss 121.92113494873047\n",
            "epoch 5 - batch 770 - loss 135.2751922607422\n",
            "epoch 5 - batch 780 - loss 113.71593475341797\n",
            "epoch 5 - batch 790 - loss 115.00687408447266\n",
            "epoch 5 - batch 800 - loss 113.63388061523438\n",
            "epoch 5 - batch 810 - loss 120.29644775390625\n",
            "epoch 5 - batch 820 - loss 109.40936279296875\n",
            "epoch 5 - batch 830 - loss 107.75386810302734\n",
            "epoch 5 - batch 840 - loss 108.62002563476562\n",
            "epoch 5 - batch 850 - loss 104.53783416748047\n",
            "epoch 5 - batch 860 - loss 107.885009765625\n",
            "epoch 5 - batch 870 - loss 92.7745132446289\n",
            "epoch 5 - batch 880 - loss 118.11688995361328\n",
            "epoch 5 - batch 890 - loss 116.15484619140625\n",
            "epoch 5 - batch 900 - loss 115.5552978515625\n",
            "epoch 5 - batch 910 - loss 96.54025268554688\n",
            "epoch 5 - batch 920 - loss 124.3371353149414\n",
            "epoch 5 - batch 930 - loss 103.77303314208984\n",
            "epoch 5 - batch 940 - loss 141.23306274414062\n",
            "epoch 5 - batch 950 - loss 102.9775161743164\n",
            "epoch 5 - batch 960 - loss 115.89203643798828\n",
            "epoch 5 - batch 970 - loss 99.25228118896484\n",
            "epoch 5 - batch 980 - loss 102.7919692993164\n",
            "epoch 5 - batch 990 - loss 100.03978729248047\n",
            "epoch 5 - batch 1000 - loss 87.80972290039062\n",
            "epoch 5 - batch 1010 - loss 110.02517700195312\n",
            "epoch 5 - batch 1020 - loss 104.5379638671875\n",
            "epoch 5 - batch 1030 - loss 114.13455200195312\n",
            "epoch 5 - batch 1040 - loss 106.83639526367188\n",
            "epoch 5 - batch 1050 - loss 115.02581787109375\n",
            "epoch 5 - batch 1060 - loss 102.80530548095703\n",
            "epoch 5 - batch 1070 - loss 116.18599700927734\n",
            "epoch 5 - batch 1080 - loss 117.52494049072266\n",
            "epoch 5 - batch 1090 - loss 115.54911041259766\n",
            "epoch 5 - batch 1100 - loss 98.72930908203125\n",
            "epoch 5 - batch 1110 - loss 112.26557159423828\n",
            "epoch 5 - batch 1120 - loss 124.60369110107422\n",
            "epoch 5 - batch 1130 - loss 105.19161224365234\n",
            "epoch 5 - batch 1140 - loss 120.58688354492188\n",
            "epoch 5 - batch 1150 - loss 112.2862548828125\n",
            "epoch 5 - batch 1160 - loss 111.761474609375\n",
            "epoch 5 - batch 1170 - loss 109.48162841796875\n",
            "epoch 5 - batch 1180 - loss 93.49935150146484\n",
            "epoch 5 - batch 1190 - loss 106.80506134033203\n",
            "epoch 5 - batch 1200 - loss 93.11751556396484\n",
            "epoch 5 - batch 1210 - loss 110.57071685791016\n",
            "epoch 5 - batch 1220 - loss 106.716796875\n",
            "epoch 5 - batch 1230 - loss 117.30159759521484\n",
            "epoch 5 training time: 678.4774265289307 sec\n",
            "evaluation of batch 0 took: 0.05190730094909668\n",
            "evaluation of batch 50 took: 0.05318117141723633\n",
            "evaluation of batch 100 took: 0.05510520935058594\n",
            "evaluation of batch 150 took: 0.12205076217651367\n",
            "evaluation of batch 200 took: 0.052094221115112305\n",
            "evaluation of batch 250 took: 0.07278919219970703\n",
            "evaluation of batch 300 took: 0.05193018913269043\n",
            "evaluation of batch 350 took: 0.05265069007873535\n",
            "evaluation of batch 400 took: 0.14069104194641113\n",
            "evaluation of batch 450 took: 0.05201148986816406\n",
            "evaluation of batch 500 took: 0.054480552673339844\n",
            "evaluation of batch 550 took: 0.07783865928649902\n",
            "evaluation of batch 600 took: 0.127732515335083\n",
            "evaluation of batch 650 took: 0.12551522254943848\n",
            "evaluation of batch 700 took: 0.05904865264892578\n",
            "evaluation of batch 750 took: 0.05306720733642578\n",
            "evaluation of batch 800 took: 0.05153512954711914\n",
            "evaluation of batch 850 took: 0.11799097061157227\n",
            "evaluation of batch 900 took: 0.13151073455810547\n",
            "evaluation of batch 950 took: 0.07276749610900879\n",
            "evaluation of batch 1000 took: 0.12069368362426758\n",
            "evaluation of batch 1050 took: 0.07879495620727539\n",
            "evaluation of batch 1100 took: 0.05211353302001953\n",
            "evaluation of batch 1150 took: 0.13590049743652344\n",
            "evaluation of batch 1200 took: 0.05294036865234375\n",
            "epoch 5 evaluation on training data time: 95.65393352508545 sec\n",
            "evaluation of batch 0 took: 0.13512539863586426\n",
            "evaluation of batch 50 took: 0.13251757621765137\n",
            "evaluation of batch 100 took: 0.11845541000366211\n",
            "evaluation of batch 150 took: 0.11872625350952148\n",
            "epoch 5 evaluation on test data time: 48.181047677993774 sec\n",
            "epoch evaluation:  {'epoch': 5, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=113.61951>, 'test_rouge_1_p': 0.0012238737824675312, 'test_rouge_1_r': 0.003557689055735928, 'test_rouge_1_f1': 0.0016846988773987557, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0012238737824675312, 'test_rouge_L_r': 0.003557689055735928, 'test_rouge_L_f1': 0.0016846988773987557}\n",
            "epoch 6 - batch 10 - loss 99.4021224975586\n",
            "epoch 6 - batch 20 - loss 121.77880859375\n",
            "epoch 6 - batch 30 - loss 104.09070587158203\n",
            "epoch 6 - batch 40 - loss 109.86949920654297\n",
            "epoch 6 - batch 50 - loss 104.87335968017578\n",
            "epoch 6 - batch 60 - loss 99.37874603271484\n",
            "epoch 6 - batch 70 - loss 99.64386749267578\n",
            "epoch 6 - batch 80 - loss 105.008056640625\n",
            "epoch 6 - batch 90 - loss 88.9111557006836\n",
            "epoch 6 - batch 100 - loss 121.27266693115234\n",
            "epoch 6 - batch 110 - loss 116.51763916015625\n",
            "epoch 6 - batch 120 - loss 92.1880874633789\n",
            "epoch 6 - batch 130 - loss 122.09488677978516\n",
            "epoch 6 - batch 140 - loss 115.23821258544922\n",
            "epoch 6 - batch 150 - loss 126.0538330078125\n",
            "epoch 6 - batch 160 - loss 98.94781494140625\n",
            "epoch 6 - batch 170 - loss 105.084716796875\n",
            "epoch 6 - batch 180 - loss 120.50311279296875\n",
            "epoch 6 - batch 190 - loss 120.2724609375\n",
            "epoch 6 - batch 200 - loss 118.14173126220703\n",
            "epoch 6 - batch 210 - loss 88.92060089111328\n",
            "epoch 6 - batch 220 - loss 101.10309600830078\n",
            "epoch 6 - batch 230 - loss 99.62398529052734\n",
            "epoch 6 - batch 240 - loss 107.51805877685547\n",
            "epoch 6 - batch 250 - loss 114.27066040039062\n",
            "epoch 6 - batch 260 - loss 111.99776458740234\n",
            "epoch 6 - batch 270 - loss 110.501220703125\n",
            "epoch 6 - batch 280 - loss 110.44200897216797\n",
            "epoch 6 - batch 290 - loss 90.06053924560547\n",
            "epoch 6 - batch 300 - loss 95.64875030517578\n",
            "epoch 6 - batch 310 - loss 82.21501159667969\n",
            "epoch 6 - batch 320 - loss 98.21749877929688\n",
            "epoch 6 - batch 330 - loss 97.02655029296875\n",
            "epoch 6 - batch 340 - loss 101.66207122802734\n",
            "epoch 6 - batch 350 - loss 102.20830535888672\n",
            "epoch 6 - batch 360 - loss 101.9014892578125\n",
            "epoch 6 - batch 370 - loss 84.1434097290039\n",
            "epoch 6 - batch 380 - loss 90.8059310913086\n",
            "epoch 6 - batch 390 - loss 96.58402252197266\n",
            "epoch 6 - batch 400 - loss 83.97843933105469\n",
            "epoch 6 - batch 410 - loss 98.5927963256836\n",
            "epoch 6 - batch 420 - loss 98.3727035522461\n",
            "epoch 6 - batch 430 - loss 98.28085327148438\n",
            "epoch 6 - batch 440 - loss 99.11595916748047\n",
            "epoch 6 - batch 450 - loss 87.0875015258789\n",
            "epoch 6 - batch 460 - loss 80.62144470214844\n",
            "epoch 6 - batch 470 - loss 86.21575927734375\n",
            "epoch 6 - batch 480 - loss 95.58065032958984\n",
            "epoch 6 - batch 490 - loss 97.68379974365234\n",
            "epoch 6 - batch 500 - loss 74.09099578857422\n",
            "epoch 6 - batch 510 - loss 76.13838958740234\n",
            "epoch 6 - batch 520 - loss 95.7855453491211\n",
            "epoch 6 - batch 530 - loss 92.1907958984375\n",
            "epoch 6 - batch 540 - loss 80.51831817626953\n",
            "epoch 6 - batch 550 - loss 89.3625717163086\n",
            "epoch 6 - batch 560 - loss 81.50785064697266\n",
            "epoch 6 - batch 570 - loss 93.5130615234375\n",
            "epoch 6 - batch 580 - loss 80.70008850097656\n",
            "epoch 6 - batch 590 - loss 80.81208038330078\n",
            "epoch 6 - batch 600 - loss 95.51522827148438\n",
            "epoch 6 - batch 610 - loss 88.45718383789062\n",
            "epoch 6 - batch 620 - loss 82.02842712402344\n",
            "epoch 6 - batch 630 - loss 92.13526153564453\n",
            "epoch 6 - batch 640 - loss 84.21182250976562\n",
            "epoch 6 - batch 650 - loss 78.47634887695312\n",
            "epoch 6 - batch 660 - loss 87.63021850585938\n",
            "epoch 6 - batch 670 - loss 84.7134780883789\n",
            "epoch 6 - batch 680 - loss 72.13188934326172\n",
            "epoch 6 - batch 690 - loss 93.55886840820312\n",
            "epoch 6 - batch 700 - loss 81.89871978759766\n",
            "epoch 6 - batch 710 - loss 92.54168701171875\n",
            "epoch 6 - batch 720 - loss 95.5826416015625\n",
            "epoch 6 - batch 730 - loss 84.52107238769531\n",
            "epoch 6 - batch 740 - loss 76.32257843017578\n",
            "epoch 6 - batch 750 - loss 81.50177001953125\n",
            "epoch 6 - batch 760 - loss 76.3228759765625\n",
            "epoch 6 - batch 770 - loss 84.89910125732422\n",
            "epoch 6 - batch 780 - loss 81.59617614746094\n",
            "epoch 6 - batch 790 - loss 68.28398895263672\n",
            "epoch 6 - batch 800 - loss 81.62276458740234\n",
            "epoch 6 - batch 810 - loss 75.90510559082031\n",
            "epoch 6 - batch 820 - loss 72.45853424072266\n",
            "epoch 6 - batch 830 - loss 74.07492065429688\n",
            "epoch 6 - batch 840 - loss 76.83776092529297\n",
            "epoch 6 - batch 850 - loss 89.80028533935547\n",
            "epoch 6 - batch 860 - loss 92.93848419189453\n",
            "epoch 6 - batch 870 - loss 91.00212860107422\n",
            "epoch 6 - batch 880 - loss 76.51940155029297\n",
            "epoch 6 - batch 890 - loss 85.16641235351562\n",
            "epoch 6 - batch 900 - loss 82.72464752197266\n",
            "epoch 6 - batch 910 - loss 77.43858337402344\n",
            "epoch 6 - batch 920 - loss 61.21662902832031\n",
            "epoch 6 - batch 930 - loss 78.53791046142578\n",
            "epoch 6 - batch 940 - loss 79.9246597290039\n",
            "epoch 6 - batch 950 - loss 72.78009796142578\n",
            "epoch 6 - batch 960 - loss 91.53473663330078\n",
            "epoch 6 - batch 970 - loss 91.9546890258789\n",
            "epoch 6 - batch 980 - loss 86.73995208740234\n",
            "epoch 6 - batch 990 - loss 73.08736419677734\n",
            "epoch 6 - batch 1000 - loss 83.41310119628906\n",
            "epoch 6 - batch 1010 - loss 93.04986572265625\n",
            "epoch 6 - batch 1020 - loss 80.9148941040039\n",
            "epoch 6 - batch 1030 - loss 90.18582916259766\n",
            "epoch 6 - batch 1040 - loss 85.47664642333984\n",
            "epoch 6 - batch 1050 - loss 96.10343170166016\n",
            "epoch 6 - batch 1060 - loss 88.90631103515625\n",
            "epoch 6 - batch 1070 - loss 83.4949722290039\n",
            "epoch 6 - batch 1080 - loss 72.77163696289062\n",
            "epoch 6 - batch 1090 - loss 65.66513061523438\n",
            "epoch 6 - batch 1100 - loss 83.15933227539062\n",
            "epoch 6 - batch 1110 - loss 73.60050201416016\n",
            "epoch 6 - batch 1120 - loss 86.2146987915039\n",
            "epoch 6 - batch 1130 - loss 80.77632904052734\n",
            "epoch 6 - batch 1140 - loss 84.24132537841797\n",
            "epoch 6 - batch 1150 - loss 84.72862243652344\n",
            "epoch 6 - batch 1160 - loss 94.353271484375\n",
            "epoch 6 - batch 1170 - loss 66.6955337524414\n",
            "epoch 6 - batch 1180 - loss 90.59439086914062\n",
            "epoch 6 - batch 1190 - loss 73.36748504638672\n",
            "epoch 6 - batch 1200 - loss 72.5781478881836\n",
            "epoch 6 - batch 1210 - loss 70.64254760742188\n",
            "epoch 6 - batch 1220 - loss 78.02474212646484\n",
            "epoch 6 - batch 1230 - loss 72.0211410522461\n",
            "epoch 6 training time: 680.7312815189362 sec\n",
            "evaluation of batch 0 took: 0.11868977546691895\n",
            "evaluation of batch 50 took: 0.07198190689086914\n",
            "evaluation of batch 100 took: 0.117462158203125\n",
            "evaluation of batch 150 took: 0.11553597450256348\n",
            "evaluation of batch 200 took: 0.11669445037841797\n",
            "evaluation of batch 250 took: 0.11634945869445801\n",
            "evaluation of batch 300 took: 0.05084228515625\n",
            "evaluation of batch 350 took: 0.05038022994995117\n",
            "evaluation of batch 400 took: 0.050276994705200195\n",
            "evaluation of batch 450 took: 0.1321876049041748\n",
            "evaluation of batch 500 took: 0.05121159553527832\n",
            "evaluation of batch 550 took: 0.04955863952636719\n",
            "evaluation of batch 600 took: 0.13462495803833008\n",
            "evaluation of batch 650 took: 0.13129091262817383\n",
            "evaluation of batch 700 took: 0.049536705017089844\n",
            "evaluation of batch 750 took: 0.050519466400146484\n",
            "evaluation of batch 800 took: 0.052260637283325195\n",
            "evaluation of batch 850 took: 0.07343196868896484\n",
            "evaluation of batch 900 took: 0.05114626884460449\n",
            "evaluation of batch 950 took: 0.11898684501647949\n",
            "evaluation of batch 1000 took: 0.049756765365600586\n",
            "evaluation of batch 1050 took: 0.11683368682861328\n",
            "evaluation of batch 1100 took: 0.13912391662597656\n",
            "evaluation of batch 1150 took: 0.12223601341247559\n",
            "evaluation of batch 1200 took: 0.05008363723754883\n",
            "epoch 6 evaluation on training data time: 86.68894982337952 sec\n",
            "evaluation of batch 0 took: 0.059951066970825195\n",
            "evaluation of batch 50 took: 0.050675153732299805\n",
            "evaluation of batch 100 took: 0.049442291259765625\n",
            "evaluation of batch 150 took: 0.04972529411315918\n",
            "epoch 6 evaluation on test data time: 37.19992756843567 sec\n",
            "epoch evaluation:  {'epoch': 6, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=77.83353>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "epoch 6 saved checkpoint: models/checkpoints/baseline/ckpt-3\n",
            "Increasing epochs without improvement to: 1\n",
            "epoch 7 - batch 10 - loss 80.52967071533203\n",
            "epoch 7 - batch 20 - loss 89.44100952148438\n",
            "epoch 7 - batch 30 - loss 61.1408576965332\n",
            "epoch 7 - batch 40 - loss 60.90281295776367\n",
            "epoch 7 - batch 50 - loss 68.2107162475586\n",
            "epoch 7 - batch 60 - loss 72.32772064208984\n",
            "epoch 7 - batch 70 - loss 72.27562713623047\n",
            "epoch 7 - batch 80 - loss 70.46923828125\n",
            "epoch 7 - batch 90 - loss 81.90145111083984\n",
            "epoch 7 - batch 100 - loss 73.36603546142578\n",
            "epoch 7 - batch 110 - loss 71.62154388427734\n",
            "epoch 7 - batch 120 - loss 77.15335845947266\n",
            "epoch 7 - batch 130 - loss 73.19679260253906\n",
            "epoch 7 - batch 140 - loss 63.59306716918945\n",
            "epoch 7 - batch 150 - loss 71.5102767944336\n",
            "epoch 7 - batch 160 - loss 53.063724517822266\n",
            "epoch 7 - batch 170 - loss 82.73999786376953\n",
            "epoch 7 - batch 180 - loss 69.06075286865234\n",
            "epoch 7 - batch 190 - loss 69.0778579711914\n",
            "epoch 7 - batch 200 - loss 66.11028289794922\n",
            "epoch 7 - batch 210 - loss 73.15706634521484\n",
            "epoch 7 - batch 220 - loss 61.77884292602539\n",
            "epoch 7 - batch 230 - loss 67.77298736572266\n",
            "epoch 7 - batch 240 - loss 68.65543365478516\n",
            "epoch 7 - batch 250 - loss 70.19458770751953\n",
            "epoch 7 - batch 260 - loss 63.60896301269531\n",
            "epoch 7 - batch 270 - loss 58.19602584838867\n",
            "epoch 7 - batch 280 - loss 64.53733825683594\n",
            "epoch 7 - batch 290 - loss 57.435821533203125\n",
            "epoch 7 - batch 300 - loss 60.767669677734375\n",
            "epoch 7 - batch 310 - loss 65.8033447265625\n",
            "epoch 7 - batch 320 - loss 61.82477951049805\n",
            "epoch 7 - batch 330 - loss 56.34516525268555\n",
            "epoch 7 - batch 340 - loss 66.2049789428711\n",
            "epoch 7 - batch 350 - loss 64.48509979248047\n",
            "epoch 7 - batch 360 - loss 71.8058090209961\n",
            "epoch 7 - batch 370 - loss 68.25981903076172\n",
            "epoch 7 - batch 380 - loss 71.04717254638672\n",
            "epoch 7 - batch 390 - loss 64.41671752929688\n",
            "epoch 7 - batch 400 - loss 69.05358123779297\n",
            "epoch 7 - batch 410 - loss 71.71138763427734\n",
            "epoch 7 - batch 420 - loss 65.89889526367188\n",
            "epoch 7 - batch 430 - loss 73.1593246459961\n",
            "epoch 7 - batch 440 - loss 64.70941925048828\n",
            "epoch 7 - batch 450 - loss 69.03194427490234\n",
            "epoch 7 - batch 460 - loss 74.04613494873047\n",
            "epoch 7 - batch 470 - loss 67.26538848876953\n",
            "epoch 7 - batch 480 - loss 63.563568115234375\n",
            "epoch 7 - batch 490 - loss 70.00350189208984\n",
            "epoch 7 - batch 500 - loss 80.28955841064453\n",
            "epoch 7 - batch 510 - loss 63.34280014038086\n",
            "epoch 7 - batch 520 - loss 62.17801284790039\n",
            "epoch 7 - batch 530 - loss 65.73920440673828\n",
            "epoch 7 - batch 540 - loss 61.51253890991211\n",
            "epoch 7 - batch 550 - loss 57.31027603149414\n",
            "epoch 7 - batch 560 - loss 59.664642333984375\n",
            "epoch 7 - batch 570 - loss 77.59672546386719\n",
            "epoch 7 - batch 580 - loss 76.50550079345703\n",
            "epoch 7 - batch 590 - loss 64.57462310791016\n",
            "epoch 7 - batch 600 - loss 69.62859344482422\n",
            "epoch 7 - batch 610 - loss 62.80173110961914\n",
            "epoch 7 - batch 620 - loss 81.61876678466797\n",
            "epoch 7 - batch 630 - loss 64.57451629638672\n",
            "epoch 7 - batch 640 - loss 63.79780197143555\n",
            "epoch 7 - batch 650 - loss 61.76426315307617\n",
            "epoch 7 - batch 660 - loss 61.73659133911133\n",
            "epoch 7 - batch 670 - loss 59.738739013671875\n",
            "epoch 7 - batch 680 - loss 63.023406982421875\n",
            "epoch 7 - batch 690 - loss 52.1679801940918\n",
            "epoch 7 - batch 700 - loss 64.80366516113281\n",
            "epoch 7 - batch 710 - loss 70.534912109375\n",
            "epoch 7 - batch 720 - loss 55.66154098510742\n",
            "epoch 7 - batch 730 - loss 66.05728149414062\n",
            "epoch 7 - batch 740 - loss 64.70594787597656\n",
            "epoch 7 - batch 750 - loss 65.87384033203125\n",
            "epoch 7 - batch 760 - loss 65.36280059814453\n",
            "epoch 7 - batch 770 - loss 57.79502868652344\n",
            "epoch 7 - batch 780 - loss 68.30091094970703\n",
            "epoch 7 - batch 790 - loss 64.97943878173828\n",
            "epoch 7 - batch 800 - loss 51.27037048339844\n",
            "epoch 7 - batch 810 - loss 64.66474151611328\n",
            "epoch 7 - batch 820 - loss 66.41089630126953\n",
            "epoch 7 - batch 830 - loss 51.47184753417969\n",
            "epoch 7 - batch 840 - loss 67.8188247680664\n",
            "epoch 7 - batch 850 - loss 43.85500717163086\n",
            "epoch 7 - batch 860 - loss 58.39284133911133\n",
            "epoch 7 - batch 870 - loss 59.06467819213867\n",
            "epoch 7 - batch 880 - loss 64.91840362548828\n",
            "epoch 7 - batch 890 - loss 56.7452278137207\n",
            "epoch 7 - batch 900 - loss 59.37760543823242\n",
            "epoch 7 - batch 910 - loss 64.5402603149414\n",
            "epoch 7 - batch 920 - loss 67.44754028320312\n",
            "epoch 7 - batch 930 - loss 64.00639343261719\n",
            "epoch 7 - batch 940 - loss 69.4996337890625\n",
            "epoch 7 - batch 950 - loss 70.21871185302734\n",
            "epoch 7 - batch 960 - loss 73.75333404541016\n",
            "epoch 7 - batch 970 - loss 62.173770904541016\n",
            "epoch 7 - batch 980 - loss 70.7353515625\n",
            "epoch 7 - batch 990 - loss 60.160526275634766\n",
            "epoch 7 - batch 1000 - loss 69.23607635498047\n",
            "epoch 7 - batch 1010 - loss 74.36123657226562\n",
            "epoch 7 - batch 1020 - loss 76.19969940185547\n",
            "epoch 7 - batch 1030 - loss 72.75312042236328\n",
            "epoch 7 - batch 1040 - loss 77.73372650146484\n",
            "epoch 7 - batch 1050 - loss 79.53104400634766\n",
            "epoch 7 - batch 1060 - loss 65.07154083251953\n",
            "epoch 7 - batch 1070 - loss 72.87516784667969\n",
            "epoch 7 - batch 1080 - loss 72.99478912353516\n",
            "epoch 7 - batch 1090 - loss 71.5722885131836\n",
            "epoch 7 - batch 1100 - loss 73.23809814453125\n",
            "epoch 7 - batch 1110 - loss 63.02335739135742\n",
            "epoch 7 - batch 1120 - loss 63.48347091674805\n",
            "epoch 7 - batch 1130 - loss 60.75941467285156\n",
            "epoch 7 - batch 1140 - loss 73.31497955322266\n",
            "epoch 7 - batch 1150 - loss 54.15876388549805\n",
            "epoch 7 - batch 1160 - loss 68.4129409790039\n",
            "epoch 7 - batch 1170 - loss 49.91372299194336\n",
            "epoch 7 - batch 1180 - loss 65.42915344238281\n",
            "epoch 7 - batch 1190 - loss 55.694488525390625\n",
            "epoch 7 - batch 1200 - loss 64.2433090209961\n",
            "epoch 7 - batch 1210 - loss 56.046871185302734\n",
            "epoch 7 - batch 1220 - loss 68.75147247314453\n",
            "epoch 7 - batch 1230 - loss 60.44264602661133\n",
            "epoch 7 training time: 680.2414562702179 sec\n",
            "evaluation of batch 0 took: 0.050614356994628906\n",
            "evaluation of batch 50 took: 0.049646854400634766\n",
            "evaluation of batch 100 took: 0.11822175979614258\n",
            "evaluation of batch 150 took: 0.05024409294128418\n",
            "evaluation of batch 200 took: 0.05335831642150879\n",
            "evaluation of batch 250 took: 0.12577271461486816\n",
            "evaluation of batch 300 took: 0.05119180679321289\n",
            "evaluation of batch 350 took: 0.12137889862060547\n",
            "evaluation of batch 400 took: 0.05033540725708008\n",
            "evaluation of batch 450 took: 0.05043959617614746\n",
            "evaluation of batch 500 took: 0.05066967010498047\n",
            "evaluation of batch 550 took: 0.06978583335876465\n",
            "evaluation of batch 600 took: 0.11469769477844238\n",
            "evaluation of batch 650 took: 0.04974198341369629\n",
            "evaluation of batch 700 took: 0.04966855049133301\n",
            "evaluation of batch 750 took: 0.05225944519042969\n",
            "evaluation of batch 800 took: 0.05030655860900879\n",
            "evaluation of batch 850 took: 0.05242300033569336\n",
            "evaluation of batch 900 took: 0.06234407424926758\n",
            "evaluation of batch 950 took: 0.050974130630493164\n",
            "evaluation of batch 1000 took: 0.1203157901763916\n",
            "evaluation of batch 1050 took: 0.04982733726501465\n",
            "evaluation of batch 1100 took: 0.04964852333068848\n",
            "evaluation of batch 1150 took: 0.049089908599853516\n",
            "evaluation of batch 1200 took: 0.05485701560974121\n",
            "epoch 7 evaluation on training data time: 85.91169667243958 sec\n",
            "evaluation of batch 0 took: 0.060921430587768555\n",
            "evaluation of batch 50 took: 0.05154228210449219\n",
            "evaluation of batch 100 took: 0.05059623718261719\n",
            "evaluation of batch 150 took: 0.05075502395629883\n",
            "epoch 7 evaluation on test data time: 37.21030306816101 sec\n",
            "epoch evaluation:  {'epoch': 7, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=60.206043>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "Increasing epochs without improvement to: 2\n",
            "epoch 8 - batch 10 - loss 61.5098762512207\n",
            "epoch 8 - batch 20 - loss 56.22568130493164\n",
            "epoch 8 - batch 30 - loss 55.11224365234375\n",
            "epoch 8 - batch 40 - loss 49.552433013916016\n",
            "epoch 8 - batch 50 - loss 59.289005279541016\n",
            "epoch 8 - batch 60 - loss 59.48929977416992\n",
            "epoch 8 - batch 70 - loss 54.12845993041992\n",
            "epoch 8 - batch 80 - loss 54.167449951171875\n",
            "epoch 8 - batch 90 - loss 57.139705657958984\n",
            "epoch 8 - batch 100 - loss 56.78559494018555\n",
            "epoch 8 - batch 110 - loss 57.65224838256836\n",
            "epoch 8 - batch 120 - loss 63.55416488647461\n",
            "epoch 8 - batch 130 - loss 62.32926559448242\n",
            "epoch 8 - batch 140 - loss 53.90922927856445\n",
            "epoch 8 - batch 150 - loss 58.8625373840332\n",
            "epoch 8 - batch 160 - loss 59.47182846069336\n",
            "epoch 8 - batch 170 - loss 53.516845703125\n",
            "epoch 8 - batch 180 - loss 55.599002838134766\n",
            "epoch 8 - batch 190 - loss 66.62873840332031\n",
            "epoch 8 - batch 200 - loss 62.14341735839844\n",
            "epoch 8 - batch 210 - loss 58.66273498535156\n",
            "epoch 8 - batch 220 - loss 75.69242858886719\n",
            "epoch 8 - batch 230 - loss 51.88547897338867\n",
            "epoch 8 - batch 240 - loss 72.25194549560547\n",
            "epoch 8 - batch 250 - loss 50.98844528198242\n",
            "epoch 8 - batch 260 - loss 60.62588882446289\n",
            "epoch 8 - batch 270 - loss 54.64056396484375\n",
            "epoch 8 - batch 280 - loss 47.80437088012695\n",
            "epoch 8 - batch 290 - loss 45.96348190307617\n",
            "epoch 8 - batch 300 - loss 61.37644577026367\n",
            "epoch 8 - batch 310 - loss 60.578678131103516\n",
            "epoch 8 - batch 320 - loss 50.17855453491211\n",
            "epoch 8 - batch 330 - loss 53.09816360473633\n",
            "epoch 8 - batch 340 - loss 53.77210998535156\n",
            "epoch 8 - batch 350 - loss 52.58195877075195\n",
            "epoch 8 - batch 360 - loss 47.89933395385742\n",
            "epoch 8 - batch 370 - loss 48.661407470703125\n",
            "epoch 8 - batch 380 - loss 41.3681755065918\n",
            "epoch 8 - batch 390 - loss 45.47316360473633\n",
            "epoch 8 - batch 400 - loss 44.1313591003418\n",
            "epoch 8 - batch 410 - loss 53.39732360839844\n",
            "epoch 8 - batch 420 - loss 48.03445816040039\n",
            "epoch 8 - batch 430 - loss 47.50595474243164\n",
            "epoch 8 - batch 440 - loss 54.203216552734375\n",
            "epoch 8 - batch 450 - loss 41.433406829833984\n",
            "epoch 8 - batch 460 - loss 49.81871032714844\n",
            "epoch 8 - batch 470 - loss 51.42015075683594\n",
            "epoch 8 - batch 480 - loss 54.99690628051758\n",
            "epoch 8 - batch 490 - loss 48.691558837890625\n",
            "epoch 8 - batch 500 - loss 53.06771469116211\n",
            "epoch 8 - batch 510 - loss 47.19956970214844\n",
            "epoch 8 - batch 520 - loss 41.81166458129883\n",
            "epoch 8 - batch 530 - loss 52.60478210449219\n",
            "epoch 8 - batch 540 - loss 54.76871109008789\n",
            "epoch 8 - batch 550 - loss 52.21186828613281\n",
            "epoch 8 - batch 560 - loss 55.08671188354492\n",
            "epoch 8 - batch 570 - loss 54.18392562866211\n",
            "epoch 8 - batch 580 - loss 56.55022048950195\n",
            "epoch 8 - batch 590 - loss 58.12091064453125\n",
            "epoch 8 - batch 600 - loss 49.240291595458984\n",
            "epoch 8 - batch 610 - loss 52.2016716003418\n",
            "epoch 8 - batch 620 - loss 46.3542366027832\n",
            "epoch 8 - batch 630 - loss 54.643611907958984\n",
            "epoch 8 - batch 640 - loss 46.42142105102539\n",
            "epoch 8 - batch 650 - loss 49.65272521972656\n",
            "epoch 8 - batch 660 - loss 57.29361343383789\n",
            "epoch 8 - batch 670 - loss 59.30546188354492\n",
            "epoch 8 - batch 680 - loss 37.621482849121094\n",
            "epoch 8 - batch 690 - loss 49.860958099365234\n",
            "epoch 8 - batch 700 - loss 49.39404296875\n",
            "epoch 8 - batch 710 - loss 57.08903503417969\n",
            "epoch 8 - batch 720 - loss 47.124507904052734\n",
            "epoch 8 - batch 730 - loss 55.93324279785156\n",
            "epoch 8 - batch 740 - loss 57.257755279541016\n",
            "epoch 8 - batch 750 - loss 51.92995071411133\n",
            "epoch 8 - batch 760 - loss 52.890472412109375\n",
            "epoch 8 - batch 770 - loss 56.04362869262695\n",
            "epoch 8 - batch 780 - loss 48.4831428527832\n",
            "epoch 8 - batch 790 - loss 45.76285934448242\n",
            "epoch 8 - batch 800 - loss 51.21223831176758\n",
            "epoch 8 - batch 810 - loss 58.04567337036133\n",
            "epoch 8 - batch 820 - loss 48.87607192993164\n",
            "epoch 8 - batch 830 - loss 45.907257080078125\n",
            "epoch 8 - batch 840 - loss 52.97792434692383\n",
            "epoch 8 - batch 850 - loss 60.172943115234375\n",
            "epoch 8 - batch 860 - loss 60.91903305053711\n",
            "epoch 8 - batch 870 - loss 60.34346389770508\n",
            "epoch 8 - batch 880 - loss 53.28853225708008\n",
            "epoch 8 - batch 890 - loss 55.20608139038086\n",
            "epoch 8 - batch 900 - loss 52.242401123046875\n",
            "epoch 8 - batch 910 - loss 50.090579986572266\n",
            "epoch 8 - batch 920 - loss 54.5288200378418\n",
            "epoch 8 - batch 930 - loss 58.297603607177734\n",
            "epoch 8 - batch 940 - loss 55.77070236206055\n",
            "epoch 8 - batch 950 - loss 48.91176223754883\n",
            "epoch 8 - batch 960 - loss 55.50751876831055\n",
            "epoch 8 - batch 970 - loss 52.13034439086914\n",
            "epoch 8 - batch 980 - loss 54.6334342956543\n",
            "epoch 8 - batch 990 - loss 45.41341781616211\n",
            "epoch 8 - batch 1000 - loss 47.48211669921875\n",
            "epoch 8 - batch 1010 - loss 64.84184265136719\n",
            "epoch 8 - batch 1020 - loss 54.81010055541992\n",
            "epoch 8 - batch 1030 - loss 47.63621139526367\n",
            "epoch 8 - batch 1040 - loss 47.56654739379883\n",
            "epoch 8 - batch 1050 - loss 54.011661529541016\n",
            "epoch 8 - batch 1060 - loss 47.95721435546875\n",
            "epoch 8 - batch 1070 - loss 58.52012634277344\n",
            "epoch 8 - batch 1080 - loss 57.53854751586914\n",
            "epoch 8 - batch 1090 - loss 61.37214660644531\n",
            "epoch 8 - batch 1100 - loss 43.573299407958984\n",
            "epoch 8 - batch 1110 - loss 49.04136276245117\n",
            "epoch 8 - batch 1120 - loss 44.46493148803711\n",
            "epoch 8 - batch 1130 - loss 52.815277099609375\n",
            "epoch 8 - batch 1140 - loss 40.44916915893555\n",
            "epoch 8 - batch 1150 - loss 39.38119888305664\n",
            "epoch 8 - batch 1160 - loss 43.17631912231445\n",
            "epoch 8 - batch 1170 - loss 49.20656204223633\n",
            "epoch 8 - batch 1180 - loss 39.924171447753906\n",
            "epoch 8 - batch 1190 - loss 51.21113967895508\n",
            "epoch 8 - batch 1200 - loss 48.60359191894531\n",
            "epoch 8 - batch 1210 - loss 42.2618293762207\n",
            "epoch 8 - batch 1220 - loss 39.651668548583984\n",
            "epoch 8 - batch 1230 - loss 41.774288177490234\n",
            "epoch 8 training time: 679.2579488754272 sec\n",
            "evaluation of batch 0 took: 0.05265307426452637\n",
            "evaluation of batch 50 took: 0.05053067207336426\n",
            "evaluation of batch 100 took: 0.12036561965942383\n",
            "evaluation of batch 150 took: 0.0516512393951416\n",
            "evaluation of batch 200 took: 0.051131248474121094\n",
            "evaluation of batch 250 took: 0.05016827583312988\n",
            "evaluation of batch 300 took: 0.05274510383605957\n",
            "evaluation of batch 350 took: 0.05312323570251465\n",
            "evaluation of batch 400 took: 0.050206661224365234\n",
            "evaluation of batch 450 took: 0.05868244171142578\n",
            "evaluation of batch 500 took: 0.05167531967163086\n",
            "evaluation of batch 550 took: 0.054825544357299805\n",
            "evaluation of batch 600 took: 0.05167579650878906\n",
            "evaluation of batch 650 took: 0.07313799858093262\n",
            "evaluation of batch 700 took: 0.05177164077758789\n",
            "evaluation of batch 750 took: 0.052239179611206055\n",
            "evaluation of batch 800 took: 0.05156397819519043\n",
            "evaluation of batch 850 took: 0.05019664764404297\n",
            "evaluation of batch 900 took: 0.11654043197631836\n",
            "evaluation of batch 950 took: 0.05003213882446289\n",
            "evaluation of batch 1000 took: 0.048868417739868164\n",
            "evaluation of batch 1050 took: 0.04985451698303223\n",
            "evaluation of batch 1100 took: 0.04935765266418457\n",
            "evaluation of batch 1150 took: 0.04977774620056152\n",
            "evaluation of batch 1200 took: 0.1198124885559082\n",
            "epoch 8 evaluation on training data time: 88.29550576210022 sec\n",
            "evaluation of batch 0 took: 0.12691473960876465\n",
            "evaluation of batch 50 took: 0.11588096618652344\n",
            "evaluation of batch 100 took: 0.11708712577819824\n",
            "evaluation of batch 150 took: 0.11513328552246094\n",
            "epoch 8 evaluation on test data time: 47.590957164764404 sec\n",
            "epoch evaluation:  {'epoch': 8, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=46.14517>, 'test_rouge_1_p': 0.026627181412337646, 'test_rouge_1_r': 0.04131789434523809, 'test_rouge_1_f1': 0.03117661003149534, 'test_rouge_2_p': 2.0292207792207792e-05, 'test_rouge_2_r': 2.1983225108225107e-05, 'test_rouge_2_f1': 2.0926339285714286e-05, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.026627181412337646, 'test_rouge_L_r': 0.04131789434523809, 'test_rouge_L_f1': 0.03117661003149534}\n",
            "epoch 9 - batch 10 - loss 37.62001037597656\n",
            "epoch 9 - batch 20 - loss 45.12605285644531\n",
            "epoch 9 - batch 30 - loss 43.020511627197266\n",
            "epoch 9 - batch 40 - loss 51.9163818359375\n",
            "epoch 9 - batch 50 - loss 48.029293060302734\n",
            "epoch 9 - batch 60 - loss 43.097286224365234\n",
            "epoch 9 - batch 70 - loss 42.8670539855957\n",
            "epoch 9 - batch 80 - loss 36.817752838134766\n",
            "epoch 9 - batch 90 - loss 43.39628982543945\n",
            "epoch 9 - batch 100 - loss 46.7843017578125\n",
            "epoch 9 - batch 110 - loss 46.38705062866211\n",
            "epoch 9 - batch 120 - loss 38.83730697631836\n",
            "epoch 9 - batch 130 - loss 46.48015213012695\n",
            "epoch 9 - batch 140 - loss 46.60572814941406\n",
            "epoch 9 - batch 150 - loss 52.67245864868164\n",
            "epoch 9 - batch 160 - loss 40.28921127319336\n",
            "epoch 9 - batch 170 - loss 38.21243667602539\n",
            "epoch 9 - batch 180 - loss 39.87666320800781\n",
            "epoch 9 - batch 190 - loss 48.98784255981445\n",
            "epoch 9 - batch 200 - loss 41.50337219238281\n",
            "epoch 9 - batch 210 - loss 49.08157730102539\n",
            "epoch 9 - batch 220 - loss 42.30497360229492\n",
            "epoch 9 - batch 230 - loss 37.425594329833984\n",
            "epoch 9 - batch 240 - loss 39.57477569580078\n",
            "epoch 9 - batch 250 - loss 42.8310661315918\n",
            "epoch 9 - batch 260 - loss 47.44656753540039\n",
            "epoch 9 - batch 270 - loss 52.33829116821289\n",
            "epoch 9 - batch 280 - loss 39.98969650268555\n",
            "epoch 9 - batch 290 - loss 42.8718147277832\n",
            "epoch 9 - batch 300 - loss 46.78181838989258\n",
            "epoch 9 - batch 310 - loss 49.53578567504883\n",
            "epoch 9 - batch 320 - loss 42.00218200683594\n",
            "epoch 9 - batch 330 - loss 37.61629104614258\n",
            "epoch 9 - batch 340 - loss 45.57644271850586\n",
            "epoch 9 - batch 350 - loss 39.451255798339844\n",
            "epoch 9 - batch 360 - loss 41.59467315673828\n",
            "epoch 9 - batch 370 - loss 38.41461181640625\n",
            "epoch 9 - batch 380 - loss 44.73390197753906\n",
            "epoch 9 - batch 390 - loss 46.83100891113281\n",
            "epoch 9 - batch 400 - loss 48.91647720336914\n",
            "epoch 9 - batch 410 - loss 52.21400833129883\n",
            "epoch 9 - batch 420 - loss 49.63443374633789\n",
            "epoch 9 - batch 430 - loss 54.32731628417969\n",
            "epoch 9 - batch 440 - loss 49.03181076049805\n",
            "epoch 9 - batch 450 - loss 42.07278823852539\n",
            "epoch 9 - batch 460 - loss 45.468265533447266\n",
            "epoch 9 - batch 470 - loss 54.0916748046875\n",
            "epoch 9 - batch 480 - loss 63.145748138427734\n",
            "epoch 9 - batch 490 - loss 56.46748352050781\n",
            "epoch 9 - batch 500 - loss 55.58590316772461\n",
            "epoch 9 - batch 510 - loss 58.7419319152832\n",
            "epoch 9 - batch 520 - loss 48.48017501831055\n",
            "epoch 9 - batch 530 - loss 48.44358825683594\n",
            "epoch 9 - batch 540 - loss 52.474361419677734\n",
            "epoch 9 - batch 550 - loss 57.99152755737305\n",
            "epoch 9 - batch 560 - loss 59.73121643066406\n",
            "epoch 9 - batch 570 - loss 53.492462158203125\n",
            "epoch 9 - batch 580 - loss 54.61381912231445\n",
            "epoch 9 - batch 590 - loss 52.10731887817383\n",
            "epoch 9 - batch 600 - loss 48.16823959350586\n",
            "epoch 9 - batch 610 - loss 55.2673225402832\n",
            "epoch 9 - batch 620 - loss 57.99203872680664\n",
            "epoch 9 - batch 630 - loss 53.45416259765625\n",
            "epoch 9 - batch 640 - loss 57.07416915893555\n",
            "epoch 9 - batch 650 - loss 48.008914947509766\n",
            "epoch 9 - batch 660 - loss 53.75455856323242\n",
            "epoch 9 - batch 670 - loss 46.24502182006836\n",
            "epoch 9 - batch 680 - loss 49.938568115234375\n",
            "epoch 9 - batch 690 - loss 52.3881950378418\n",
            "epoch 9 - batch 700 - loss 44.20924377441406\n",
            "epoch 9 - batch 710 - loss 42.179744720458984\n",
            "epoch 9 - batch 720 - loss 53.550872802734375\n",
            "epoch 9 - batch 730 - loss 41.680450439453125\n",
            "epoch 9 - batch 740 - loss 45.759212493896484\n",
            "epoch 9 - batch 750 - loss 44.13674545288086\n",
            "epoch 9 - batch 760 - loss 46.80523681640625\n",
            "epoch 9 - batch 770 - loss 42.65275192260742\n",
            "epoch 9 - batch 780 - loss 50.44331741333008\n",
            "epoch 9 - batch 790 - loss 39.843624114990234\n",
            "epoch 9 - batch 800 - loss 44.88331985473633\n",
            "epoch 9 - batch 810 - loss 43.89713668823242\n",
            "epoch 9 - batch 820 - loss 41.41061019897461\n",
            "epoch 9 - batch 830 - loss 48.81819534301758\n",
            "epoch 9 - batch 840 - loss 46.05863571166992\n",
            "epoch 9 - batch 850 - loss 37.4566650390625\n",
            "epoch 9 - batch 860 - loss 38.249935150146484\n",
            "epoch 9 - batch 870 - loss 44.163482666015625\n",
            "epoch 9 - batch 880 - loss 40.91085433959961\n",
            "epoch 9 - batch 890 - loss 41.304935455322266\n",
            "epoch 9 - batch 900 - loss 37.72535705566406\n",
            "epoch 9 - batch 910 - loss 42.28755569458008\n",
            "epoch 9 - batch 920 - loss 40.11278533935547\n",
            "epoch 9 - batch 930 - loss 38.558658599853516\n",
            "epoch 9 - batch 940 - loss 48.72618103027344\n",
            "epoch 9 - batch 950 - loss 41.513362884521484\n",
            "epoch 9 - batch 960 - loss 41.01667785644531\n",
            "epoch 9 - batch 970 - loss 38.32057571411133\n",
            "epoch 9 - batch 980 - loss 37.4537353515625\n",
            "epoch 9 - batch 990 - loss 37.026512145996094\n",
            "epoch 9 - batch 1000 - loss 36.642608642578125\n",
            "epoch 9 - batch 1010 - loss 37.039817810058594\n",
            "epoch 9 - batch 1020 - loss 36.6422233581543\n",
            "epoch 9 - batch 1030 - loss 40.67729568481445\n",
            "epoch 9 - batch 1040 - loss 41.13601303100586\n",
            "epoch 9 - batch 1050 - loss 36.5925407409668\n",
            "epoch 9 - batch 1060 - loss 45.21684646606445\n",
            "epoch 9 - batch 1070 - loss 40.7413444519043\n",
            "epoch 9 - batch 1080 - loss 38.50873947143555\n",
            "epoch 9 - batch 1090 - loss 40.521507263183594\n",
            "epoch 9 - batch 1100 - loss 41.1990852355957\n",
            "epoch 9 - batch 1110 - loss 35.46181106567383\n",
            "epoch 9 - batch 1120 - loss 35.23939895629883\n",
            "epoch 9 - batch 1130 - loss 38.69840621948242\n",
            "epoch 9 - batch 1140 - loss 42.005157470703125\n",
            "epoch 9 - batch 1150 - loss 36.4404411315918\n",
            "epoch 9 - batch 1160 - loss 48.24583053588867\n",
            "epoch 9 - batch 1170 - loss 44.934051513671875\n",
            "epoch 9 - batch 1180 - loss 39.156551361083984\n",
            "epoch 9 - batch 1190 - loss 33.33127212524414\n",
            "epoch 9 - batch 1200 - loss 39.80881881713867\n",
            "epoch 9 - batch 1210 - loss 40.33170700073242\n",
            "epoch 9 - batch 1220 - loss 47.84751510620117\n",
            "epoch 9 - batch 1230 - loss 39.471343994140625\n",
            "epoch 9 training time: 682.5831241607666 sec\n",
            "evaluation of batch 0 took: 0.12136530876159668\n",
            "evaluation of batch 50 took: 0.049784183502197266\n",
            "evaluation of batch 100 took: 0.04983925819396973\n",
            "evaluation of batch 150 took: 0.05107998847961426\n",
            "evaluation of batch 200 took: 0.05063629150390625\n",
            "evaluation of batch 250 took: 0.04933977127075195\n",
            "evaluation of batch 300 took: 0.05148720741271973\n",
            "evaluation of batch 350 took: 0.12084031105041504\n",
            "evaluation of batch 400 took: 0.049397945404052734\n",
            "evaluation of batch 450 took: 0.07171034812927246\n",
            "evaluation of batch 500 took: 0.07396197319030762\n",
            "evaluation of batch 550 took: 0.11865758895874023\n",
            "evaluation of batch 600 took: 0.12071919441223145\n",
            "evaluation of batch 650 took: 0.0504298210144043\n",
            "evaluation of batch 700 took: 0.049948930740356445\n",
            "evaluation of batch 750 took: 0.055602073669433594\n",
            "evaluation of batch 800 took: 0.0489344596862793\n",
            "evaluation of batch 850 took: 0.04865097999572754\n",
            "evaluation of batch 900 took: 0.0729975700378418\n",
            "evaluation of batch 950 took: 0.051021575927734375\n",
            "evaluation of batch 1000 took: 0.04920458793640137\n",
            "evaluation of batch 1050 took: 0.12936735153198242\n",
            "evaluation of batch 1100 took: 0.05538749694824219\n",
            "evaluation of batch 1150 took: 0.13503694534301758\n",
            "evaluation of batch 1200 took: 0.05044293403625488\n",
            "epoch 9 evaluation on training data time: 87.07853102684021 sec\n",
            "evaluation of batch 0 took: 0.06078481674194336\n",
            "evaluation of batch 50 took: 0.05150413513183594\n",
            "evaluation of batch 100 took: 0.04879617691040039\n",
            "evaluation of batch 150 took: 0.04954338073730469\n",
            "epoch 9 evaluation on test data time: 37.33217096328735 sec\n",
            "epoch evaluation:  {'epoch': 9, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=41.35266>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "epoch 9 saved checkpoint: models/checkpoints/baseline/ckpt-4\n",
            "Increasing epochs without improvement to: 1\n",
            "epoch 10 - batch 10 - loss 38.9448356628418\n",
            "epoch 10 - batch 20 - loss 42.947906494140625\n",
            "epoch 10 - batch 30 - loss 42.96772766113281\n",
            "epoch 10 - batch 40 - loss 41.74824905395508\n",
            "epoch 10 - batch 50 - loss 37.98792266845703\n",
            "epoch 10 - batch 60 - loss 43.31697082519531\n",
            "epoch 10 - batch 70 - loss 44.05461502075195\n",
            "epoch 10 - batch 80 - loss 42.51014709472656\n",
            "epoch 10 - batch 90 - loss 41.1852912902832\n",
            "epoch 10 - batch 100 - loss 43.6347541809082\n",
            "epoch 10 - batch 110 - loss 46.86351013183594\n",
            "epoch 10 - batch 120 - loss 37.46029281616211\n",
            "epoch 10 - batch 130 - loss 33.805728912353516\n",
            "epoch 10 - batch 140 - loss 36.61153793334961\n",
            "epoch 10 - batch 150 - loss 38.75358963012695\n",
            "epoch 10 - batch 160 - loss 43.32991027832031\n",
            "epoch 10 - batch 170 - loss 39.51927185058594\n",
            "epoch 10 - batch 180 - loss 45.2252311706543\n",
            "epoch 10 - batch 190 - loss 43.44484329223633\n",
            "epoch 10 - batch 200 - loss 43.345340728759766\n",
            "epoch 10 - batch 210 - loss 43.07806396484375\n",
            "epoch 10 - batch 220 - loss 43.79243850708008\n",
            "epoch 10 - batch 230 - loss 36.430782318115234\n",
            "epoch 10 - batch 240 - loss 42.60219192504883\n",
            "epoch 10 - batch 250 - loss 39.71235275268555\n",
            "epoch 10 - batch 260 - loss 48.89886474609375\n",
            "epoch 10 - batch 270 - loss 36.14417266845703\n",
            "epoch 10 - batch 280 - loss 49.3248405456543\n",
            "epoch 10 - batch 290 - loss 43.65377426147461\n",
            "epoch 10 - batch 300 - loss 39.399593353271484\n",
            "epoch 10 - batch 310 - loss 44.640682220458984\n",
            "epoch 10 - batch 320 - loss 45.21553039550781\n",
            "epoch 10 - batch 330 - loss 47.847293853759766\n",
            "epoch 10 - batch 340 - loss 40.22539520263672\n",
            "epoch 10 - batch 350 - loss 51.828556060791016\n",
            "epoch 10 - batch 360 - loss 42.493404388427734\n",
            "epoch 10 - batch 370 - loss 42.63666915893555\n",
            "epoch 10 - batch 380 - loss 45.34288024902344\n",
            "epoch 10 - batch 390 - loss 48.53042221069336\n",
            "epoch 10 - batch 400 - loss 51.570098876953125\n",
            "epoch 10 - batch 410 - loss 45.08012008666992\n",
            "epoch 10 - batch 420 - loss 51.48737716674805\n",
            "epoch 10 - batch 430 - loss 50.585968017578125\n",
            "epoch 10 - batch 440 - loss 50.15239334106445\n",
            "epoch 10 - batch 450 - loss 39.83173751831055\n",
            "epoch 10 - batch 460 - loss 41.59862518310547\n",
            "epoch 10 - batch 470 - loss 43.5341911315918\n",
            "epoch 10 - batch 480 - loss 45.39662170410156\n",
            "epoch 10 - batch 490 - loss 46.00312423706055\n",
            "epoch 10 - batch 500 - loss 46.8115119934082\n",
            "epoch 10 - batch 510 - loss 45.86785888671875\n",
            "epoch 10 - batch 520 - loss 37.6235466003418\n",
            "epoch 10 - batch 530 - loss 36.658382415771484\n",
            "epoch 10 - batch 540 - loss 42.68792724609375\n",
            "epoch 10 - batch 550 - loss 42.78759765625\n",
            "epoch 10 - batch 560 - loss 41.24643325805664\n",
            "epoch 10 - batch 570 - loss 38.86362075805664\n",
            "epoch 10 - batch 580 - loss 36.76700210571289\n",
            "epoch 10 - batch 590 - loss 44.16817092895508\n",
            "epoch 10 - batch 600 - loss 37.8420295715332\n",
            "epoch 10 - batch 610 - loss 35.09373474121094\n",
            "epoch 10 - batch 620 - loss 35.0389518737793\n",
            "epoch 10 - batch 630 - loss 33.5678596496582\n",
            "epoch 10 - batch 640 - loss 43.87391662597656\n",
            "epoch 10 - batch 650 - loss 35.209381103515625\n",
            "epoch 10 - batch 660 - loss 33.70762634277344\n",
            "epoch 10 - batch 670 - loss 37.32401657104492\n",
            "epoch 10 - batch 680 - loss 30.850797653198242\n",
            "epoch 10 - batch 690 - loss 42.92427444458008\n",
            "epoch 10 - batch 700 - loss 38.17665100097656\n",
            "epoch 10 - batch 710 - loss 33.740169525146484\n",
            "epoch 10 - batch 720 - loss 37.519596099853516\n",
            "epoch 10 - batch 730 - loss 33.235469818115234\n",
            "epoch 10 - batch 740 - loss 37.418827056884766\n",
            "epoch 10 - batch 750 - loss 29.683517456054688\n",
            "epoch 10 - batch 760 - loss 37.440486907958984\n",
            "epoch 10 - batch 770 - loss 32.9583625793457\n",
            "epoch 10 - batch 780 - loss 33.64214324951172\n",
            "epoch 10 - batch 790 - loss 27.245512008666992\n",
            "epoch 10 - batch 800 - loss 31.299972534179688\n",
            "epoch 10 - batch 810 - loss 30.82961082458496\n",
            "epoch 10 - batch 820 - loss 33.40584945678711\n",
            "epoch 10 - batch 830 - loss 32.250205993652344\n",
            "epoch 10 - batch 840 - loss 39.03904342651367\n",
            "epoch 10 - batch 850 - loss 35.47343063354492\n",
            "epoch 10 - batch 860 - loss 38.23601150512695\n",
            "epoch 10 - batch 870 - loss 32.01554489135742\n",
            "epoch 10 - batch 880 - loss 36.78602981567383\n",
            "epoch 10 - batch 890 - loss 34.33462142944336\n",
            "epoch 10 - batch 900 - loss 42.613033294677734\n",
            "epoch 10 - batch 910 - loss 35.58327865600586\n",
            "epoch 10 - batch 920 - loss 32.33835983276367\n",
            "epoch 10 - batch 930 - loss 38.91690444946289\n",
            "epoch 10 - batch 940 - loss 38.66746520996094\n",
            "epoch 10 - batch 950 - loss 38.57539749145508\n",
            "epoch 10 - batch 960 - loss 35.27862548828125\n",
            "epoch 10 - batch 970 - loss 40.60806655883789\n",
            "epoch 10 - batch 980 - loss 40.75118637084961\n",
            "epoch 10 - batch 990 - loss 46.427547454833984\n",
            "epoch 10 - batch 1000 - loss 43.967529296875\n",
            "epoch 10 - batch 1010 - loss 38.351463317871094\n",
            "epoch 10 - batch 1020 - loss 40.063133239746094\n",
            "epoch 10 - batch 1030 - loss 41.16939926147461\n",
            "epoch 10 - batch 1040 - loss 46.16914749145508\n",
            "epoch 10 - batch 1050 - loss 48.838226318359375\n",
            "epoch 10 - batch 1060 - loss 42.70454406738281\n",
            "epoch 10 - batch 1070 - loss 47.675655364990234\n",
            "epoch 10 - batch 1080 - loss 42.64046096801758\n",
            "epoch 10 - batch 1090 - loss 42.490516662597656\n",
            "epoch 10 - batch 1100 - loss 43.55732345581055\n",
            "epoch 10 - batch 1110 - loss 42.74588394165039\n",
            "epoch 10 - batch 1120 - loss 43.50251770019531\n",
            "epoch 10 - batch 1130 - loss 42.179229736328125\n",
            "epoch 10 - batch 1140 - loss 51.185604095458984\n",
            "epoch 10 - batch 1150 - loss 46.471221923828125\n",
            "epoch 10 - batch 1160 - loss 46.72541427612305\n",
            "epoch 10 - batch 1170 - loss 48.006038665771484\n",
            "epoch 10 - batch 1180 - loss 43.2692985534668\n",
            "epoch 10 - batch 1190 - loss 50.12582778930664\n",
            "epoch 10 - batch 1200 - loss 40.975364685058594\n",
            "epoch 10 - batch 1210 - loss 48.4100341796875\n",
            "epoch 10 - batch 1220 - loss 50.84525680541992\n",
            "epoch 10 - batch 1230 - loss 48.74534606933594\n",
            "epoch 10 training time: 681.1556663513184 sec\n",
            "evaluation of batch 0 took: 0.051181793212890625\n",
            "evaluation of batch 50 took: 0.1186974048614502\n",
            "evaluation of batch 100 took: 0.05451488494873047\n",
            "evaluation of batch 150 took: 0.13723492622375488\n",
            "evaluation of batch 200 took: 0.05226540565490723\n",
            "evaluation of batch 250 took: 0.07048702239990234\n",
            "evaluation of batch 300 took: 0.05371689796447754\n",
            "evaluation of batch 350 took: 0.05043148994445801\n",
            "evaluation of batch 400 took: 0.07459092140197754\n",
            "evaluation of batch 450 took: 0.06308937072753906\n",
            "evaluation of batch 500 took: 0.05081939697265625\n",
            "evaluation of batch 550 took: 0.12213516235351562\n",
            "evaluation of batch 600 took: 0.05246591567993164\n",
            "evaluation of batch 650 took: 0.05259108543395996\n",
            "evaluation of batch 700 took: 0.12250471115112305\n",
            "evaluation of batch 750 took: 0.04939460754394531\n",
            "evaluation of batch 800 took: 0.15322422981262207\n",
            "evaluation of batch 850 took: 0.12252402305603027\n",
            "evaluation of batch 900 took: 0.1289381980895996\n",
            "evaluation of batch 950 took: 0.0537416934967041\n",
            "evaluation of batch 1000 took: 0.04998278617858887\n",
            "evaluation of batch 1050 took: 0.07193541526794434\n",
            "evaluation of batch 1100 took: 0.05043172836303711\n",
            "evaluation of batch 1150 took: 0.07636117935180664\n",
            "evaluation of batch 1200 took: 0.06948971748352051\n",
            "epoch 10 evaluation on training data time: 89.59936809539795 sec\n",
            "evaluation of batch 0 took: 0.1418135166168213\n",
            "evaluation of batch 50 took: 0.13499164581298828\n",
            "evaluation of batch 100 took: 0.13060259819030762\n",
            "evaluation of batch 150 took: 0.13234663009643555\n",
            "epoch 10 evaluation on test data time: 50.0987663269043 sec\n",
            "epoch evaluation:  {'epoch': 10, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=47.635612>, 'test_rouge_1_p': 0.0002642214556277057, 'test_rouge_1_r': 0.000526540516774892, 'test_rouge_1_f1': 0.0003385530133196526, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0002642214556277057, 'test_rouge_L_r': 0.000526540516774892, 'test_rouge_L_f1': 0.0003385530133196526}\n",
            "Increasing epochs without improvement to: 2\n",
            "epoch 11 - batch 10 - loss 46.295230865478516\n",
            "epoch 11 - batch 20 - loss 41.244384765625\n",
            "epoch 11 - batch 30 - loss 47.1577033996582\n",
            "epoch 11 - batch 40 - loss 49.409393310546875\n",
            "epoch 11 - batch 50 - loss 48.42961120605469\n",
            "epoch 11 - batch 60 - loss 44.562686920166016\n",
            "epoch 11 - batch 70 - loss 47.036495208740234\n",
            "epoch 11 - batch 80 - loss 43.982852935791016\n",
            "epoch 11 - batch 90 - loss 46.88356399536133\n",
            "epoch 11 - batch 100 - loss 44.54567337036133\n",
            "epoch 11 - batch 110 - loss 51.856258392333984\n",
            "epoch 11 - batch 120 - loss 51.06336975097656\n",
            "epoch 11 - batch 130 - loss 47.30320358276367\n",
            "epoch 11 - batch 140 - loss 48.068363189697266\n",
            "epoch 11 - batch 150 - loss 45.08891677856445\n",
            "epoch 11 - batch 160 - loss 52.77097702026367\n",
            "epoch 11 - batch 170 - loss 40.91585159301758\n",
            "epoch 11 - batch 180 - loss 40.464595794677734\n",
            "epoch 11 - batch 190 - loss 48.64007568359375\n",
            "epoch 11 - batch 200 - loss 47.7782096862793\n",
            "epoch 11 - batch 210 - loss 44.126312255859375\n",
            "epoch 11 - batch 220 - loss 51.190185546875\n",
            "epoch 11 - batch 230 - loss 41.74612808227539\n",
            "epoch 11 - batch 240 - loss 45.51448059082031\n",
            "epoch 11 - batch 250 - loss 38.979888916015625\n",
            "epoch 11 - batch 260 - loss 38.22492599487305\n",
            "epoch 11 - batch 270 - loss 48.979557037353516\n",
            "epoch 11 - batch 280 - loss 41.360870361328125\n",
            "epoch 11 - batch 290 - loss 38.74549102783203\n",
            "epoch 11 - batch 300 - loss 35.2633171081543\n",
            "epoch 11 - batch 310 - loss 40.52072525024414\n",
            "epoch 11 - batch 320 - loss 39.32859802246094\n",
            "epoch 11 - batch 330 - loss 41.461856842041016\n",
            "epoch 11 - batch 340 - loss 38.34016418457031\n",
            "epoch 11 - batch 350 - loss 42.40617752075195\n",
            "epoch 11 - batch 360 - loss 35.21609115600586\n",
            "epoch 11 - batch 370 - loss 44.02915573120117\n",
            "epoch 11 - batch 380 - loss 34.64753341674805\n",
            "epoch 11 - batch 390 - loss 38.31327819824219\n",
            "epoch 11 - batch 400 - loss 40.17911911010742\n",
            "epoch 11 - batch 410 - loss 34.62214279174805\n",
            "epoch 11 - batch 420 - loss 35.02622985839844\n",
            "epoch 11 - batch 430 - loss 36.179508209228516\n",
            "epoch 11 - batch 440 - loss 37.58451843261719\n",
            "epoch 11 - batch 450 - loss 31.8492488861084\n",
            "epoch 11 - batch 460 - loss 35.81685256958008\n",
            "epoch 11 - batch 470 - loss 34.964630126953125\n",
            "epoch 11 - batch 480 - loss 36.1466178894043\n",
            "epoch 11 - batch 490 - loss 36.20811080932617\n",
            "epoch 11 - batch 500 - loss 34.30733108520508\n",
            "epoch 11 - batch 510 - loss 38.0170783996582\n",
            "epoch 11 - batch 520 - loss 35.677146911621094\n",
            "epoch 11 - batch 530 - loss 36.50137710571289\n",
            "epoch 11 - batch 540 - loss 33.41896057128906\n",
            "epoch 11 - batch 550 - loss 37.79538345336914\n",
            "epoch 11 - batch 560 - loss 34.68068313598633\n",
            "epoch 11 - batch 570 - loss 34.54689025878906\n",
            "epoch 11 - batch 580 - loss 36.427032470703125\n",
            "epoch 11 - batch 590 - loss 36.39778518676758\n",
            "epoch 11 - batch 600 - loss 38.44314193725586\n",
            "epoch 11 - batch 610 - loss 37.30333709716797\n",
            "epoch 11 - batch 620 - loss 33.64335632324219\n",
            "epoch 11 - batch 630 - loss 33.33678436279297\n",
            "epoch 11 - batch 640 - loss 39.797630310058594\n",
            "epoch 11 - batch 650 - loss 40.785308837890625\n",
            "epoch 11 - batch 660 - loss 31.689132690429688\n",
            "epoch 11 - batch 670 - loss 34.43562698364258\n",
            "epoch 11 - batch 680 - loss 35.389156341552734\n",
            "epoch 11 - batch 690 - loss 38.966007232666016\n",
            "epoch 11 - batch 700 - loss 42.647743225097656\n",
            "epoch 11 - batch 710 - loss 35.96578598022461\n",
            "epoch 11 - batch 720 - loss 35.39357376098633\n",
            "epoch 11 - batch 730 - loss 36.556026458740234\n",
            "epoch 11 - batch 740 - loss 39.72654342651367\n",
            "epoch 11 - batch 750 - loss 37.28681182861328\n",
            "epoch 11 - batch 760 - loss 44.028018951416016\n",
            "epoch 11 - batch 770 - loss 39.907283782958984\n",
            "epoch 11 - batch 780 - loss 31.54473304748535\n",
            "epoch 11 - batch 790 - loss 36.39500427246094\n",
            "epoch 11 - batch 800 - loss 39.08151626586914\n",
            "epoch 11 - batch 810 - loss 33.48567199707031\n",
            "epoch 11 - batch 820 - loss 37.6515998840332\n",
            "epoch 11 - batch 830 - loss 41.33487319946289\n",
            "epoch 11 - batch 840 - loss 38.50083541870117\n",
            "epoch 11 - batch 850 - loss 39.89614486694336\n",
            "epoch 11 - batch 860 - loss 41.609066009521484\n",
            "epoch 11 - batch 870 - loss 37.041744232177734\n",
            "epoch 11 - batch 880 - loss 35.51815414428711\n",
            "epoch 11 - batch 890 - loss 41.09345245361328\n",
            "epoch 11 - batch 900 - loss 32.4899787902832\n",
            "epoch 11 - batch 910 - loss 34.38700866699219\n",
            "epoch 11 - batch 920 - loss 38.48650360107422\n",
            "epoch 11 - batch 930 - loss 39.432987213134766\n",
            "epoch 11 - batch 940 - loss 39.27085494995117\n",
            "epoch 11 - batch 950 - loss 43.103271484375\n",
            "epoch 11 - batch 960 - loss 39.89420700073242\n",
            "epoch 11 - batch 970 - loss 40.94015121459961\n",
            "epoch 11 - batch 980 - loss 37.06546401977539\n",
            "epoch 11 - batch 990 - loss 41.611602783203125\n",
            "epoch 11 - batch 1000 - loss 34.76139831542969\n",
            "epoch 11 - batch 1010 - loss 39.20111846923828\n",
            "epoch 11 - batch 1020 - loss 36.52095413208008\n",
            "epoch 11 - batch 1030 - loss 35.62035369873047\n",
            "epoch 11 - batch 1040 - loss 41.91638946533203\n",
            "epoch 11 - batch 1050 - loss 39.92583084106445\n",
            "epoch 11 - batch 1060 - loss 37.08280944824219\n",
            "epoch 11 - batch 1070 - loss 43.05659484863281\n",
            "epoch 11 - batch 1080 - loss 40.79927062988281\n",
            "epoch 11 - batch 1090 - loss 37.073429107666016\n",
            "epoch 11 - batch 1100 - loss 41.6096076965332\n",
            "epoch 11 - batch 1110 - loss 41.238399505615234\n",
            "epoch 11 - batch 1120 - loss 41.887821197509766\n",
            "epoch 11 - batch 1130 - loss 42.14448928833008\n",
            "epoch 11 - batch 1140 - loss 43.627254486083984\n",
            "epoch 11 - batch 1150 - loss 48.973079681396484\n",
            "epoch 11 - batch 1160 - loss 43.521148681640625\n",
            "epoch 11 - batch 1170 - loss 44.84274673461914\n",
            "epoch 11 - batch 1180 - loss 46.56058120727539\n",
            "epoch 11 - batch 1190 - loss 47.61538314819336\n",
            "epoch 11 - batch 1200 - loss 45.94852066040039\n",
            "epoch 11 - batch 1210 - loss 45.23178482055664\n",
            "epoch 11 - batch 1220 - loss 48.8825569152832\n",
            "epoch 11 - batch 1230 - loss 52.11809158325195\n",
            "epoch 11 training time: 680.7737786769867 sec\n",
            "evaluation of batch 0 took: 0.1365516185760498\n",
            "evaluation of batch 50 took: 0.12400579452514648\n",
            "evaluation of batch 100 took: 0.05445671081542969\n",
            "evaluation of batch 150 took: 0.05082845687866211\n",
            "evaluation of batch 200 took: 0.05300784111022949\n",
            "evaluation of batch 250 took: 0.11669182777404785\n",
            "evaluation of batch 300 took: 0.05150198936462402\n",
            "evaluation of batch 350 took: 0.051598310470581055\n",
            "evaluation of batch 400 took: 0.05157351493835449\n",
            "evaluation of batch 450 took: 0.052649497985839844\n",
            "evaluation of batch 500 took: 0.05259823799133301\n",
            "evaluation of batch 550 took: 0.050716400146484375\n",
            "evaluation of batch 600 took: 0.1216745376586914\n",
            "evaluation of batch 650 took: 0.05817008018493652\n",
            "evaluation of batch 700 took: 0.0544736385345459\n",
            "evaluation of batch 750 took: 0.052448272705078125\n",
            "evaluation of batch 800 took: 0.053025245666503906\n",
            "evaluation of batch 850 took: 0.11458253860473633\n",
            "evaluation of batch 900 took: 0.0503237247467041\n",
            "evaluation of batch 950 took: 0.05202627182006836\n",
            "evaluation of batch 1000 took: 0.1350395679473877\n",
            "evaluation of batch 1050 took: 0.12302088737487793\n",
            "evaluation of batch 1100 took: 0.05100846290588379\n",
            "evaluation of batch 1150 took: 0.07296895980834961\n",
            "evaluation of batch 1200 took: 0.053949832916259766\n",
            "epoch 11 evaluation on training data time: 89.4171347618103 sec\n",
            "evaluation of batch 0 took: 0.06045246124267578\n",
            "evaluation of batch 50 took: 0.049839019775390625\n",
            "evaluation of batch 100 took: 0.05267906188964844\n",
            "evaluation of batch 150 took: 0.04925727844238281\n",
            "epoch 11 evaluation on test data time: 37.05630874633789 sec\n",
            "epoch evaluation:  {'epoch': 11, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=47.376766>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "Increasing epochs without improvement to: 3\n",
            "epoch 12 - batch 10 - loss 42.32817459106445\n",
            "epoch 12 - batch 20 - loss 53.45560073852539\n",
            "epoch 12 - batch 30 - loss 42.61149215698242\n",
            "epoch 12 - batch 40 - loss 43.36179733276367\n",
            "epoch 12 - batch 50 - loss 47.60444641113281\n",
            "epoch 12 - batch 60 - loss 45.961727142333984\n",
            "epoch 12 - batch 70 - loss 44.33584213256836\n",
            "epoch 12 - batch 80 - loss 46.61824035644531\n",
            "epoch 12 - batch 90 - loss 43.55196762084961\n",
            "epoch 12 - batch 100 - loss 41.56059265136719\n",
            "epoch 12 - batch 110 - loss 45.6828498840332\n",
            "epoch 12 - batch 120 - loss 43.632389068603516\n",
            "epoch 12 - batch 130 - loss 46.287967681884766\n",
            "epoch 12 - batch 140 - loss 42.10426712036133\n",
            "epoch 12 - batch 150 - loss 41.849220275878906\n",
            "epoch 12 - batch 160 - loss 44.7485237121582\n",
            "epoch 12 - batch 170 - loss 37.58940124511719\n",
            "epoch 12 - batch 180 - loss 39.64577102661133\n",
            "epoch 12 - batch 190 - loss 47.1109504699707\n",
            "epoch 12 - batch 200 - loss 45.025455474853516\n",
            "epoch 12 - batch 210 - loss 47.7289924621582\n",
            "epoch 12 - batch 220 - loss 38.96345138549805\n",
            "epoch 12 - batch 230 - loss 45.37687683105469\n",
            "epoch 12 - batch 240 - loss 40.681644439697266\n",
            "epoch 12 - batch 250 - loss 43.17762756347656\n",
            "epoch 12 - batch 260 - loss 43.15775680541992\n",
            "epoch 12 - batch 270 - loss 36.323062896728516\n",
            "epoch 12 - batch 280 - loss 41.234649658203125\n",
            "epoch 12 - batch 290 - loss 42.46358108520508\n",
            "epoch 12 - batch 300 - loss 44.858978271484375\n",
            "epoch 12 - batch 310 - loss 38.21182632446289\n",
            "epoch 12 - batch 320 - loss 43.80685043334961\n",
            "epoch 12 - batch 330 - loss 39.98569107055664\n",
            "epoch 12 - batch 340 - loss 43.75755310058594\n",
            "epoch 12 - batch 350 - loss 40.72071075439453\n",
            "epoch 12 - batch 360 - loss 39.8944206237793\n",
            "epoch 12 - batch 370 - loss 43.04197311401367\n",
            "epoch 12 - batch 380 - loss 39.33677673339844\n",
            "epoch 12 - batch 390 - loss 43.495880126953125\n",
            "epoch 12 - batch 400 - loss 40.44511413574219\n",
            "epoch 12 - batch 410 - loss 39.4478874206543\n",
            "epoch 12 - batch 420 - loss 44.67062759399414\n",
            "epoch 12 - batch 430 - loss 40.54922103881836\n",
            "epoch 12 - batch 440 - loss 38.091522216796875\n",
            "epoch 12 - batch 450 - loss 39.37704849243164\n",
            "epoch 12 - batch 460 - loss 40.65966796875\n",
            "epoch 12 - batch 470 - loss 36.83599853515625\n",
            "epoch 12 - batch 480 - loss 39.10298538208008\n",
            "epoch 12 - batch 490 - loss 41.35947036743164\n",
            "epoch 12 - batch 500 - loss 48.33420944213867\n",
            "epoch 12 - batch 510 - loss 38.03315734863281\n",
            "epoch 12 - batch 520 - loss 40.74577331542969\n",
            "epoch 12 - batch 530 - loss 36.00859451293945\n",
            "epoch 12 - batch 540 - loss 40.77582931518555\n",
            "epoch 12 - batch 550 - loss 43.574283599853516\n",
            "epoch 12 - batch 560 - loss 34.475460052490234\n",
            "epoch 12 - batch 570 - loss 40.31700897216797\n",
            "epoch 12 - batch 580 - loss 34.633548736572266\n",
            "epoch 12 - batch 590 - loss 35.418113708496094\n",
            "epoch 12 - batch 600 - loss 32.642704010009766\n",
            "epoch 12 - batch 610 - loss 34.70643997192383\n",
            "epoch 12 - batch 620 - loss 38.78364181518555\n",
            "epoch 12 - batch 630 - loss 34.482418060302734\n",
            "epoch 12 - batch 640 - loss 37.119632720947266\n",
            "epoch 12 - batch 650 - loss 36.115352630615234\n",
            "epoch 12 - batch 660 - loss 31.726839065551758\n",
            "epoch 12 - batch 670 - loss 44.137210845947266\n",
            "epoch 12 - batch 680 - loss 38.49935531616211\n",
            "epoch 12 - batch 690 - loss 37.30349349975586\n",
            "epoch 12 - batch 700 - loss 34.664737701416016\n",
            "epoch 12 - batch 710 - loss 39.62874984741211\n",
            "epoch 12 - batch 720 - loss 40.55104446411133\n",
            "epoch 12 - batch 730 - loss 38.1829833984375\n",
            "epoch 12 - batch 740 - loss 40.1797981262207\n",
            "epoch 12 - batch 750 - loss 34.102298736572266\n",
            "epoch 12 - batch 760 - loss 39.102691650390625\n",
            "epoch 12 - batch 770 - loss 40.20608901977539\n",
            "epoch 12 - batch 780 - loss 28.871536254882812\n",
            "epoch 12 - batch 790 - loss 33.97807693481445\n",
            "epoch 12 - batch 800 - loss 36.89591979980469\n",
            "epoch 12 - batch 810 - loss 33.199337005615234\n",
            "epoch 12 - batch 820 - loss 40.6253776550293\n",
            "epoch 12 - batch 830 - loss 37.7607536315918\n",
            "epoch 12 - batch 840 - loss 35.07554626464844\n",
            "epoch 12 - batch 850 - loss 35.79412841796875\n",
            "epoch 12 - batch 860 - loss 36.588924407958984\n",
            "epoch 12 - batch 870 - loss 35.928016662597656\n",
            "epoch 12 - batch 880 - loss 34.38736343383789\n",
            "epoch 12 - batch 890 - loss 32.594173431396484\n",
            "epoch 12 - batch 900 - loss 43.38804244995117\n",
            "epoch 12 - batch 910 - loss 38.82273483276367\n",
            "epoch 12 - batch 920 - loss 38.18838882446289\n",
            "epoch 12 - batch 930 - loss 38.734596252441406\n",
            "epoch 12 - batch 940 - loss 41.44087219238281\n",
            "epoch 12 - batch 950 - loss 41.07986831665039\n",
            "epoch 12 - batch 960 - loss 43.90802001953125\n",
            "epoch 12 - batch 970 - loss 35.824127197265625\n",
            "epoch 12 - batch 980 - loss 32.73741149902344\n",
            "epoch 12 - batch 990 - loss 38.676334381103516\n",
            "epoch 12 - batch 1000 - loss 35.081642150878906\n",
            "epoch 12 - batch 1010 - loss 43.00615310668945\n",
            "epoch 12 - batch 1020 - loss 39.20384979248047\n",
            "epoch 12 - batch 1030 - loss 44.43162155151367\n",
            "epoch 12 - batch 1040 - loss 31.61475944519043\n",
            "epoch 12 - batch 1050 - loss 42.26163101196289\n",
            "epoch 12 - batch 1060 - loss 39.08729553222656\n",
            "epoch 12 - batch 1070 - loss 42.12394332885742\n",
            "epoch 12 - batch 1080 - loss 40.48927688598633\n",
            "epoch 12 - batch 1090 - loss 38.35499954223633\n",
            "epoch 12 - batch 1100 - loss 40.6581916809082\n",
            "epoch 12 - batch 1110 - loss 37.608558654785156\n",
            "epoch 12 - batch 1120 - loss 40.56106185913086\n",
            "epoch 12 - batch 1130 - loss 40.3388786315918\n",
            "epoch 12 - batch 1140 - loss 38.62431335449219\n",
            "epoch 12 - batch 1150 - loss 35.810123443603516\n",
            "epoch 12 - batch 1160 - loss 41.93719482421875\n",
            "epoch 12 - batch 1170 - loss 44.10005187988281\n",
            "epoch 12 - batch 1180 - loss 36.277122497558594\n",
            "epoch 12 - batch 1190 - loss 35.46126174926758\n",
            "epoch 12 - batch 1200 - loss 32.97163009643555\n",
            "epoch 12 - batch 1210 - loss 38.532222747802734\n",
            "epoch 12 - batch 1220 - loss 42.39570236206055\n",
            "epoch 12 - batch 1230 - loss 33.99717330932617\n",
            "epoch 12 training time: 681.4341945648193 sec\n",
            "evaluation of batch 0 took: 0.05253434181213379\n",
            "evaluation of batch 50 took: 0.07327866554260254\n",
            "evaluation of batch 100 took: 0.055800437927246094\n",
            "evaluation of batch 150 took: 0.06936907768249512\n",
            "evaluation of batch 200 took: 0.07101011276245117\n",
            "evaluation of batch 250 took: 0.05275845527648926\n",
            "evaluation of batch 300 took: 0.05344414710998535\n",
            "evaluation of batch 350 took: 0.05466151237487793\n",
            "evaluation of batch 400 took: 0.04994463920593262\n",
            "evaluation of batch 450 took: 0.05125689506530762\n",
            "evaluation of batch 500 took: 0.05096578598022461\n",
            "evaluation of batch 550 took: 0.0502016544342041\n",
            "evaluation of batch 600 took: 0.05254936218261719\n",
            "evaluation of batch 650 took: 0.04940676689147949\n",
            "evaluation of batch 700 took: 0.05394554138183594\n",
            "evaluation of batch 750 took: 0.058791160583496094\n",
            "evaluation of batch 800 took: 0.050458669662475586\n",
            "evaluation of batch 850 took: 0.12726140022277832\n",
            "evaluation of batch 900 took: 0.1182396411895752\n",
            "evaluation of batch 950 took: 0.052379608154296875\n",
            "evaluation of batch 1000 took: 0.05276989936828613\n",
            "evaluation of batch 1050 took: 0.12362384796142578\n",
            "evaluation of batch 1100 took: 0.11622333526611328\n",
            "evaluation of batch 1150 took: 0.05271601676940918\n",
            "evaluation of batch 1200 took: 0.05294036865234375\n",
            "epoch 12 evaluation on training data time: 92.48411464691162 sec\n",
            "evaluation of batch 0 took: 0.13395977020263672\n",
            "evaluation of batch 50 took: 0.1287689208984375\n",
            "evaluation of batch 100 took: 0.12934041023254395\n",
            "evaluation of batch 150 took: 0.12460517883300781\n",
            "epoch 12 evaluation on test data time: 48.71146368980408 sec\n",
            "epoch evaluation:  {'epoch': 12, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=42.6772>, 'test_rouge_1_p': 0.0, 'test_rouge_1_r': 0.0, 'test_rouge_1_f1': 0.0, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.0, 'test_rouge_L_r': 0.0, 'test_rouge_L_f1': 0.0}\n",
            "epoch 12 saved checkpoint: models/checkpoints/baseline/ckpt-5\n",
            "Increasing epochs without improvement to: 4\n",
            "epoch 13 - batch 10 - loss 45.970767974853516\n",
            "epoch 13 - batch 20 - loss 41.77582931518555\n",
            "epoch 13 - batch 30 - loss 46.75335693359375\n",
            "epoch 13 - batch 40 - loss 40.26453399658203\n",
            "epoch 13 - batch 50 - loss 44.40888595581055\n",
            "epoch 13 - batch 60 - loss 46.098751068115234\n",
            "epoch 13 - batch 70 - loss 41.39973068237305\n",
            "epoch 13 - batch 80 - loss 45.977718353271484\n",
            "epoch 13 - batch 90 - loss 43.76119613647461\n",
            "epoch 13 - batch 100 - loss 46.30552291870117\n",
            "epoch 13 - batch 110 - loss 43.39072036743164\n",
            "epoch 13 - batch 120 - loss 50.72871017456055\n",
            "epoch 13 - batch 130 - loss 46.71388244628906\n",
            "epoch 13 - batch 140 - loss 43.96236801147461\n",
            "epoch 13 - batch 150 - loss 45.13047409057617\n",
            "epoch 13 - batch 160 - loss 44.51442337036133\n",
            "epoch 13 - batch 170 - loss 40.489166259765625\n",
            "epoch 13 - batch 180 - loss 54.122318267822266\n",
            "epoch 13 - batch 190 - loss 44.72670364379883\n",
            "epoch 13 - batch 200 - loss 46.742496490478516\n",
            "epoch 13 - batch 210 - loss 41.28628921508789\n",
            "epoch 13 - batch 220 - loss 42.05315399169922\n",
            "epoch 13 - batch 230 - loss 50.36659622192383\n",
            "epoch 13 - batch 240 - loss 46.69050216674805\n",
            "epoch 13 - batch 250 - loss 47.1035270690918\n",
            "epoch 13 - batch 260 - loss 51.1857795715332\n",
            "epoch 13 - batch 270 - loss 42.741207122802734\n",
            "epoch 13 - batch 280 - loss 46.0949821472168\n",
            "epoch 13 - batch 290 - loss 48.21355056762695\n",
            "epoch 13 - batch 300 - loss 45.15421676635742\n",
            "epoch 13 - batch 310 - loss 42.96818161010742\n",
            "epoch 13 - batch 320 - loss 43.203983306884766\n",
            "epoch 13 - batch 330 - loss 42.60177230834961\n",
            "epoch 13 - batch 340 - loss 45.908660888671875\n",
            "epoch 13 - batch 350 - loss 40.87057113647461\n",
            "epoch 13 - batch 360 - loss 47.49289321899414\n",
            "epoch 13 - batch 370 - loss 45.219970703125\n",
            "epoch 13 - batch 380 - loss 41.60500717163086\n",
            "epoch 13 - batch 390 - loss 50.27277755737305\n",
            "epoch 13 - batch 400 - loss 46.131622314453125\n",
            "epoch 13 - batch 410 - loss 45.568389892578125\n",
            "epoch 13 - batch 420 - loss 46.562801361083984\n",
            "epoch 13 - batch 430 - loss 36.86406326293945\n",
            "epoch 13 - batch 440 - loss 39.600460052490234\n",
            "epoch 13 - batch 450 - loss 39.87974166870117\n",
            "epoch 13 - batch 460 - loss 40.21116256713867\n",
            "epoch 13 - batch 470 - loss 38.73451232910156\n",
            "epoch 13 - batch 480 - loss 45.1033821105957\n",
            "epoch 13 - batch 490 - loss 43.3433723449707\n",
            "epoch 13 - batch 500 - loss 51.2823600769043\n",
            "epoch 13 - batch 510 - loss 44.23280334472656\n",
            "epoch 13 - batch 520 - loss 45.8134765625\n",
            "epoch 13 - batch 530 - loss 44.53851318359375\n",
            "epoch 13 - batch 540 - loss 40.45361328125\n",
            "epoch 13 - batch 550 - loss 41.89231872558594\n",
            "epoch 13 - batch 560 - loss 43.11439895629883\n",
            "epoch 13 - batch 570 - loss 42.18618392944336\n",
            "epoch 13 - batch 580 - loss 45.715087890625\n",
            "epoch 13 - batch 590 - loss 44.1972770690918\n",
            "epoch 13 - batch 600 - loss 39.853546142578125\n",
            "epoch 13 - batch 610 - loss 41.78688430786133\n",
            "epoch 13 - batch 620 - loss 40.553985595703125\n",
            "epoch 13 - batch 630 - loss 41.156742095947266\n",
            "epoch 13 - batch 640 - loss 40.522552490234375\n",
            "epoch 13 - batch 650 - loss 36.62124252319336\n",
            "epoch 13 - batch 660 - loss 40.49955368041992\n",
            "epoch 13 - batch 670 - loss 37.1089973449707\n",
            "epoch 13 - batch 680 - loss 40.89542770385742\n",
            "epoch 13 - batch 690 - loss 37.91277313232422\n",
            "epoch 13 - batch 700 - loss 36.50741958618164\n",
            "epoch 13 - batch 710 - loss 39.983638763427734\n",
            "epoch 13 - batch 720 - loss 37.5513801574707\n",
            "epoch 13 - batch 730 - loss 38.42762756347656\n",
            "epoch 13 - batch 740 - loss 37.23868179321289\n",
            "epoch 13 - batch 750 - loss 39.16701126098633\n",
            "epoch 13 - batch 760 - loss 34.753055572509766\n",
            "epoch 13 - batch 770 - loss 37.68595886230469\n",
            "epoch 13 - batch 780 - loss 47.07876205444336\n",
            "epoch 13 - batch 790 - loss 37.98076248168945\n",
            "epoch 13 - batch 800 - loss 41.57155990600586\n",
            "epoch 13 - batch 810 - loss 38.59609603881836\n",
            "epoch 13 - batch 820 - loss 39.0877799987793\n",
            "epoch 13 - batch 830 - loss 37.38934326171875\n",
            "epoch 13 - batch 840 - loss 35.59149169921875\n",
            "epoch 13 - batch 850 - loss 32.33038330078125\n",
            "epoch 13 - batch 860 - loss 35.15056228637695\n",
            "epoch 13 - batch 870 - loss 31.11078453063965\n",
            "epoch 13 - batch 880 - loss 41.99381637573242\n",
            "epoch 13 - batch 890 - loss 36.49140930175781\n",
            "epoch 13 - batch 900 - loss 30.6494083404541\n",
            "epoch 13 - batch 910 - loss 34.02729034423828\n",
            "epoch 13 - batch 920 - loss 38.5561637878418\n",
            "epoch 13 - batch 930 - loss 37.56006622314453\n",
            "epoch 13 - batch 940 - loss 34.854827880859375\n",
            "epoch 13 - batch 950 - loss 36.14584732055664\n",
            "epoch 13 - batch 960 - loss 34.2592887878418\n",
            "epoch 13 - batch 970 - loss 42.09366989135742\n",
            "epoch 13 - batch 980 - loss 40.78731918334961\n",
            "epoch 13 - batch 990 - loss 45.2358512878418\n",
            "epoch 13 - batch 1000 - loss 42.0573844909668\n",
            "epoch 13 - batch 1010 - loss 44.10902404785156\n",
            "epoch 13 - batch 1020 - loss 39.03923416137695\n",
            "epoch 13 - batch 1030 - loss 37.883827209472656\n",
            "epoch 13 - batch 1040 - loss 35.93095779418945\n",
            "epoch 13 - batch 1050 - loss 38.4504280090332\n",
            "epoch 13 - batch 1060 - loss 39.188899993896484\n",
            "epoch 13 - batch 1070 - loss 39.629112243652344\n",
            "epoch 13 - batch 1080 - loss 37.12517166137695\n",
            "epoch 13 - batch 1090 - loss 42.81704330444336\n",
            "epoch 13 - batch 1100 - loss 34.70005416870117\n",
            "epoch 13 - batch 1110 - loss 36.50724411010742\n",
            "epoch 13 - batch 1120 - loss 41.38885498046875\n",
            "epoch 13 - batch 1130 - loss 39.51933288574219\n",
            "epoch 13 - batch 1140 - loss 41.46530532836914\n",
            "epoch 13 - batch 1150 - loss 32.730594635009766\n",
            "epoch 13 - batch 1160 - loss 41.93335723876953\n",
            "epoch 13 - batch 1170 - loss 38.23404312133789\n",
            "epoch 13 - batch 1180 - loss 31.96660041809082\n",
            "epoch 13 - batch 1190 - loss 33.953697204589844\n",
            "epoch 13 - batch 1200 - loss 37.04207992553711\n",
            "epoch 13 - batch 1210 - loss 33.21604537963867\n",
            "epoch 13 - batch 1220 - loss 38.90181350708008\n",
            "epoch 13 - batch 1230 - loss 41.461769104003906\n",
            "epoch 13 training time: 680.8485054969788 sec\n",
            "evaluation of batch 0 took: 0.12572312355041504\n",
            "evaluation of batch 50 took: 0.07101321220397949\n",
            "evaluation of batch 100 took: 0.07073354721069336\n",
            "evaluation of batch 150 took: 0.07288074493408203\n",
            "evaluation of batch 200 took: 0.05100250244140625\n",
            "evaluation of batch 250 took: 0.05041337013244629\n",
            "evaluation of batch 300 took: 0.054990530014038086\n",
            "evaluation of batch 350 took: 0.1176154613494873\n",
            "evaluation of batch 400 took: 0.11464476585388184\n",
            "evaluation of batch 450 took: 0.05275988578796387\n",
            "evaluation of batch 500 took: 0.05026698112487793\n",
            "evaluation of batch 550 took: 0.04962038993835449\n",
            "evaluation of batch 600 took: 0.05005788803100586\n",
            "evaluation of batch 650 took: 0.11458730697631836\n",
            "evaluation of batch 700 took: 0.04935169219970703\n",
            "evaluation of batch 750 took: 0.0497434139251709\n",
            "evaluation of batch 800 took: 0.051445960998535156\n",
            "evaluation of batch 850 took: 0.05125117301940918\n",
            "evaluation of batch 900 took: 0.05101275444030762\n",
            "evaluation of batch 950 took: 0.12174487113952637\n",
            "evaluation of batch 1000 took: 0.051971435546875\n",
            "evaluation of batch 1050 took: 0.05773568153381348\n",
            "evaluation of batch 1100 took: 0.051483869552612305\n",
            "evaluation of batch 1150 took: 0.0702676773071289\n",
            "evaluation of batch 1200 took: 0.052015066146850586\n",
            "epoch 13 evaluation on training data time: 89.89283514022827 sec\n",
            "evaluation of batch 0 took: 0.13669943809509277\n",
            "evaluation of batch 50 took: 0.1330406665802002\n",
            "evaluation of batch 100 took: 0.12782692909240723\n",
            "evaluation of batch 150 took: 0.12600970268249512\n",
            "epoch 13 evaluation on test data time: 49.195313930511475 sec\n",
            "epoch evaluation:  {'epoch': 13, 'epoch_test_loss': <tf.Tensor: shape=(), dtype=float32, numpy=39.014206>, 'test_rouge_1_p': 0.003409513663419913, 'test_rouge_1_r': 0.006951772186147189, 'test_rouge_1_f1': 0.004349660727755289, 'test_rouge_2_p': 0.0, 'test_rouge_2_r': 0.0, 'test_rouge_2_f1': 0.0, 'test_rouge_3_p': 0.0, 'test_rouge_3_r': 0.0, 'test_rouge_3_f1': 0.0, 'test_rouge_L_p': 0.003409513663419913, 'test_rouge_L_r': 0.006951772186147189, 'test_rouge_L_f1': 0.004349660727755289}\n",
            "Increasing epochs without improvement to: 5\n",
            "Early stopping of training after 5 epochs without improvement\n",
            "Best epoch was: 8, Best Rouge-L F1 was: 0.03117661003149534\n",
            "Saving checkpoint...\n",
            "Saved checkpoint: models/checkpoints/baseline/ckpt-6\n",
            "Done saving model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
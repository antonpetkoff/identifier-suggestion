# Identifier Suggestion

Models for source code identifier suggestion built by learning from Big Code

## Setup

### Requirements

1. Have Python 3.7 or above installed.

2. Have a Unix-like terminal with `bash`, `coreutils` and some other utilities like `wget` (required by some .sh scripts).

### Installation

1. Run `pip install -r requirements.txt` to install all Python package dependencies.

### Installation with a virtual environment

For example, you can use the standard `venv`:

1. Run `python3 -m venv .venv` to create a virtual environment.

1. Run `source .venv/bin/activate` to activate the virtual environment. This step must be executed on every new terminal session.

1. Run `pip install -r requirements.txt` to install all Python package dependencies.

## Project Structure

Follows the [Cookiecutter Data Science project structure](https://drivendata.github.io/cookiecutter-data-science/).

```text
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
├── Pipfile            <- Contains all installed packages
└── Pipfile.lock       <- Declares all dependencies (and sub-dependencies) of the project
                          and the current hashes for the downloaded files.
```

## Tasks

Support:

- [ ] Organize project structure to make it more easily reproducible
  - [ ] Write scripts for downloading the source code repositories (with wget)
  - [ ] Write scripts for generating the datasets

Fundamentals:

- [ ] Create data input pipeline with tf.data Dataset API
- [ ] Add Beam Search Decoder for making multiple suggestions
- [ ] Fixate all random seeds for reproducible results
- [ ] Run one full experiment
  - [ ] Log training and evaluation
  - [ ] Save configuration
  - [ ] Save checkpoints of model weights
  - [ ] Make predictions
  - [ ] Run evaluation on test set

Modelling and Feature Engineering (Creative):

- [ ] Does subtoken splitting improve performance?
- [ ] Does attention improve performance?
- [ ] Does a bidirectional RNN perform better than a unidirectional RNN?
- [ ] Do binary/categorical features improve performance?
